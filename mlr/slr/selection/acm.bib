@inproceedings{10.1145/3195836.3195853,
author = {Alami, Adam and Dittrich, Yvonne and W\k{a}sowski, Andrzej},
title = {Influencers of Quality Assurance in an Open Source Community},
year = {2018},
isbn = {9781450357258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195836.3195853},
doi = {10.1145/3195836.3195853},
abstract = {ROS (Robot Operating System) is an open source community in robotics that is developing standard robotics operating system facilities such as hardware abstraction, low-level device control, communication middleware, and a wide range of software components for robotics functionality. This paper studies the quality assurance practices of the ROS community. We use qualitative methods to understand how ideology, priorities of the community, culture, sustainability, complexity, and adaptability of the community affect the implementation of quality assurance practices. Our analysis suggests that software engineering practices require social and cultural alignment and adaptation to the community particularities to achieve seamless implementation in open source environments. This alignment should be incorporated into the design and implementation of quality assurance practices in open source communities.},
booktitle = {Proceedings of the 11th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {61–68},
numpages = {8},
keywords = {OSS community, quality assurance, open source software},
location = {Gothenburg, Sweden},
series = {CHASE '18}
}

@inproceedings{10.1145/2077489.2077526,
author = {dos Santos, Rodrigo Pereira and Werner, Cl\'{a}udia},
title = {Treating Business Dimension in Software Ecosystems},
year = {2011},
isbn = {9781450310475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077489.2077526},
doi = {10.1145/2077489.2077526},
abstract = {Software Ecosystems (SECOs) have emerged as an approach to improve Software Engineering (SE) in industry considering relations among companies and stakeholders. Companies have opened up their platforms and artifacts to others, including partners and third-part developers. Thus, systems architecture, businesses and social networks should be understood in an integrated way. This concern motivated an initial proposal of a framework for SECOs engineering in order to outline a set of steps that combines three different dimensions of SECOs and joins different perspectives in SECOs research literature through a survey. In this paper, the focus is on the second dimension, business. A preliminary analysis carried out on a Software Reuse Lab's SECO at COPPE/UFRJ points out that concepts explored by researchers can be merged in a broader SE approach.},
booktitle = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems},
pages = {197–201},
numpages = {5},
keywords = {component-based software engineering, value-based software engineering, software ecosystems, software reuse},
location = {San Francisco, California},
series = {MEDES '11}
}

@inproceedings{10.1145/1842752.1842774,
author = {Campbell, P. R. J. and Ahmed, Faheem},
title = {A Three-Dimensional View of Software Ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842774},
doi = {10.1145/1842752.1842774},
abstract = {The concept of a Software ecosystem is gaining in popularity amongst large organizations and inherently relies on the adoption of common architectural development for multiple product development. The adoption of these approaches represents a significant shift in traditional software development style and process methodology. Currently several organizations are in practice with this new process model which embraces business, third party involvement and open architecture as its central pillars and these institutions have thrived as a result. The advent of software ecosystems have caused major players in the software industry to rethink their operating practices and engage with third parties, opening their platforms to external entities to attain business objectives. In this paper we present a three dimensional view of the software ecosystem model examine the role played by each of the three central pillars; business; architecture; and social aspects. We further highlight their relationships and conclude that this study will help in further aiding understanding of the overall engineering process of ecosystem software.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {81–84},
numpages = {4},
keywords = {software ecosystems, software product lines, architecture},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/1858996.1859058,
author = {Lungu, Mircea and Robbes, Romain and Lanza, Michele},
title = {Recovering Inter-Project Dependencies in Software Ecosystems},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859058},
doi = {10.1145/1858996.1859058},
abstract = {In large software systems, knowing the dependencies between modules or components is critical to assess the impact of changes. To recover the dependencies, fact extractors analyze the system as a whole and build the dependency graph, parsing the system down to the statement level. At the level of software ecosystems, which are collections of software projects, the dependencies that need to be recovered reside not only within the individual systems, but also between the libraries, frameworks, and entire software systems that make up the complete ecosystem; scaling issues arise.In this paper we present and evaluate several variants of a lightweight and scalable approach to recover dependencies between the software projects of an ecosystem. We evaluate our recovery algorithms on the Squeak 3.10 Universe, an ecosystem containing more than 200 software projects.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {309–312},
numpages = {4},
keywords = {software ecosystems, static analysis, dependency analysis},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2797433.2797475,
author = {Serebrenik, Alexander and Mens, Tom},
title = {Challenges in Software Ecosystems Research},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797475},
doi = {10.1145/2797433.2797475},
abstract = {The paper is a meta-analysis of the research field of software ecosystems, by method of surveying 26 authors in the field. It presents a relevant list of literature and six themes in which challenges for software ecosystems can be grouped: Architecture and Design, Governance, Dynamics and Evolution, Data Analytics, Domain-Specific Ecosystems Solutions, and Ecosystems Analysis. As such, it provides a roadmap for future research in the field.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {40},
numpages = {6},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3387940.3392188,
author = {Foundjem, Armstrong},
title = {Cross-Distribution Feedback in Software Ecosystems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392188},
doi = {10.1145/3387940.3392188},
abstract = {Despite the proliferation of software ecosystems (SECOs), growing a sustainable and healthy SECO remains a significant challenge. One approach to mitigate this challenge is the utilization of a mechanism that collects feedback from distributors (distros) and end-users of the SECO releases. This presentation aims at investigating the effectiveness of the feedback mechanism implemented by OpenStack to address the needs of end-users and distros. I mined the OpenStack repositories and mapped 20 distros' bug-related activities. Results suggest that OpenStack releases are actively maintained for 18 months before reaching end-of-life (EOL), which makes coordination with distros difficult because distros usually provide services to their end-users for a period between 36 - 60 months before reaching EOL. Also, bugs are fixed faster by the distros (7 - 76 days) than the OpenStack community (average of 4 months). However, only 22% of the bugs addressed by OpenStack distros are pushed back upstream.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {723–724},
numpages = {2},
keywords = {Downstream/Upstream development, Cross-bugs, Dependencies, Software Ecosystem},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2889160.2892649,
author = {Aldaeej, Abdullah and Badreddin, Omar},
title = {Towards Promoting Design and UML Modeling Practices in the Open Source Community},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2892649},
doi = {10.1145/2889160.2892649},
abstract = {Despite the emergence of UML as the defacto modeling and design tool for software engineering, its adoption remains dismal. Software development, particularly in the open source community, remains code-centric. Adoption of UML in open source projects represents a significant lost opportunity. In this paper, we present an approach to encourage upfront design practices and the adoption of UML modeling in open source projects. In particular, we demonstrate the approach for small contributions and bug fixes. The approach relies on integrating UML-level abstractions into the code. This integration means that open source developers can continue to use their familiar text-based tools to manage the source code and contributions, while at the same time benefit from UML added value of abstractions and comprehension. Other benefits of this approach include broadening the boundaries of bug fix contribution by including modelers and end-users, and incrementally add UML model diagrams into open source project's documentation.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {722–724},
numpages = {3},
keywords = {UML, model oriented programming, bug fixing, open source projects, reserve engineering, program comprehension, code generation, software design, umple, forward engineering},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/1842752.1842777,
author = {Dhungana, Deepak and Groher, Iris and Schludermann, Elisabeth and Biffl, Stefan},
title = {Software Ecosystems vs. Natural Ecosystems: Learning from the Ingenious Mind of Nature},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842777},
doi = {10.1145/1842752.1842777},
abstract = {The use of the term ecosystem in the context of extensible software platforms and third-party developers or user communities has made us ponder about the similarities between software ecosystems and natural ecosystems. We therefore compare software ecosystems and natural ecosystems to present an agenda for further research by analyzing some key characteristics of both types of ecosystems. We discuss the regulatory factors and mechanisms existing in nature, and then deduce key challenges that need to be dealt with, in order to achieve healthy operation of software ecosystems.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {96–102},
numpages = {7},
keywords = {software, nature, ecosystem},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/2797433.2797479,
author = {Jansen, Slinger and Handoyo, Eko and Alves, Carina},
title = {Scientists' Needs in Modelling Software Ecosystems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797479},
doi = {10.1145/2797433.2797479},
abstract = {Currently the landscape of software ecosystem modelling methods and languages is like Babel after the fall of the tower: there are many methods and languages available and interchanging data between researchers and organizations that actively govern their ecosystem, is practically impossible. The lack of a universally accepted set of modelling methods is hampering the advancement of software ecosystems research. Using a literature study and a set of interviews amongst peers, we aim to establish a set of understandings and requirements for a universally accepted set of software ecosystem modelling methods. The work is an initial push in a larger research initiative that has the goal of advancing the maturity of (software) ecosystems modelling. The success of such an initiative will be found in the availability of common databases, better interchange formats between researchers, and more capable software ecosystem modelling tools.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {44},
numpages = {6},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/1985793.1985940,
author = {Robbes, Romain and Lungu, Mircea},
title = {A Study of Ripple Effects in Software Ecosystems (NIER Track)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985940},
doi = {10.1145/1985793.1985940},
abstract = {When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation - known as a ripple effect - is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes.Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on early results of such an empirical study of API changes that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating aroung the Squeak and Pharo software ecosystems: six years of evolution, nearly 3,000 contributors, and close to 2,500 distinct systems.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {904–907},
numpages = {4},
keywords = {mining software repositories, empirical studies, software ecosystems},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2701319.2701333,
author = {Weber, Jens H. and Katahoire, Anita and Price, Morgan},
title = {Uncovering Variability Models for Software Ecosystems from Multi-Repository Structures},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701333},
doi = {10.1145/2701319.2701333},
abstract = {Variability is a significant source of complexity in many large-scale software systems. Software variability must be managed in order to effectively tame the arising complexity. Consequently, variability management processes are at the heart of current software product line engineering practices. However, legacy software systems exist that have not been developed with such practices. Moreover, an increasing amount of software is developed in large, fragmented communities, also referred to as software ecosystems. Variability in such systems is often not explicitly managed and causes significant difficulties during software maintenance and evolution. Methods and tools for uncovering and explicitly managing this variability have been subject to ongoing research. This paper presents our research in progress of empirically studying the application and combination of such methods in the context of real-world industrial case study in the health care domain.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {103–108},
numpages = {6},
keywords = {software product lines, multi-repository software, software ecosystems, variability management},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3167132.3167290,
author = {Miranda, Andr\'{e} and Pimentel, Jo\~{a}o},
title = {On the Use of Package Managers by the C++ Open-Source Community},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167290},
doi = {10.1145/3167132.3167290},
abstract = {The use of package managers is commonplace for software developers working with programming languages such as Ruby, Python, and JavaScript. This is not the case for C++ developers, which present a low adoption rate of package managers.The goal of this study is to understand what is preventing C++ developers from adopting package managers in the context of open-source software (OSS) projects. In order to achieve this goal, we performed a questionnaire survey with 343 developers from 42 OSS projects. The survey participants answered a questionnaire with 29 questions.After the analysis of the collected data, we could conclude that most participants are not reluctant to use C++ package managers and that Open-Source licensing, High Availability of Libraries, Good Documentation, and Ease of Configuration can be considered crucial factors for the successful adoption of C++ dependency management via language-specific package managers.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1483–1491},
numpages = {9},
keywords = {software maintenance, software tools, open-source software, software engineering, empirical study},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/1842752.1842776,
author = {Bosch, Jan},
title = {Architecture Challenges for Software Ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842776},
doi = {10.1145/1842752.1842776},
abstract = {Successful software products and product lines exhibit a force of gravity that attracts external developers. The organization developing the product actively seeks to involve external developers to add functionality to the system. On the other hand, external developers flock to a successful product as it provides an established market where underserved niches provide a business opportunity for these players. The product or product line evolves into a platform for external developers. This causes several software architecture challenges for a software ecosystem, including interface stability, evolution management, guaranteeing security and reliability and composition of independently developed functionality. The paper describes these challenges, discusses practical solutions and identifies research challenges.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {93–95},
numpages = {3},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3555228.3555264,
author = {Soupinski, Felipe and Arantes, Pedro and Steinmacher, Igor and Wiese, Igor and Borges, Hudson and Cafeo, Bruno and Font\~{a}o, Awdren},
title = {”We Are Dying!” On Death Signals of Software Ecosystems},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555228.3555264},
doi = {10.1145/3555228.3555264},
booktitle = {Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
pages = {363–369},
numpages = {7},
location = {Virtual Event, Brazil},
series = {SBES '22}
}

@inproceedings{10.1145/1842752.1842780,
author = {Seichter, Dominik and Dhungana, Deepak and Pleuss, Andreas and Hauptmann, Benedikt},
title = {Knowledge Management in Software Ecosystems: Software Artefacts as First-Class Citizens},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842780},
doi = {10.1145/1842752.1842780},
abstract = {Collaborative development of software products across organisational boundaries in software ecosystems adds new challenges to existing software engineering processes. We propose a new approach for handling the diverse software artefacts in ecosystems by adapting features from social network sites. We promote artefacts to first-class citizens in such networks and specify different types of relationships between artefacts and actors. This helps in detaching tacit knowledge from vendors, suppliers, developers and users of an ecosystem and fosters easier management of software artefacts. We discuss this by example scenarios and present a prototypic implementation.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {119–126},
numpages = {8},
keywords = {ecosystems, social networks, software artefacts, knowledge management},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3377811.3380442,
author = {Ma, Wanwangying and Chen, Lin and Zhang, Xiangyu and Feng, Yang and Xu, Zhaogui and Chen, Zhifei and Zhou, Yuming and Xu, Baowen},
title = {Impact Analysis of Cross-Project Bugs on Software Ecosystems},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380442},
doi = {10.1145/3377811.3380442},
abstract = {Software projects are increasingly forming social-technical ecosystems within which individual projects rely on the infrastructures or functional components provided by other projects, leading to complex inter-dependencies. Through inter-project dependencies, a bug in an upstream project may have profound impact on a large number of downstream projects, resulting in cross-project bugs. This emerging type of bugs has brought new challenges in bug fixing due to their unclear influence on downstream projects. In this paper, we present an approach to estimating the impact of a cross-project bug within its ecosystem by identifying the affected downstream modules (classes/methods). Note that a downstream project that uses a buggy upstream function may not be affected as the usage does not satisfy the failure inducing preconditions. For a reported bug with the known root cause function and failure inducing preconditions, we first collect the candidate downstream modules that call the upstream function through an ecosystem-wide dependence analysis. Then, the paths to the call sites of the buggy upstream function are encoded as symbolic constraints. Solving the constraints, together with the failure inducing preconditions, identifies the affected downstream modules. Our evaluation of 31 existing upstream bugs on the scientific Python ecosystem containing 121 versions of 22 popular projects (with a total of 16 millions LOC) shows that the approach is highly effective: from the 25490 candidate downstream modules that invoke the buggy upstream functions, it identifies 1132 modules where the upstream bugs can be triggered, pruning 95.6% of the candidates. The technique has no false negatives and an average false positive rate of 7.9%. Only 49 downstream modules (out of the 1132 we found) were reported before to be affected.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {100–111},
numpages = {12},
keywords = {bug impact, dependence analysis, cross-project bugs, symbolic constraints, software ecosystems},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/2542651,
author = {Mitropoulos, Dimitris},
title = {Security Bugs in Large Software Ecosystems},
year = {2013},
issue_date = {Winter 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/2542651},
doi = {10.1145/2542651},
abstract = {The XRDS blog highlights a range of topics from security and privacy to neuroscience. Selected blog posts, edited for print, will be featured in every issue. Please visit xrds.acm.org/blog to read each post in its entirety. Keeping with our theme of professional development, included is a guest post on how to craft a publishable research paper.},
journal = {XRDS},
month = {dec},
pages = {15–16},
numpages = {2}
}

@inproceedings{10.1145/2950290.2950325,
author = {Bogart, Christopher and K\"{a}stner, Christian and Herbsleb, James and Thung, Ferdian},
title = {How to Break an API: Cost Negotiation and Community Values in Three Software Ecosystems},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950325},
doi = {10.1145/2950290.2950325},
abstract = {Change introduces conflict into software ecosystems: breaking changes may ripple through the ecosystem and trigger rework for users of a package, but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs. We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change, Eclipse, R/CRAN, and Node.js/npm, to understand how developers make decisions about change and change-related costs and what practices, tooling, and policies are used. We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem. Our results illustrate that there is a large design space in how to build an ecosystem, its policies and its supporting infrastructure; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {109–120},
numpages = {12},
keywords = {Software ecosystems, Dependency management, semantic versioning, Qualitative research, Collaboration},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3236405.3236425,
author = {Hinterreiter, Daniel},
title = {Supporting Feature-Oriented Development and Evolution in Industrial Software Ecosystems},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236425},
doi = {10.1145/3236405.3236425},
abstract = {Companies nowadays need to serve a mass market while at the same time customers request highly individual solutions. To handle this problem, development is frequently organized in software ecosystems (SECOs), i.e., interrelated software product lines involving internal and external developers. Individual products for customers are derived and adapted by adding new features or creating new versions of existing features to meet the customer-specific requirements. Development teams typically use version control systems to track fine-grained, implementation-level changes to product lines and products. However, it is difficult to relate such low-level changes to features and their evolution in the SECO. State-of-the-art approaches addressing this issue are variation control systems, which allow tracking of changes at the level of features. However, these systems have not found their way into mainstream development so far. In this thesis we will describe which workflows and additions to variation control systems are required to support feature-oriented development in an industrial SECO environment. We will further investigate mechanisms that support feature-based monitoring to guide the evolution in SECOs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {79–86},
numpages = {8},
keywords = {software evolution, variation control systems, software product lines, configuration management},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3550355.3552413,
author = {Borum, Holger Stadel and Seidl, Christoph},
title = {Survey of Established Practices in the Life Cycle of Domain-Specific Languages},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552413},
doi = {10.1145/3550355.3552413},
abstract = {Domain-specific languages (DSLs) have demonstrated their usefulness within many domains such as finance, robotics, and telecommunication. This success has been exemplified by the publication of a wide range of articles regarding specific DSLs and their merits in terms of improved software quality, programmer efficiency, security, etc. However, there is little public information on what happens to these DSLs after they are developed and published. The lack of information makes it difficult for a DSL practitioner or tool creator to identify trends, current practices, and issues within the field. In this paper, we seek to establish the current state of a DSL's life cycle by analysing 30 questionnaire answers from DSL authors on the design and development, launch, evolution, and end of life of their DSL. On this empirical foundation, we make six recommendations to DSL practitioners, scholars, and tool creators on the subjects of user involvement in the design process, DSL evolution, and the end of life of DSLs.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {266–277},
numpages = {12},
keywords = {domain-specific languages, survey},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/1862372.1862384,
author = {Goeminne, Mathieu and Mens, Tom},
title = {A Framework for Analysing and Visualising Open Source Software Ecosystems},
year = {2010},
isbn = {9781450301282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1862372.1862384},
doi = {10.1145/1862372.1862384},
abstract = {Nowadays, most empirical studies in open source software evolution are based on the analysis of program code alone. In order to get a better understanding of how software evolves over time, many more entities that are part of the software ecosystem need to be taken into account. We present a general framework to automate the analysis of the evolution of software ecosystems. The framework incorporates a database that stores all relevant information obtained thanks to several mining tools, and provides a unified data source to visualisation tools. One such visualisation tool is integrated in order to get a first quick overview of the evolution of different aspects of the software project under study. The framework is extensible in order to accommodate more and different types of input and output, depending on the needs of the user. We compare our framework against existing solutions, and show how we can use this framework for carrying out concrete ecosystem evolution experiments.},
booktitle = {Proceedings of the Joint ERCIM Workshop on Software Evolution (EVOL) and International Workshop on Principles of Software Evolution (IWPSE)},
pages = {42–47},
numpages = {6},
location = {Antwerp, Belgium},
series = {IWPSE-EVOL '10}
}

@article{10.1145/3418209,
author = {Boldi, Paolo and Gousios, Georgios},
title = {Fine-Grained Network Analysis for Modern Software Ecosystems},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418209},
doi = {10.1145/3418209},
abstract = {Modern software development is increasingly dependent on components, libraries, and frameworks coming from third-party vendors or open-source suppliers and made available through a number of platforms (or forges). This way of writing software puts an emphasis on reuse and on composition, commoditizing the services that modern applications require. On the other hand, bugs and vulnerabilities in a single library living in one such ecosystem can affect, directly or by transitivity, a huge number of other libraries and applications. Currently, only product-level information on library dependencies is used to contain this kind of danger, but this knowledge often reveals itself too imprecise to lead to effective (and possibly automated) handling policies. We will discuss how fine-grained function-level dependencies can greatly improve reliability and reduce the impact of vulnerabilities on the whole software ecosystem.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {1},
numpages = {14},
keywords = {network analysis, security breaches, Software reuse}
}

@inproceedings{10.5555/3104068.3104081,
author = {Coutinho, Emanuel Ferreira and Viana, Davi and dos Santos, Rodrigo Pereira},
title = {An Exploratory Study on the Need for Modeling Software Ecosystems: The Case of SOLAR SECO},
year = {2017},
isbn = {9781538604267},
publisher = {IEEE Press},
abstract = {Software-intensive systems have become increasingly ubiquitous, large, and complex, with dissemination in several application domains and tightly dependent upon different technologies. Such systems are usually centered in a software platform so that increasing attention has been paid to influence and interdependency in relationships among all the involved players, forming software ecosystems (SECO). Despite the initial advances in SECO research, few analytical models, case studies with real data, and integrated tool support exist. A great barrier for the evolution of the field towards aiding decision-making in the real industry is the lack of SECO modeling support. In this paper, we aim to perform an exploratory study on the need for modeling in the SECO field. We preliminarily identified some modeling elements from the SECO literature and explored them in the context of a real SECO in the educational domain.},
booktitle = {Proceedings of the 9th International Workshop on Modelling in Software Engineering},
pages = {47–53},
numpages = {7},
keywords = {e-learning, software supply network, virtual learning environment, software ecosystem, modeling},
location = {Buenos Aires, Argentina},
series = {MISE '17}
}

@inproceedings{10.1145/2635868.2635876,
author = {Schultis, Klaus-Benedikt and Elsner, Christoph and Lohmann, Daniel},
title = {Architecture Challenges for Internal Software Ecosystems: A Large-Scale Industry Case Study},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635876},
doi = {10.1145/2635868.2635876},
abstract = {The idea of software ecosystems encourages organizations to open software projects for external businesses, governing the cross-organizational development by architectural and other measures. Even within a single organization, this paradigm can be of high value for large-scale decentralized software projects that involve various internal, yet self-contained organizational units. However, this intra-organizational decentralization causes architecture challenges that must be understood to reason about suitable architectural measures. We present an in-depth case study on collaboration and architecture challenges in two of these large-scale software projects at Siemens. We performed a total of 46 hours of semi-structured interviews with 17 leading software architects from all involved organizational units. Our major findings are: (1) three collaboration models on a continuum that ranges from high to low coupling, (2) a classification of architecture challenges, together with (3) a qualitative and quantitative exposure of the identified recurring issues along each collaboration model. Our study results provide valuable insights for both industry and academia: Practitioners that find themselves in one of the collaboration models can use empirical evidence on challenges to make informed decisions about counteractive measures. Researchers can focus their attention on challenges faced by practitioners to make software engineering more effective.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {542–552},
numpages = {11},
keywords = {software architecture, Software ecosystem, software product line, case study, decentralized software engineering, collaboration},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2851613.2851756,
author = {Valen\c{c}a, George and Alves, Carina},
title = {Understanding How Power Influences Business and Requirements Decisions in Software Ecosystems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851756},
doi = {10.1145/2851613.2851756},
abstract = {This paper investigates partnerships between Small and Medium Enterprises building a software ecosystem. We aimed to analyse how power-dependence relations affect business and requirements decisions taken together by partners. We addressed this goal by performing an exploratory case study of five software companies in an emergent software ecosystem. Based on established theories on power, we describe and analyse companies' interactions in this specific context. We believe that by exploring this subject we can support partner firms to understand power configuration in the network and balance opposing forces while evolving their integrated products.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1258–1263},
numpages = {6},
keywords = {power, software product management, software ecosystem, small and medium enterprises},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/1810295.1810356,
author = {Lungu, Mircea and Lanza, Michele},
title = {The Small Project Observatory: A Tool for Reverse Engineering Software Ecosystems},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810356},
doi = {10.1145/1810295.1810356},
abstract = {Software evolution researchers have focused mostly on analyzing single software systems. However, often projects are developed and co-exist within software ecosystems, i.e., the larger contexts of companies, research groups or open-source communities. We present The Small Project Observatory, a web-based analysis platform for ecosystem reverse engineering through interactive visualization and exploration.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {289–292},
numpages = {4},
keywords = {software visualization, mining software repositories, software evolution, software ecosystems},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@proceedings{10.1145/3528229,
title = {SESoS '22: Proceedings of the 10th IEEE/ACM International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2022},
isbn = {9781450393348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2022 provides researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for development and evolution of complex software-intensive systems, inspiring visions for the future of Software Engineering for Systems-of-Systems (SoS) and Software Ecosystems (SECO), as well as paving the way for a more structured community effort.},
location = {Pittsburgh, Pennsylvania}
}

@article{10.1145/3447245,
author = {Bogart, Chris and K\"{a}stner, Christian and Herbsleb, James and Thung, Ferdian},
title = {When and How to Make Breaking Changes: Policies and Practices in 18 Open Source Software Ecosystems},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3447245},
doi = {10.1145/3447245},
abstract = {Open source software projects often rely on package management systems that help projects discover, incorporate, and maintain dependencies on other packages, maintained by other people. Such systems save a great deal of effort over ad hoc ways of advertising, packaging, and transmitting useful libraries, but coordination among project teams is still needed when one package makes a breaking change affecting other packages. Ecosystems differ in their approaches to breaking changes, and there is no general theory to explain the relationships between features, behavioral norms, ecosystem outcomes, and motivating values. We address this through two empirical studies. In an interview case study, we contrast Eclipse, NPM, and CRAN, demonstrating that these different norms for coordination of breaking changes shift the costs of using and maintaining the software among stakeholders, appropriate to each ecosystem’s mission. In a second study, we combine a survey, repository mining, and document analysis to broaden and systematize these observations across 18 ecosystems. We find that all ecosystems share values such as stability and compatibility, but differ in other values. Ecosystems’ practices often support their espoused values, but in surprisingly diverse ways. The data provides counterevidence against easy generalizations about why ecosystem communities do what they do.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {42},
numpages = {56},
keywords = {semantic versioning, collaboration, qualitative research, dependency management, Software ecosystems}
}

@inproceedings{10.1145/3067695.3082525,
author = {Landsborough, Jason and Harding, Stephen and Fugate, Sunny},
title = {Learning from Super-Mutants: Searching Post-Apocalyptic Software Ecosystems for Novel Semantics-Preserving Transforms},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3082525},
doi = {10.1145/3067695.3082525},
abstract = {In light of recent advances in genetic-algorithm-driven automated program modification, our team has been actively exploring the art, engineering, and discovery of novel semantics-preserving transforms. While modern compilers represent some of the best ideas we have for automated program modification, current approaches represent only a small subset of the types of transforms which can be achieved. In the wilderness of post-apocalyptic software ecosystems of genetically-modified and mutant programs, there exist a broad array of potentially useful software mutations, including semantics-preserving transforms that may play an important role in future software design, development, and most importantly, evolution.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1529–1536},
numpages = {8},
keywords = {search based software engineering, genetic improvement, validation, genetic algorithm},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.1145/2536146.2536159,
author = {Costa, Gabriella and Silva, Felyppe and Santos, Rodrigo and Werner, Cl\'{a}udia and Oliveira, Toacy},
title = {From Applications to a Software Ecosystem Platform: An Exploratory Study},
year = {2013},
isbn = {9781450320047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536146.2536159},
doi = {10.1145/2536146.2536159},
abstract = {The essence of the software ecosystem concept encourages external developers to interact with a software platform, enabling them to contribute or evolve such platform. Trying to expand a product into a platform on which components, services and applications can be created, a company may provide the basis for an ecosystem and its life cycle. In this paper, we describe the evolution process of an application towards an ecosystem using current models and taking into account the reuse of an existing software system. The application DiaDia has been developed for smartphones and its main function consists in helping the treatment of diabetic patients. Two different models are used to discuss the transition of the application to an ecosystem: one is an adaptation of the Software Supply Network which represents the main actors that interact with the ecosystem, and the other covers the process of creating an application and its evolution to an ecosystem through Business Process Model and Notation.},
booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
pages = {9–16},
numpages = {8},
keywords = {ecosystems modeling, mobile applications, software reuse, software ecosystems, software life cycle},
location = {Luxembourg, Luxembourg},
series = {MEDES '13}
}

@inproceedings{10.1145/3236024.3236062,
author = {Valiev, Marat and Vasilescu, Bogdan and Herbsleb, James},
title = {Ecosystem-Level Determinants of Sustained Activity in Open-Source Projects: A Case Study of the PyPI Ecosystem},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236062},
doi = {10.1145/3236024.3236062},
abstract = {Open-source projects do not exist in a vacuum. They benefit from reusing other projects and themselves are being reused by others, creating complex networks of interdependencies, i.e., software ecosystems. Therefore, the sustainability of projects comprising ecosystems may no longer by determined solely by factors internal to the project, but rather by the ecosystem context as well.  In this paper we report on a mixed-methods study of ecosystem-level factors affecting the sustainability of open-source Python projects. Quantitatively, using historical data from 46,547 projects in the PyPI ecosystem, we modeled the chances of project development entering a period of dormancy (limited activity) as a function of the projects' position in their dependency networks, organizational support, and other factors. Qualitatively, we triangulated the revealed effects and further expanded on our models through interviews with project maintainers. Results show that the number of project ties and the relative position in the dependency network have significant impact on sustained project activity, with nuanced effects early in a project's life cycle and later on.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {644–655},
numpages = {12},
keywords = {Open Source, Software ecosystems, Survival modeling},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2797433.2797477,
author = {Syeed, M. M. Mahbubul and Lokhman, Alexander and Mikkonen, Tommi and Hammouda, Imed},
title = {Pluggable Systems as Architectural Pattern: An Ecosystemability Perspective},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797477},
doi = {10.1145/2797433.2797477},
abstract = {In this paper we review the use of plug-in architectures as a technological platform for software ecosystems. Our observation is that the software community has viewed and used plug-ins as powerful extension mechanisms offering a wide range of quality properties. Looking beyond such low-level technical interpretation, we argue that pluggable systems should be perceived and treated as a higher level architectural pattern. In order to back our perspective we present the pattern following widely adopted documentation scheme, we show example usage of the pattern in the Eclipse ecosystem, and we discuss different implementation options of the pattern when building new technical solutions for ecosystems.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {42},
numpages = {6},
keywords = {software ecosystems, architectural patterns, Plug-in systems},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/2364412.2364452,
author = {Berger, Thorsten},
title = {Variability Modeling in the Wild},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364452},
doi = {10.1145/2364412.2364452},
abstract = {Variability modeling is one of the key disciplines in software product line engineering and has been addressed by academic and industrial research over the past twenty years. While the research community's focus was on creating notations and tools, most of which based on feature modeling, there are relatively few empirical studies that aim at understanding the actual use of these techniques.In this light, we present empirical work that investigates variability modeling in the context of software product lines. We study concepts and semantics of real-world variability languages and the usage of these concepts in real, large-scale variability models. We further extend our discussion to variability in software ecosystems, which target inter-organizational reuse and are often seen as natural successors of software product lines. We provide empirical evidence that the well-researched concepts of feature modeling are used in practice, but also that more advanced concepts are needed. We observe that some assumptions about realistic variability models in the literature do not hold. Further, our findings indicate that variability models are not suited for software ecosystems, and that particular kinds of dependencies are needed to enable growth of such ecosystems.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {233–241},
numpages = {9},
keywords = {variability modeling, empirical software engineering, software product lines, software ecosystems},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1145/3571848,
author = {Alfadel, Mahmoud and Costa, Diego Elias and Shihab, Emad and Adams, Bram},
title = {On the Discoverability of Npm Vulnerabilities in Node.Js Projects},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571848},
doi = {10.1145/3571848},
abstract = {The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. Therefore, we perform a large-scale empirical study examining 6,546 Node.js applications. We define three discoverability levels based on vulnerabilities lifecycle (undisclosed, reported, and public). We find that although the majority of the affected applications (99.42%) depend on undisclosed vulnerable packages, 206&nbsp;(4.63%) applications were exposed to dependencies with public vulnerabilities. The major culprit for the applications being affected by public vulnerabilities is the lack of dependency updates; in 90.8% of the cases, a fix is available but not patched by application maintainers. Moreover, we find that applications remain affected by public vulnerabilities for a long time (103 days). Finally, we devise DepReveal, a tool that supports our discoverability analysis approach, to help developers better understand vulnerabilities in their application dependencies and plan their project maintenance.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {91},
numpages = {27},
keywords = {software packages, dependency vulnerabilities, software ecosystems, Open source software}
}

@inproceedings{10.1145/3555776.3577702,
author = {Buhnova, Barbora and Halasz, David and Iqbal, Danish and Bangui, Hind},
title = {Survey on Trust in Software Engineering for Autonomous Dynamic Ecosystems},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577702},
doi = {10.1145/3555776.3577702},
abstract = {Software systems across various application domains are undergoing a major shift, from static systems of systems to dynamic ecosystems characterized by largely autonomous software agents, engaging in mutual coalitions and partnerships to complete complex collaborative tasks. One of the key challenges facing software engineering along with this shift, is our preparedness to leverage the concept of mutual trust building among the dynamic system components, to support safe collaborations with (possibly malicious or misbehaving) components outside the boundaries of our control. To support safe evolution towards dynamic software ecosystems, this paper examines the current progress in the research on trust in software engineering across various application domains. To this end, it presents a survey of existing work in this area, and suggests the directions in which further research is needed. These directions include the research of social metrics supporting trust assessment, fine-grained quantification of trust-assessment results, and opening the discussion on governance mechanisms responsible for trust-score management and propagation across the integrated software ecosystems.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1490–1497},
numpages = {8},
keywords = {trust, autonomous systems, dynamic software ecosystems, software engineering, survey},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3422392.3422458,
author = {Santos, Italo and Coutinho, Emanuel F. and Souza, Simone R. S.},
title = {Software Testing Ecosystems Insights and Research Opportunities},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422458},
doi = {10.1145/3422392.3422458},
abstract = {[Context] Software testing research is a robust field of study with a lot of research that aims to improve testing activities. There are several related elements in the testing activity that can be further investigated through the concepts defined in the software ecosystem (SECO) research. The study of testing and joint SECO addresses challenges and opportunities that go forward application development in a project, through an entire development network from suppliers to stakeholders. [Objective] In this paper, we propose a definition of software testing ecosystem (STECO) and report research opportunities, to bring close these two different research fields fostering the conduction of new research at the intersection of these two topics to investigate the elements of testing activity that constitute an ecosystem and how it can be tested to improve software quality. [Method] The concept of STECO is defined and we perform a literature search. Hence, to illustrate the idea of a STECO, we designed a model to describe the elements involved around the JUnit SECO and describe some research opportunities considering the knowledge of the testing and SECO fields. [Results and Conclusions] The following research opportunities are highlighted to foster new research works: (i) the conduction of functional and nonfunctional requirements testing, as well as the challenges imposed from this activity; (ii) the need for testing products developed separated from the central SECO platform; (iii) conduction of studies on testing automation on SECO platforms; (iv) testing between the different technologies that composed a SECO; and (v) testing the mobile software ecosystem (MSECO) environment, that has been presenting a fast growth in the number of applications.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {421–426},
numpages = {6},
keywords = {software ecosystems, Software testing, research opportunities},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3284179.3284263,
author = {Marcos-Pablos, Samuel and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}},
title = {Trends in European Research Projects Focused on Technological Ecosystems in the Health Sector},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284263},
doi = {10.1145/3284179.3284263},
abstract = {Over the past decade, the health domain has grown at a fast pace. The stakeholders are not only limited to patients, but also include formal and informal careers, doctors, research institutions and technological solution providers. As such, different technological ecosystems of interconnected health communities have arisen to adopt the best practices to improve the wellbeing and health of patients. In order to identify the lacks and opportunities in this area, this paper aims at providing a comprehensive overview of the ecosystems in the health domain, presenting a systematic mapping study of research projects developed in Europe and related to the field. The systematic mapping review was conducted on the AAL Programme, CORDIS and KEEP databases. The paper describes the methodology employed for conducting such a review, and provides an analysis of results that give an overview of the evolution of related European projects until today along with the conclusions obtained from the study.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {495–503},
numpages = {9},
keywords = {technological ecosystems, software ecosystems, systematic literature, health sector, European projects, systematic mapping},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/2797433.2797481,
author = {Manikas, Konstantinos and Kontogiorgos, Dimosthenis},
title = {Characterizing Software Activity: The Influence of Software to Ecosystem Health},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797481},
doi = {10.1145/2797433.2797481},
abstract = {The health of a software ecosystem reflects the ability of the ecosystem to endure and remain variable and productive over time. Measurements of health in a software ecosystem are applied to inform on how the ecosystem is evolving, evaluate changes, and predict future states. In this study we investigate the influence of software to the overall health of the ecosystem. We propose an approach of measuring the activity of the ecosystem over time and identifying the influence to health. We do this in two ways: (i) we study the evolution of the software network over time to identify changes in the structure of the software network and investigate whether they relate to general changes in the ecosystem. (ii) We propose the identification of the influence of the independent software components to ecosystem health at an activity level. We do so by defining keystone and dominator activities and propose tentative means of measuring them. We apply our proposed approach to the platform of the Apache Cordova ecosystem, an ecosystem with a community-based platform and independent contributions. Our analysis identifies two points in time where the ecosystem is under major change. These points are confirmed independently by both the measures of software network and keystone and dominator activities.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {46},
numpages = {6},
keywords = {ecosystem health, software ecosystems, graph analysis, software activity},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3362789.3362949,
author = {Marcos-Pablos, Samuel and Garc\'{\i}a-Holgado, Alicia and Garc\'{\i}a-Pe\~{n}alvo, Francisco J.},
title = {Modelling the Business Structure of a Digital Health Ecosystem},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362949},
doi = {10.1145/3362789.3362949},
abstract = {The current trend in digital solutions for the health sector is to move from fragmented services to progressively more integrated services provided by multiple stakeholders through technological ecosystem platforms. However, the business model is scarcely taken into account at the early stages of development of this type of ecosystems specially in the health sector. In the present paper a general approach towards the exploitation of a technological ecosystem focused on caregivers is presented. It follows the Business Process Model and Notation (BPMN) in order to develop different ecosystem's exploitation alternatives, taking into account the ecosystem stakeholders and their main value propositions. This serves as a starting data model in the software development process from which different business exploitation alternatives can be elaborated.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {838–846},
numpages = {9},
keywords = {software ecosystems, BPMN, health sector, Technological ecosystems, health ecosystems, business modelling},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/2993412.3003389,
author = {Lungu, Mircea F.},
title = {Bootstrapping an Ubiquitous Monitoring Ecosystem for Accelerating Vocabulary Acquisition},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003389},
doi = {10.1145/2993412.3003389},
abstract = {Learning the vocabulary of a new language is a very slow and time consuming process which can take many years of dedicated study. Free reading is known to be important for improving vocabulary and so are optimally timed repetitions of learned concepts. However, these two have not been put together until now.This paper presents the architecture of a monitoring ecosystem of applications which tracks the reading activities of a learner and builds a model of their evolving knowledge. Based on this model it can steer their future reading and studying sessions in such a way as to accelerate the speed with which they acquire new vocabulary.The paper describes several requirements for such an ecosystem, together with a prototpye implementation, and component applications. Finally a series of open questions that highlight opportunities for future research are discussed.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {28},
numpages = {4},
keywords = {software ecosystems, applied linguistics, HCI},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1109/ICSE43902.2021.00121,
author = {Ferreira, Gabriel and Jia, Limin and Sunshine, Joshua and K\"{a}stner, Christian},
title = {Containing Malicious Package Updates in Npm with a Lightweight Permission System},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00121},
doi = {10.1109/ICSE43902.2021.00121},
abstract = {The large amount of third-party packages available in fast-moving software ecosystems, such as Node.js/npm, enables attackers to compromise applications by pushing malicious updates to their package dependencies. Studying the npm repository, we observed that many packages in the npm repository that are used in Node.js applications perform only simple computations and do not need access to filesystem or network APIs. This offers the opportunity to enforce least-privilege design per package, protecting applications and package dependencies from malicious updates. We propose a lightweight permission system that protects Node.js applications by enforcing package permissions at runtime. We discuss the design space of solutions and show that our system makes a large number of packages much harder to be exploited, almost for free.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1334–1346},
numpages = {13},
keywords = {permission system, security, design trade-offs, package management, sand-boxing, supply-chain security, malicious package updates},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2701319.2701331,
author = {Lettner, Daniela and Gr\"{u}nbacher, Paul},
title = {Using Feature Feeds to Improve Developer Awareness in Software Ecosystem Evolution},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701331},
doi = {10.1145/2701319.2701331},
abstract = {In many domains organizations need to serve a mass market while at the same time customers request highly individual solutions. Companies thus form software ecosystems (SECOs) comprising various related hardware and software product lines (SPLs). Technology changes, internal enhancements, and customer requests drive the evolution of such SECOs. Multiple projects are conducted in parallel to deliver customized solutions to customers. Developers often adhere to a staged configuration process: first, required software components are selected to derive an initial product, which is then evolved by refining features and adapting source code to meet customer requirements. These customer-specific solutions are often created using a clone-and-own approach and typically contain features potentially reusable in other solutions. However, the awareness of developers about such platform extensions is typically low and feedback from products to SPLs is often lacking. In this research-in-progress paper we thus present a publish-subscribe approach fostering the awareness about feature implementations in SECOs. The approach is based on feature feeds and SECO awareness models.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {11–18},
numpages = {8},
keywords = {evolution, variability models, Ecosystems, awareness},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/2797433.2797478,
author = {Alami, Daniel and Rodr\'{\i}guez, Mar\'{\i}a and Jansen, Slinger},
title = {Relating Health to Platform Success: Exploring Three E-Commerce Ecosystems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797478},
doi = {10.1145/2797433.2797478},
abstract = {The market for e-commerce systems is saturated with more than 60 different solutions that compete and try to accommodate different needs. However, it appears that three main platforms account for roughly half of the market share: Magento, PrestaShop, and WooCommerce. This paper evaluates these platforms from a software ecosystems health perspective. By shedding light on the success factors of these ecosystems, we aim at establishing what makes an e-commerce ecosystem healthy. This knowledge provides ecosystem orchestrators with an overview of the current health of these ecosystems, and researchers with an application of ecosystem health assessment.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {43},
numpages = {6},
keywords = {Software Ecosystem, Magento, e-Commerce Platform, Health Assessment, WooCommerce, PrestaShop},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.5555/3103196.3103210,
author = {Carvalho, Iuri and Campos, Fernanda and Braga, Regina and David, Jos\'{e} Maria N. and Stroelle, Victor and Ara\'{u}jo, Marco Ant\^{o}nio},
title = {HEAL ME: An Architecture for Health Software Ecosystem Evaluation},
year = {2017},
isbn = {9781538627990},
publisher = {IEEE Press},
abstract = {Software Ecosystems are complex environments. Several actors are grouped on a software platform, facilitating investments in research, development, and reuse of components. However, such investments may be lost if the ecosystem is not solid. Therefore, it is important to assess the health of an ecosystem. This work presents HEAL-ME architecture, which main goal is to evaluate the health of software ecosystems. A pilot case study using a scientific software ecosystem is also presented. Enhancements will be provided for a complete SECO health assessment.},
booktitle = {Proceedings of the Joint 5th International Workshop on Software Engineering for Systems-of-Systems and 11th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {59–65},
numpages = {7},
keywords = {ontology, component --- SECO health, architecture, software ecosystem, quality},
location = {Buenos Aires, Argentina},
series = {JSOS '17}
}

@inproceedings{10.1145/3422392.3422445,
author = {Massanori, Daniel and Cafeo, Bruno B. P. and Wiese, Igor and Font\~{a}o, Awdren},
title = {Death of a Software Ecosystem: A Developer Relations (DevRel) Perspective},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422445},
doi = {10.1145/3422392.3422445},
abstract = {The Developer Relations (DevRel) is a strategy to attract, engage and mature developers in contributing to a platform. It supports the establishment of a Software Ecosystem (SECO). However, even with investments in the DevRel, some organizations face the death of their ecosystems, for example: Symbian (2012), Firefox OS (2016), Windows Phone (2017). It can also be compared based on Ecology to a disruption of the "food chain" that can turn a dynamic ecosystem in a static or dead ecosystem. For example, Microsoft announced in 2017 that Windows Phone would no longer push any updates and became only focusing on maintenance. We want to contribute in understanding how, why and when a SECO is turning on static (i.e., dying) and the "post mortem" status of a SECO. We initially study the Windows Phone from 46,030 questions in Stack Overflow to understand what happens to a SECO when the core platform is discontinued. From our result analysis we perceived that it can be useful to understanding the "vital signals" of ecosystem collapse, migratory/survival patterns, technical resource recycling and the energy transfer among individuals, populations, communities and SECOs. We also contributes with 14 insights.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {399–404},
numpages = {6},
keywords = {Software Repositories, Software Ecosystem, Developer Relations},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/1370700.1370707,
author = {M\"{u}ller, Hausi and Pezz\`{e}, Mauro and Shaw, Mary},
title = {Visibility of Control in Adaptive Systems},
year = {2008},
isbn = {9781605580265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370700.1370707},
doi = {10.1145/1370700.1370707},
abstract = {Adaptive systems respond to changes in their internal state or external environment with guidance from an underlying control system. ULS systems are particularly likely to require dynamic adaptation because of their decentralized control and the large number of independent stakeholders whose actions are integral to the system's behavior. Adaptation may take various forms, but the system structure will almost inevitably include one or more closed feedback loops. We argue that adaptability is a characteristic of a solution, not of a problem, and that the feedback loop governing control of adaptability should be explicit in design and analysis and either explicit or clearly traceable in implementation.},
booktitle = {Proceedings of the 2nd International Workshop on Ultra-Large-Scale Software-Intensive Systems},
pages = {23–26},
numpages = {4},
keywords = {autonomic systems, self-adaptive systems, continuous evolution, ultra-large scale systems, software ecosystems},
location = {Leipzig, Germany},
series = {ULSSIS '08}
}

@inproceedings{10.1145/3194124.3194128,
author = {Hyrynsalmi, Sami and Ruohonen, Jukka and Sepp\"{a}nen, Marko},
title = {Healthy until Otherwise Proven: Some Proposals for Renewing Research of Software Ecosystem Health},
year = {2018},
isbn = {9781450357302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194124.3194128},
doi = {10.1145/3194124.3194128},
abstract = {The software ecosystem has become a central conceptualisation for characterising the contemporary software business world. To understand and evaluate ecosystems, the concept of 'ecosystem health' was borrowed from the field of biology. In a 'healthy' ecosystem, the participants will flourish and, vice versa, suffer in an unhealthy one. Yet, there is a lack of empirical validations for the current approach as well as certain limitations regarding the concept. This paper will present a critique on current ecosystem health measurement and evaluation approaches. In addition, there is discussion on three proposals that could help to refocus the academic research on software ecosystem health.},
booktitle = {Proceedings of the 1st International Workshop on Software Health},
pages = {18–24},
numpages = {7},
keywords = {software ecosystem, platform economy, position paper, ecosystem health, software business ecosystem},
location = {Gothenburg, Sweden},
series = {SoHeal '18}
}

@inproceedings{10.1109/SoHeal.2019.00008,
author = {Charleux, Amel and Viseur, Robert},
title = {Exploring Impacts of Managerial Decisions and Community Composition on the Open Source Projects' Health},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SoHeal.2019.00008},
doi = {10.1109/SoHeal.2019.00008},
abstract = {This research aims to explore the impacts of managerial decisions and community composition on the health of open source communities and ecosystems. Combining a longitudinal single case study to more than 46 interviews with open source community members (editors and contributors), we were able to identify key managerial changes that impact community activity and highlight how certain strategic parameters of the project determine the community composition and its health. In this study, by health we qualify a wide range of aspects going from the dynamisms of external contributions to the valuation of contributors, contributions and dissemination to end users. We firstly show that managerial decisions are as important as technical choices regarding the software health. Some purely managerial decisions regarding governance structures for example can impact the community willingness to contribute and therefore the available resources to develop the software. Secondly, we put forward the importance of having a coopetitive community (i.e. community with several competing firms) to stimulate and attract valuable users and contributors that highly participate to the software health according to our interviewees.},
booktitle = {Proceedings of the 2nd International Workshop on Software Health},
pages = {1–8},
numpages = {8},
keywords = {open source, governance, business models, coopetition},
location = {Montreal, Quebec, Canada},
series = {SoHeal '19}
}

@inproceedings{10.1145/3357384.3357971,
author = {Gong, Qingyuan and Zhang, Jiayun and Chen, Yang and Li, Qi and Xiao, Yu and Wang, Xin and Hui, Pan},
title = {Detecting Malicious Accounts in Online Developer Communities Using Deep Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357971},
doi = {10.1145/3357384.3357971},
abstract = {Online developer communities like GitHub provide services such as distributed version control and task management, which allow a massive number of developers to collaborate online. However, the openness of the communities makes themselves vulnerable to different types of malicious attacks, since the attackers can easily join and interact with legitimate users. In this work, we formulate the malicious account detection problem in online developer communities, and propose GitSec, a deep learning-based solution to detect malicious accounts. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles as well as dynamic activity characteristics. On one hand, GitSec makes use of users' descriptive features from the profiles. On the other hand, GitSec processes users' dynamic behavioral data by constructing two user activity sequences and applying a parallel neural network design to deal with each of them, respectively. An attention mechanism is used to integrate the information generated by the parallel neural networks. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our extensive evaluations show that GitSec is an accurate detection system, with an F1-score of 0.922 and an AUC value of 0.940.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1251–1260},
numpages = {10},
keywords = {deep learning, online developer community, malicious account detection, social networks},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3417113.3422185,
author = {Fan, Zhehao and Feng, Zhiyong and Xue, Xiao and Chen, Shizhan and Wu, Hongyue},
title = {Ecosystem Evolution Analysis and Trend Prediction of Projects in Android Application Framework},
year = {2021},
isbn = {9781450381284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417113.3422185},
doi = {10.1145/3417113.3422185},
abstract = {The application framework layer in the Android system consists of numerous project repositories, which rely on each other to form a co-evolving software ecosystem. Android's application framework layer provides many useful APIs to millions of Android Apps, so its evolution will affect the robustness and stability of Android Apps. Code dependency analysis technology is a common way to analyze software ecosystems. However, the code size of projects in the Android application framework layer is so huge that ordinary analysis methods are unacceptable due to the excessive resources required.In this paper, we propose an approach for evolution analysis and trend prediction based on the subgraph of code dependency network graph, in order to realize the effective analysis of large-scale software ecosystem. Based on the source code data of the application framework collected from AOSP, our proposed approach is verified. The prediction results of our model show that the average values of precision and recall are 90.0% and 90.4% respectively, which proves that our approach can well is effective.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {67–72},
numpages = {6},
keywords = {software ecosystem, trend prediction, code dependency network, evolution analysis},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3533700,
author = {Lin, Jiahuei and Sayagh, Mohammed and Hassan, Ahmed E.},
title = {The Co-Evolution of the WordPress Platform and Its Plugins},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533700},
doi = {10.1145/3533700},
abstract = {One can extend the features of a software system by installing a set of additional components called plugins. WordPress, as a typical example of such plugin-based software ecosystems, is used by millions of websites and has a large number (i.e., 54,777) of available plugins. These plugin-based software ecosystems are different from traditional ecosystems (e.g., NPM dependencies) in the sense that there is high coupling between a platform and its plugins compared to traditional ecosystems for which components might not necessarily depend on each other (e.g., NPM libraries do not depend on a specific version of NPM or a specific version of a client software system). The high coupling between a plugin and its platform and other plugins causes incompatibility issues that occur during the co-evolution of a plugin and its platform as well as other plugins. In fact, incompatibility issues represent a major challenge when upgrading WordPress or its plugins. According to our study of the top 500 most-released WordPress plugins, we observe that incompatibility issues represent the third major cause for bad releases, which are rapidly (within the next 24 hours) fixed via urgent releases. Thirty-two percent of these incompatibilities are between a plugin and WordPress while 19% are between peer plugins. In this article, we study how plugins co-evolve with the underlying platform as well as other plugins, in an effort to understand the practices that are related support such co-evolution and reduce incompatibility issues. In particular, we investigate how plugins support the latest available versions of WordPress, as well as how plugins are related to each other, and how they co-evolve. We observe that a plugin’s support of new versions of WordPress with a large amount of code change is risky, as the releases that declare such support have a higher chance to be followed by an urgent release compared to ordinary releases. Although plugins support the latest WordPress version, plugin developers omit important changes such as deleting the use of removed WordPress APIs, which are removed a median of 873 days after the APIs have been removed from the source code of WordPress. Plugins introduce new releases that are made according to a median of five other plugins, which we refer to as peer-triggered releases. A median of 20% of the peer-triggered releases are urgent releases that fix problems in their previous releases. The most common goal of peer-triggered releases is the fixing of incompatibility issues that a plugin detects as late as after a median of 36 days since the last release of another plugin. Our work sheds light on the co-evolution of WordPress plugins with their platform as well as peer plugins in an effort to uncover the practices of plugin evolution, so WordPress can accordingly design approaches to avoid incompatibility issues.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {19},
numpages = {24},
keywords = {plugins co-evolution, incompatibility issues, Plugin-based ecosystems}
}

@inproceedings{10.1145/3266237.3266243,
author = {Costa, Joenio and Meirelles, Paulo and Chavez, Christina},
title = {On the Sustainability of Academic Software: The Case of Static Analysis Tools},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266243},
doi = {10.1145/3266237.3266243},
abstract = {From 1991 to 2015, 60 papers published in the ASE and SCAM conferences introduced static analysis prototypes or tools as academic software developed to support research. In this study, we characterize such academic software concerning sustainability. We performed an exploratory study regarding publicization (whether the software is available from an explicitly given URL in a software publication), evolution stage (initial development, evolution, servicing, phase-out or close-down), and recognition (the way others mention the software in their papers). Thereby, we discussed the results under the umbrella of software sustainability. Results for the academic software for static analysis published at ASE and SCAM, show that 40% are not publicly available from the URL informed by the authors; 78% are in an initial development stage, discontinued, or closed-down; 23% has no mentions in relevant digital libraries besides the original software publication, but 30% received contributions to their source code. We observed that a large number of academic static analysis software has inadequate publicization, short life cycles and low recognition. A systematic analysis of publicization, software life cycle, and recognition of academic software is viable, and its results may be useful to support rapid decision-making on adopting academic software for use or even as a target for contribution. The results may also promote a more inclusive view of scientific reputation with respect to the academic software produced by researchers.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {202–207},
numpages = {6},
keywords = {publicization of software, academic software, software evolution, technical sustainability, recognition of software},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.5555/3103196.3103215,
author = {Ribeiro, Maria Ivanilse Calderon and Dias-Neto, Arilo Cl\'{a}udio},
title = {Company Health in Mobile Software Ecosystem (MSECO): Research Perspectives and Challenges},
year = {2017},
isbn = {9781538627990},
publisher = {IEEE Press},
abstract = {This paper presents initial perspectives in the search for diagnosing the health of a Mobile Software Ecosystem (MSECO) company. The research intends to investigate a companys health through user reviews about mobile applications (Apps) and build and present indicators that can identify problems, failures and differences in the quality of the apps to guide the company in the search for "immunization" or "medication" so that it does not become "sick" from the adoption of measures related to the use of indicators for the type of "disease".},
booktitle = {Proceedings of the Joint 5th International Workshop on Software Engineering for Systems-of-Systems and 11th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {74–75},
numpages = {2},
keywords = {health, MSECO, indicator, diagnosis},
location = {Buenos Aires, Argentina},
series = {JSOS '17}
}

@inproceedings{10.1145/2901739.2901743,
author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
title = {A Look at the Dynamics of the JavaScript Package Ecosystem},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901743},
doi = {10.1145/2901739.2901743},
abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node. js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {351–361},
numpages = {11},
keywords = {JavaScript, software ecosystem analysis, Node.js},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1145/1943371.1943389,
author = {Krishna, Raj P. M. and Srinivasa, K . G.},
title = {Analysis of Projects and Volunteer Participation in Large Scale Free and Open Source Software Ecosystem},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1943371.1943389},
doi = {10.1145/1943371.1943389},
abstract = {The ecology of Free and Open Source Software (FOSS) is dotted by projects of every kind ranging from small desktop applications to large mission critical systems. To enable maximum visibility among the developer community, these projects are often hosted in community project management portals. The current work studies one such portal, sourceforge. net by analysing the data of 200,000 projects and 2 million developers for the period Feb 2005 to Aug 2009. The scope of the present study includes the analysis of developer contribution. The slow growth rate of developer community and high number of single developer projects are the major findings of the present work.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–5},
numpages = {5}
}

@inproceedings{10.1145/1595800.1595807,
author = {Boucharas, Vasilis and Jansen, Slinger and Brinkkemper, Sjaak},
title = {Formalizing Software Ecosystem Modeling},
year = {2009},
isbn = {9781605586779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595800.1595807},
doi = {10.1145/1595800.1595807},
abstract = {Currently there is no formal modeling standard for software ecosystems that models both the ecosystem and the environment in which software products and services operate. Major implications are (1) software vendors have trouble distinguishing the specific software ecosystems in which they are active and (2) they have trouble using these ecosystems to their strategic advantage. In this paper we present and formalize a standards-setting approach to software product and software supply network modeling. Applying this approach enables software vendors to communicate about relationships in the software supply network, theorize about weak spots/links in their business model and anticipate upcoming changes in the software ecosystem.},
booktitle = {Proceedings of the 1st International Workshop on Open Component Ecosystems},
pages = {41–50},
numpages = {10},
keywords = {software supply network, software ecosystem modeling, product deployment context},
location = {Amsterdam, The Netherlands},
series = {IWOCE '09}
}

@inproceedings{10.1145/2556624.2556625,
author = {Seidl, Christoph and Schaefer, Ina and A\ss{}mann, Uwe},
title = {Capturing Variability in Space and Time with Hyper Feature Models},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556625},
doi = {10.1145/2556624.2556625},
abstract = {Software product lines (SPLs) and software ecosystems (SECOs) are approaches to capturing families of closely related software systems in terms of common and variable functionality. SPLs and especially SECOs are subject to evolution to adapt to new or changed requirements resulting in different versions of the software family and its variable assets. These versions may have to be maintained and used for products even after they were superseded by newer versions. Variability models describing valid combinations of variable assets, such as feature models, capture variability in space (configuration), but not variability in time (evolution) making it impossible to respect versions of variable assets in product definitions on a conceptual level. In this paper, we propose Hyper Feature Models (HFMs) explicitly providing feature versions as configurable units for product definition. Furthermore, we provide a version-aware constraint language to specify dependencies between features and ranges of feature versions as well as a procedure to automatically select valid combinations of versions for a pre-configuration of features. We demonstrate our approach in a case study.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {8},
keywords = {software product line (SPL), software ecosystem (SECO), constraint, version, variability in time, hyper feature model (HFM), evolution},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1145/3378064,
author = {Bozan, Karoly and Lyytinen, Kalle and Rose, Gregory M.},
title = {How to Transition Incrementally to Microservice Architecture},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3378064},
doi = {10.1145/3378064},
abstract = {A field study examines technological advances that have created versatile software ecosystems to develop and deploy microservices.},
journal = {Commun. ACM},
month = {dec},
pages = {79–85},
numpages = {7}
}

@inproceedings{10.1145/2393596.2393662,
author = {Robbes, Romain and Lungu, Mircea and R\"{o}thlisberger, David},
title = {How Do Developers React to API Deprecation? The Case of a Smalltalk Ecosystem},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393662},
doi = {10.1145/2393596.2393662},
abstract = {When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation---known as a ripple effect---is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes.Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on an empirical study of API deprecations that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating around the Squeak and Pharo software ecosystems: seven years of evolution, more than 3,000 contributors, and more than 2,600 distinct systems. We analyzed 577 methods and 186 classes that were deprecated, and answer research questions regarding the frequency, magnitude, duration, adaptation, and consistency of the ripple effects triggered by API changes.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {56},
numpages = {11},
keywords = {ecosystems, empirical studies, mining software repositories},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/2565585.2565598,
author = {Challen, Geoffrey and Haseley, Scott and Maiti, Anudipa and Nandugudi, Anandatirtha and Prasad, Guru and Puri, Mukta and Wang, Junfei},
title = {The Mote is Dead: Long Live the Discarded Smartphone!},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565598},
doi = {10.1145/2565585.2565598},
abstract = {As the rapid pace of smartphone improvements drives consumer appetites for the latest and greatest devices, the hidden cost is millions of tons of e-waste containing hazardous chemicals that are difficult to dispose of safely. Studies show that smartphone users are replacing their devices every 18 months, almost three times faster than desktop computers [1, 3], producing millions of discarded smartphones each year that end up lying in desk drawers, buried in landfills, or shipped to third-world countries where they are burned to extract precious metals, a process that damages both the health of those involved and the environment.Fortunately, the capabilities of discarded smartphones make them ideal for reuse. Instead of ending up in a landfill, a discarded smartphone could be integrated into a home security system or transformed into a health care device for the elderly. In this paper, we evaluate using discarded smartphones to replace traditional sensor network "motes". Compared with motes, discarded devices have many advantages: price, performance, connectivity, interfaces, and ease of programming. While the main question is whether their energy consumption is low enough to enable harvesting solutions to allow continuous operation, we present preliminary results indicating that this may be possible.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {5},
numpages = {6},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inproceedings{10.1145/2599990.2600009,
author = {Teixeira, Jose and Lin, Tingting},
title = {Collaboration in the Open-Source Arena: The Webkit Case},
year = {2014},
isbn = {9781450326254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2599990.2600009},
doi = {10.1145/2599990.2600009},
abstract = {In an era of software crisis, the move of firms towards distributed software development teams is being challenged by emerging collaboration issues. On this matter, the open-source phenomenon may shed some light, as successful cases on distributed collaboration in the open-source community have been recurrently reported. In this paper, we explore the collaboration networks in the WebKit open-source project, by mining WebKit's source-code version-control-system data with Social Network Analysis (SNA). Our approach allows us to observe how key events in the mobile-device industry have affected the WebKit collaboration network over time. With our findings, we show the explanation power from network visualizations capturing the collaborative dynamics of a high-networked software project over time; and highlight the power of the open-source fork concept as a nexus enabling both features of competition and collaboration. We also reveal the WebKit project as a valuable research site manifesting the novel notion of open-coopetition, where rival firms collaborate with competitors in the open-source community.},
booktitle = {Proceedings of the 52nd ACM Conference on Computers and People Research},
pages = {121–129},
numpages = {9},
keywords = {free-software, software ecosystem, coopetition, open-coopetition, distributed software development, webkit, open-source},
location = {Singapore, Singapore},
series = {SIGSIM-CPR '14}
}

@inproceedings{10.1145/2024445.2024458,
author = {Lavall\'{e}e, Mathieu and Robillard, Pierre N.},
title = {Causes of Premature Aging during Software Development: An Observational Study},
year = {2011},
isbn = {9781450308489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024445.2024458},
doi = {10.1145/2024445.2024458},
abstract = {Much work has been done on the subject of what happens to software architecture during maintenance activities. There seems to be a consensus that it degrades during the evolution of the software. More recent work shows that this degradation occurs even during development activities: design decisions are either adjusted or forgotten. Some studies have looked into the causes of this degradation, but these have mostly done so at a very high level. This study examines three projects at code level. Three architectural pre-implementation designs are compared with their post-implementation design counterparts, with special attention paid to the causes of the changes. We found many negative changes causing anti-patterns, at the package, class, and method levels. After analysis of the code, we were able to find the specific reasons for the poor design decisions. Although the underlying causes are varied, they can be grouped into three basic categories: knowledge problems, artifact problems, and management problems. This categorization shows that anti-pattern causes are varied and are not all due to the developers. The main conclusion is that promoting awareness of anti-patterns to developers is insufficient to prevent them since some of the causes escape their grasp.},
booktitle = {Proceedings of the 12th International Workshop on Principles of Software Evolution and the 7th Annual ERCIM Workshop on Software Evolution},
pages = {61–70},
numpages = {10},
keywords = {software aging, design erosion, post-implementation design, pre-implementation design},
location = {Szeged, Hungary},
series = {IWPSE-EVOL '11}
}

@inproceedings{10.1145/2597073.2597123,
author = {Mitropoulos, Dimitris and Karakoidas, Vassilios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
title = {The Bug Catalog of the Maven Ecosystem},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597123},
doi = {10.1145/2597073.2597123},
abstract = {Examining software ecosystems can provide the research community with data regarding artifacts, processes, and communities. We present a dataset obtained from the Maven central repository ecosystem (approximately 265GB of data) by statically analyzing the repository to detect potential software bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version we also stored specific metadata such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results, as we show in specific examples.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {372–375},
numpages = {4},
keywords = {Maven Repository, Software Bugs, FindBugs},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1145/3494834.3498568,
author = {Xin, Reynold and McKinney, Wes and Gates, Alan and McCubbin, Chris},
title = {It Takes a Community: The Open-Source Challenge},
year = {2021},
issue_date = {September-October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/3494834.3498568},
doi = {10.1145/3494834.3498568},
abstract = {Of the many challenges faced by open-source developers, among the most daunting are some that other programmers scarcely ever think about. Building a successful open-source community depends on many different elements, some of which are familiar to any developer. Just as important are the skills to recruit, to inspire, to mentor, to manage, and to mediate disputes. But what exactly does it take to pull all that off?},
journal = {Queue},
month = {nov},
pages = {115–136},
numpages = {22}
}

@inproceedings{10.1145/3136040.3136051,
author = {Al-Kofahi, Jafar M. and Kothari, Suresh and K\"{a}stner, Christian},
title = {Four Languages and Lots of Macros: Analyzing Autotools Build Systems},
year = {2017},
isbn = {9781450355247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136040.3136051},
doi = {10.1145/3136040.3136051},
abstract = {Build systems are crucial for software system development, however there is a lack of tool support to help with their high maintenance overhead. GNU Autotools are widely used in the open source community, but users face various challenges from its hard to comprehend nature and staging of multiple code generation steps, often leading to low quality and error-prone build code. In this paper, we present a platform, AutoHaven, to provide a foundation for developers to create analysis tools to help them understand, maintain, and migrate their GNU Autotools build systems. Internally it uses approximate parsing and symbolic analysis of the build logic. We illustrate the use of the platform with two tools: ACSense helps developers to better understand their build systems and ACSniff detects build smells to improve build code quality. Our evaluation shows that AutoHaven can support most GNU Autotools build systems and can detect build smells in the wild.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {176–186},
numpages = {11},
keywords = {build-system, build maintenance, GNU Autotool, Autoconf},
location = {Vancouver, BC, Canada},
series = {GPCE 2017}
}

@article{10.1145/3470133,
author = {Lamothe, Maxime and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Shang, Weiyi},
title = {A Systematic Review of API Evolution Literature},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470133},
doi = {10.1145/3470133},
abstract = {Recent software advances have led to an expansion of the development and usage of application programming interfaces (APIs). From millions of Android packages (APKs) available on Google Store to millions of open-source packages available in Maven, PyPI, and npm, APIs have become an integral part of software development.Like any software artifact, software APIs evolve and suffer from this evolution. Prior research has uncovered many challenges to the development, usage, and evolution of APIs. While some challenges have been studied and solved, many remain. These challenges are scattered in the literature, which hides advances and cloaks the remaining challenges.In this systematic literature review on APIs and API evolution, we uncover and describe publication trends and trending topics. We compile common research goals, evaluation methods, metrics, and subjects. We summarize the current state-of-the-art and outline known existing challenges as well as new challenges uncovered during this review.We conclude that the main remaining challenges related to APIs and API evolution are (1) automatically identifying and leveraging factors that drive API changes, (2) creating and using uniform benchmarks for research evaluation, and (3) understanding the impact of API evolution on API developers and users with respect to various programming languages.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {171},
numpages = {36},
keywords = {APIs, API evolution, SLR}
}

@article{10.1145/3360613,
author = {Shi, August and Hadzi-Tanovic, Milica and Zhang, Lingming and Marinov, Darko and Legunsen, Owolabi},
title = {Reflection-Aware Static Regression Test Selection},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360613},
doi = {10.1145/3360613},
abstract = {Regression test selection (RTS) aims to speed up regression testing by rerunning only tests that are affected by code changes. RTS can be performed using static or dynamic analysis techniques. Our prior study showed that static and dynamic RTS perform similarly for medium-sized Java projects. However, the results of that prior study also showed that static RTS can be unsafe, missing to select tests that dynamic RTS selects, and that reflection was the only cause of unsafety observed among the evaluated projects.  In this paper, we investigate five techniques—three purely static techniques and two hybrid static-dynamic techniques—that aim to make static RTS safe with respect to reflection. We implement these reflection-aware (RA) techniques by extending the reflection-unaware (RU) class-level static RTS technique in a tool called STARTS. To evaluate these RA techniques, we compare their end-to-end times with RU, and with RetestAll, which reruns all tests after every code change. We also compare safety and precision of the RA techniques with Ekstazi, a state-of-the-art dynamic RTS technique; precision is a measure of unaffected tests selected.  Our evaluation on 1173 versions of 24 open-source Java projects shows negative results. The RA techniques improve the safety of RU but at very high costs. The purely static techniques are safe in our experiments but decrease the precision of RU, with end-to-end time at best 85.8% of RetestAll time, versus 69.1% for RU. One hybrid static-dynamic technique improves the safety of RU but at high cost, with end-to-end time that is 91.2% of RetestAll. The other hybrid static-dynamic technique provides better precision, is safer than RU, and incurs lower end-to-end time—75.8% of RetestAll, but it can still be unsafe in the presence of test-order dependencies. Our study highlights the challenges involved in making static RTS safe with respect to reflection.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {187},
numpages = {29},
keywords = {regression test selection, regression testing, static analysis, reflection, class firewall, string analysis}
}

@inproceedings{10.1145/3340481.3342730,
author = {Petrik, Dimitri and Herzwurm, Georg},
title = {IIoT Ecosystem Development through Boundary Resources: A Siemens MindSphere Case Study},
year = {2019},
isbn = {9781450368544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340481.3342730},
doi = {10.1145/3340481.3342730},
abstract = {Emerging Industrial Internet of Things (iIoT) platforms generate cross-company added value, providing functionalities and technologies for a variety of digital services in the industrial engineering. iIoT platforms integrate various stakeholders, such as end customers and complementors and build iIoT ecosystems. Earlier research has recognized boundary resources as an emergence and governance mechanism for software ecosystems. In this study we apply the boundary resources for iIoT by exploring the longitudinal case study of the Siemens MindSphere ecosystem. The goal of this exploratory paper is to show which boundary resources are currently used in iIoT ecosystems and how do they impact the development of iIoT ecosystems.},
booktitle = {Proceedings of the 2nd ACM SIGSOFT International Workshop on Software-Intensive Business: Start-Ups, Platforms, and Ecosystems},
pages = {1–6},
numpages = {6},
keywords = {Boundary Resources, iIoT Ecosystem, Industrial IoT, iIoT Platform, Platform Emergence},
location = {Tallinn, Estonia},
series = {IWSiB 2019}
}

@inproceedings{10.1145/2377816.2377825,
author = {Passos, Leonardo and Czarnecki, Krzysztof and W\k{a}sowski, Andrzej},
title = {Towards a Catalog of Variability Evolution Patterns: The Linux Kernel Case},
year = {2012},
isbn = {9781450313094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377816.2377825},
doi = {10.1145/2377816.2377825},
abstract = {A complete understanding of evolution of variability requires analysis over all project spaces that contain it: source code, build system and the variability model. Aiming at better understanding of how complex variant-rich software evolve, we set to study one, the Linux kernel, in detail. We qualitatively analyze a number of evolution steps in the kernel history and present our findings as a preliminary sample of a catalog of evolution patterns. Our patterns focus on how the variability evolves when features are removed from the variability model, but are kept as part of the software. The identified patterns relate changes to the variability model, the build system, and implementation code. Despite preliminary, they already indicate evolution steps that have not been captured by prior studies, both empirical and theoretical.},
booktitle = {Proceedings of the 4th International Workshop on Feature-Oriented Software Development},
pages = {62–69},
numpages = {8},
keywords = {software product lines, evolution, variability, Linux, patterns},
location = {Dresden, Germany},
series = {FOSD '12}
}

@inproceedings{10.1145/3387940.3392209,
author = {Zimmermann, Th\'{e}o},
title = {A First Look at an Emerging Model of Community Organizations for the Long-Term Maintenance of Ecosystems' Packages},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392209},
doi = {10.1145/3387940.3392209},
abstract = {One of the biggest strength of many modern programming languages is their rich open source package ecosystem. Indeed, modern language-specific package managers have made it much easier to share reusable code and depend on components written by someone else (often by total strangers). However, while they make programmers more productive, such practices create new health risks at the level of the ecosystem: when a heavily-used package stops being maintained, all the projects that depend on it are threatened. In this paper, I ask three questions. RQ1: How prevalent is this threat? In particular, how many depended-upon packages are maintained by a single person (who can drop out at any time)? I show that this is the case for a significant proportion of such packages. RQ2: How can project authors that depend on a package react to its maintainer becoming unavailable? I list a few options, and I focus in particular on the notion of fork. RQ3: How can the programmers of an ecosystem react collectively to such events, or prepare for them? I give a first look at an emerging model of community organizations for the long-term maintenance of packages, that appeared in several ecosystems.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {711–718},
numpages = {8},
keywords = {open source, package ecosystem, community, fork, maintenance},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2964284.2973806,
author = {Mekuria, Rufael and Cesar, Pablo},
title = {MP3DG-PCC, Open Source Software Framework for Implementation and Evaluation of Point Cloud Compression},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973806},
doi = {10.1145/2964284.2973806},
abstract = {We present MP3DG-PCC, an open source framework for design, implementation and evaluation of point cloud compression algorithms. The framework includes objective quality metrics, lossy and lossless anchor codecs, and a test bench for consistent comparative evaluation. The framework and proposed methodology is in use for the development of an international point cloud compression standard in MPEG. In addition, the library is integrated with the popular point cloud library, making a large number of point cloud processing available and aligning the work with the broader open source community.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1222–1226},
numpages = {5},
keywords = {evaluation, point cloud compression, 3d virtual reality, compression},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/2819366.2819374,
author = {Fontana, Francesca Arcelli and Braione, Pietro and Roveda, Riccardo and Zanoni, Marco},
title = {A Context-Aware Style of Software Design},
year = {2015},
publisher = {IEEE Press},
abstract = {Contemporary large software systems rely on complex software ecosystems for managing infrastructural tasks. While these ecosystems facilitate software development, the software architect must put care in not relying on assumptions on behaviors and policies of the ecosystem that may change with platform evolution. Based on our experience with developing analyses within MARPLE, a framework for software comprehension and architecture reconstruction, we propose an abstract, context-aware style for specifying software. In the spirit of decoupling computation from coordination, this style decouples the specification of the computations to be performed from the specification of the contexts where they must take place. Software described in this way exposes its primitives at a level of abstraction closer to that of the framework, enabling better reasoning on the features of the design, easing correct implementation, and fostering a better interaction between the software and the framework it relies on.},
booktitle = {Proceedings of the Second International Workshop on Context for Software Development},
pages = {15–19},
numpages = {5},
location = {Florence, Italy},
series = {CSD '15}
}

@inproceedings{10.5555/2431518.2431825,
author = {Bizub, Warren and Brandt, Julia},
title = {Transitioning to the next Generation (Nextgen) Defense Training Environment (DTE)},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {Department of Defense (DoD) closed architectures and proprietary solutions limit the ability to provide the warfighter gaming, semantic reasoning and social networking capabilities employed by industry and readily available in the open source community. Exorbitant sustainment costs of legacy solutions are unjustifiable and significantly inhibit transition to enhanced solutions. Additionally, legacy solutions leave a dependence on an aging workforce of static-centric modeling &amp; simulation (M&amp;S) subject matter expertise (SME) to promote reuse, while budget cuts increase attrition among junior-level technical staff. This paper describes challenges and recommendations for changing the DoD M&amp;S training paradigm to facilitate interoperability, incorporate emerging semantic web technologies, and provide a knowledge base to promote reuse. Two ongoing R&amp;D projects will be used to illustrate innovative strategies and their potential to alleviate many legacy system interoperability issues while transitioning to a Defense Training Environment (DTE) where US and Coalition Command and Control (C2) and M&amp;S systems seamlessly interoperate to train as we fight.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2564–2575},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.5555/2819009.2819129,
author = {Tymchuk, Yuriy and Mocci, Andrea and Lanza, Michele},
title = {ViDI: The Visual Design Inspector},
year = {2015},
publisher = {IEEE Press},
abstract = {We present ViDI (Visual Design Inspector), a novel code review tool which focuses on quality concerns and design inspection as its cornerstones. It leverages visualization techniques to represent the reviewed software and augments the visualization with the results of quality analysis tools. To effectively understand the contribution of a reviewer in terms of the impact of her changes on the overall system quality, ViDI supports the recording and further inspection of reviewing sessions. ViDI is an advanced prototype which we will soon release to the Pharo open-source community.Video URL: http://youtu.be/EtdkcNBJAec},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {653–656},
numpages = {4},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1109/ICPC.2019.00047,
author = {Levy, Omer and Feitelson, Dror G.},
title = {Understanding Large-Scale Software: A Hierarchical View},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00047},
doi = {10.1109/ICPC.2019.00047},
abstract = {Program comprehension accounts for a large portion of software development costs and effort. The academic literature contains research on program comprehension of short code snippets, but comprehension at the system level is no less important. We claim that comprehending a software system is a distinct activity that differs from code comprehension. We interview experienced developers, architects, and managers in the software industry and open-source community, to uncover the meaning of program comprehension at the system level. The interviews demonstrate, among other things, that system comprehension is detached from code and programming language, and includes scope that is not captured in the code. It focuses on the structure of the system and less on the code itself. This is a continuous, iterative process, which mixes white-box and blackbox approaches at different layers of the system, and combines both bottom-up and top-down comprehension strategies.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {283–293},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/1806799.1806824,
author = {Di Penta, Massimiliano and German, Daniel M. and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
title = {An Exploratory Study of the Evolution of Software Licensing},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806824},
doi = {10.1145/1806799.1806824},
abstract = {Free and open source software systems (FOSS) are distributed and made available to users under different software licenses, mentioned in FOSS code by means of licensing statements. Various factors, such as changes in the legal landscape, commercial code licensed as FOSS, or code reused from other FOSS systems, lead to evolution of licensing, which may affect the way a system or part thereof can be subsequently used. Therefore, it is crucial to monitor licensing evolution. However, manually tracking the licensing evolution of thousands of files is a daunting task.After presenting several cases of the effects of licensing evolution, we propose an approach to automatically track changes occurring in the licensing terms of a system. Then, we report an empirical study of the licensing evolution of six different FOSS systems. Results show that licensing underwent frequent and substantial changes.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {145–154},
numpages = {10},
keywords = {evolution, mining software repositories, empirical study, open source systems, software licenses},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/3623759.3624548,
author = {Castes, Charly and Ghosn, Adrien},
title = {Dynamic Linkers Are the Narrow Waist of Operating Systems},
year = {2023},
isbn = {9798400704048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623759.3624548},
doi = {10.1145/3623759.3624548},
abstract = {While software applications, programming languages, and hardware have changed, operating systems have not. Widely-used commodity operating systems are still modelled after the ones designed in the seventies. The accumulated burden of backward compatibility with the large software ecosystems that run our workloads prevents systems from embracing more efficient and disruptive designs explored by the system research community.This paper advocates a fresh approach to operating system research, where innovations are incrementally integrated into operating systems, without disrupting existing software, to gradually reshape our daily-use systems. The dynamic linker emerges as a pivotal element in this transformation process, redefining system behavior. The paper outlines specific use cases, covering performance enhancements, strengthened security measures, streamlined software deployment, and enriched programming language abstractions. Additionally, the paper introduces Spidl, an experimental modular dynamic linker to facilitate the exploration of this promising new research avenue.},
booktitle = {Proceedings of the 12th Workshop on Programming Languages and Operating Systems},
pages = {26–33},
numpages = {8},
location = {Koblenz, Germany},
series = {PLOS '23}
}

@inproceedings{10.5555/2663638.2663665,
author = {Prause, Christian R. and Eisenhauer, Markus},
title = {First Results from an Investigation into the Validity of Developer Reputation Derived from Wiki Articles and Source Code},
year = {2012},
isbn = {9781467318242},
publisher = {IEEE Press},
abstract = {The internal quality of software is often neglected by developers for various reasons like time pressure or a general dislike for certain activities. Yet internal quality is important to speed up development and to keep software maintainable. We present a way to use reputation systems to improve the internal quality of software by putting artifacts like wiki articles and source code under their control. Specifically, we show that reputation scores derived from such artifacts reflect actual reputation in the developer community using data from a work group wiki and an open source project.},
booktitle = {Proceedings of the 5th International Workshop on Co-Operative and Human Aspects of Software Engineering},
pages = {126–128},
numpages = {3},
keywords = {wiki, validation, reputation, quality, source code},
location = {Zurich, Switzerland},
series = {CHASE '12}
}

@inproceedings{10.1109/FLOSS.2007.5,
author = {Delorey, Daniel P. and Knutson, Charles D. and Chun, Scott},
title = {Do Programming Languages Affect Productivity? A Case Study Using Data from Open Source Projects},
year = {2007},
isbn = {0769529615},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2007.5},
doi = {10.1109/FLOSS.2007.5},
abstract = {Brooks and others long ago suggested that on average computer programmers write the same number of lines of code in a given amount of time regardless of the programming language used. We examine data collected from the CVS repositories of 9,999 open source projects hosted on SourceForge.net to test this assumption for 10 of the most popular programming languages in use in the open source community. We find that for 24 of the 45 pairwise comparisons, the programming language is a significant factor in determining the rate at which source code is written, even after accounting for variations between programmers and projects.},
booktitle = {Proceedings of the First International Workshop on Emerging Trends in FLOSS Research and Development},
pages = {8},
series = {FLOSS '07}
}

@inproceedings{10.5555/2821679.2831287,
author = {Lepp\"{a}nen, Marko and Eloranta, Veli-Pekka},
title = {Patterns for Distributed Machine Control System Data Sharing},
year = {2012},
isbn = {9781450327862},
publisher = {The Hillside Group},
address = {USA},
abstract = {In this paper we will present three patterns for sharing sensory data and other information in distributed machine control systems. A distributed machine control system is a software entity that is specifically designed to control a certain hardware system. This special hardware is a part of a work machine, which can be a forest harvester, a drilling machine, elevator system etc. or some process automation system. Some of the key attributes of such software systems are their close relation to the hardware, strict real-time requirements, functional safety, fault tolerance, high availability and long life cycle. Data sharing is essential part of achieving distribution as collaboration between different parts of the system requires data exchange.},
booktitle = {Proceedings of the 19th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {15},
keywords = {variables, patterns, data exchange},
location = {Tucson, Arizona},
series = {PLoP '12}
}

@inproceedings{10.5555/2820518.2820544,
author = {Blincoe, Kelly and Harrison, Francis and Damian, Daniela},
title = {Ecosystems in GitHub and a Method for Ecosystem Identification Using Reference Coupling},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Software projects are not developed in isolation. Recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, identifying technical dependencies at the ecosystem level can be challenging. In this paper, we propose a new method, known as reference coupling, for detecting technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects, and we identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners' social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors' social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {202–207},
numpages = {6},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/1810295.1810310,
author = {Nanda, Mangala Gowri and Gupta, Monika and Sinha, Saurabh and Chandra, Satish and Schmidt, David and Balachandran, Pradeep},
title = {Making Defect-Finding Tools Work for You},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810310},
doi = {10.1145/1810295.1810310},
abstract = {Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {99–108},
numpages = {10},
keywords = {static analysis portal, defect views, defect prioritization, defect merging, defect differencing},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.5555/2818754.2818805,
author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys},
title = {When and Why Your Code Starts to Smell Bad},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {403–414},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1109/ICSE48619.2023.00027,
author = {Huang, Sunzhou and Wang, Xiaoyin},
title = {PExReport: Automatic Creation of Pruned Executable Cross-Project Failure Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00027},
doi = {10.1109/ICSE48619.2023.00027},
abstract = {Modern software development extensively depends on existing libraries written by other developer teams from the same or a different organization. When a developer executes the software, the execution trace may go across the boundaries of multiple software products and create cross-project failures (CPFs). Existing studies show that a stand-alone executable failure report may enable the most effective communication, but creating such a report is often challenging due to the complicated files and dependencies interactions in the software ecosystems. In this paper, to solve the CPF report trilemma, we developed PExReport, which automatically creates stand-alone executable CPF reports. PExReport leverages build tools to prune source code and dependencies, and further analyzes the build process to create a pruned build environment for reproducing the CPF. We performed an evaluation on 74 software project issues with 198 CPFs, and the evaluation results show that PExReport can create executable CPF reports for 184 out of 198 test failures in our dataset, with an average reduction of 72.97% on source classes and the classes in internal JARs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {184–195},
numpages = {12},
keywords = {debloating, failure reproduction, executable failure report, build environment, build tool, cross-project failure},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3209281.3209335,
author = {Oliveira, Marcelo Iury S. and L\'{o}scio, Bernadette Farias},
title = {What is a Data Ecosystem?},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209335},
doi = {10.1145/3209281.3209335},
abstract = {The way that individuals and organizations have produced and consumed data has changed with the advent of new technologies. As a consequence, data has become a tradable and valuable good. There are now Data Ecosystems, in which a number of actors interact with each other to exchange, produce and consume data. Such ecosystems provide an environment for creating, managing and sustaining data sharing initiatives. Despite Data Ecosystems are gaining importance, until now, there is no common agreement on what theories should look like in Data Ecosystems. The evidence is the lack of a well-accepted definition of the term Data Ecosystem. In order to overcome this gap, in this paper, we investigate some theoretical issues that are relevant for Data Ecosystems. Our main focus is on the aspects related to the components of a Data Ecosystem as well as to propose a common definition for a Data Ecosystem term. Therefore, the aim of our work is two-fold. First, we investigate the state of research on the Data Ecosystem field and related kinds of ecosystems, such as Business and Software Ecosystems to enable the development of a common knowledge base. Second, we extract constructs from relevant studies in order to build a common and cohesive definition for Data Ecosystems.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {74},
numpages = {9},
keywords = {data ecosystem, government data ecosystem, formalization},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3357765.3359515,
author = {Hinterreiter, Daniel and Nieke, Michael and Linsbauer, Lukas and Seidl, Christoph and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Harmonized Temporal Feature Modeling to Uniformly Perform, Track, Analyze, and Replay Software Product Line Evolution},
year = {2019},
isbn = {9781450369800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357765.3359515},
doi = {10.1145/3357765.3359515},
abstract = {A feature model (FM) describes commonalities and variability within a software product line (SPL) and represents the configuration options at one point in time. A temporal feature model (TFM) additionally represents FM evolution, e.g., the change history or the planning of future releases. The increasing number of different TFM notations hampers research collaborations due to a lack of interoperability regarding notations, editors, and analyses. We present a common API for TFMs, which provides the core of a TFM ecosystem, to harmonize notations. We identified the requirements for the API based on systematically classifying and comparing the capabilities of existing TFM approaches. Our approach allows to work seamlessly with different TFM notations to perform, track, analyze and replay evolution. Our evaluation investigates two research questions on the expressiveness (RQ1) and utility (RQ2) of our approach by presenting implementations for several existing FM and TFM notations and replaying evolution histories from two case study systems.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {115–128},
numpages = {14},
keywords = {evolution, software product lines},
location = {Athens, Greece},
series = {GPCE 2019}
}

@inproceedings{10.1145/3551349.3556921,
author = {Xu, Meiqiu and Wang, Ying and Cheung, Shing-Chi and Yu, Hai and Zhu, Zhiliang},
title = {Insight: Exploring Cross-Ecosystem Vulnerability Impacts},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556921},
doi = {10.1145/3551349.3556921},
abstract = {Vulnerabilities, referred to as CLV issues, are induced by cross-language invocations of vulnerable libraries. Such issues greatly increase the attack surface of Python/Java projects due to their pervasive use of C libraries. Existing Python/Java build tools in PyPI and Maven ecosystems fail to report the dependency on vulnerable libraries written in other languages such as C. CLV issues are easily missed by developers. In this paper, we conduct the first empirical study on the status quo of CLV issues in PyPI and Maven ecosystems. It is found that 82,951 projects in these ecosystems are directly or indirectly dependent on libraries compiled from the C project versions that are identified to be vulnerable in CVE reports. Our study arouses the awareness of CLV issues in popular ecosystems and presents related analysis results. The study also leads to the development of the first automated mechanism, Insight, which provides a turn-key solution to the identification of CLV issues in PyPI and Maven projects based on published CVE reports of vulnerable C projects. Insight automatically identifies if a PyPI or Maven project is using a C library compiled from vulnerable C project versions in published CVE reports. It also deduces the vulnerable APIs involved by analyzing the usage of various foreign function interfaces such as CFFI and JNI in the concerned PyPI or Maven project. Insight achieves a high detection rate of 88.4% on a popular CLV issue benchmark. Contributing to the open-source community, we report 226 CLV issues detected in the actively maintained PyPI and Maven projects that are directly dependent on vulnerable C library versions. Our reports are well received and appreciated by developers with queries on the availability of Insight. 127 reported issues (56.2%) were quickly confirmed by developers and 74.8% of them were fixed/under fixing by popular projects, such as Mongodb&nbsp;[40] and Eclipse/Sumo&nbsp;[19].},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {58},
numpages = {13},
keywords = {Software Ecosystem, Multilingual Program Analysis},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/507052.507053,
author = {Satyanarayanan, M.},
title = {The Evolution of Coda},
year = {2002},
issue_date = {May 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/507052.507053},
doi = {10.1145/507052.507053},
abstract = {Failure-resilient, scalable, and secure read-write access to shared information by mobile and static users over wireless and wired networks is a fundamental computing challenge. In this article, we describe how the Coda file system has evolved to meet this challenge through the development of mechanisms for server replication, disconnected operation, adaptive use of weak connectivity, isolation-only transactions, translucent caching, and opportunistic exploitation of hardware surrogates. For each mechanism, the article explains how usage experience with it led to the insights for another mechanism. It also shows how Coda has been influenced by the work of other researchers and by industry. The article closes with a discussion of the technical and nontechnical lessons that can be learned from the evolution of the system.},
journal = {ACM Trans. Comput. Syst.},
month = {may},
pages = {85–124},
numpages = {40},
keywords = {disaster recovery, weakly connected operation, mobile computing, failure, continuous data access, conflict resolution, Adaptation, Linux, server replication, isolation-only transactions, low-bandwidth networks, optimistic replica control, Windows, intermittent networks, caching, high availability, data staging, translucent cache management, hoarding, disconnected operation, UNIX}
}

@inproceedings{10.1145/3238147.3240470,
author = {Hassan, Foyzul and Rodriguez, Rodney and Wang, Xiaoyin},
title = {RUDSEA: Recommending Updates of Dockerfiles via Software Environment Analysis},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240470},
doi = {10.1145/3238147.3240470},
abstract = {Dockerfiles are configuration files of docker images which package all dependencies of a software to enable convenient software deployment and porting. In other words, dockerfiles list all environment assumptions of a software application's build and / or execution, so they need to be frequently updated when the environment assumptions change during fast software evolution. In this paper, we propose RUDSEA, a novel approach to recommend updates of dockerfiles to developers based on analyzing changes on software environment assumptions and their impacts. Our evaluation on 1,199 real-world instruction updates shows that RUDSEA can recommend correct update locations for 78.5% of the updates, and correct code changes for 44.1% of the updates.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {796–801},
numpages = {6},
keywords = {String Analysis, Dockerfile, Software Environment},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3092703.3092713,
author = {Koyuncu, Anil and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Klein, Jacques and Monperrus, Martin and Le Traon, Yves},
title = {Impact of Tool Support in Patch Construction},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092713},
doi = {10.1145/3092703.3092713},
abstract = {In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {237–248},
numpages = {12},
keywords = {Automation, Debugging, Repair, Linux, Tools, Empirical, Patch},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/2901739.2901744,
author = {Avery, Daniel and Dam, Hoa Khanh and Savarimuthu, Bastin Tony Roy and Ghose, Aditya},
title = {Externalization of Software Behavior by the Mining of Norms},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901744},
doi = {10.1145/2901739.2901744},
abstract = {Open Source Software Development (OSSD) often suffers from conflicting views and actions due to the perceived flat and open ecology of an open source community. This often manifests itself as a lack of codified knowledge that is easily accessible for community members. How decisions are made and expectations of a software system are often described in detail through the many forms of social communications that take place within a community. These social interactions form norms which are influential in dictating what behaviors are expected in a community and of the system. In this paper, we provide a tool which mines these social interactions (in the form of bug reports) and extract norms of the system, externalizing this information into a codified form that allows others within the community to be aware of without having witnessed the social interactions.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {223–234},
numpages = {12},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/2491411.2492402,
author = {Lopez, Nicolas},
title = {Using Topic Models to Understand the Evolution of a Software Ecosystem},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2492402},
doi = {10.1145/2491411.2492402},
abstract = {The development of a software system is now ever more frequently a part of a larger development effort, including multiple software systems that co-exist in the same environment: a software ecosystem. Though most studies of the evolution of software have focused on a single software system, there is much that we can learn from the analysis of a set of interrelated systems. Topic modeling techniques show promise for mining the data stored in software repositories to understand the evolution of a system. In my research I seek to explore how topic modeling techniques can aid in understanding the evolution of a software ecosystem. The results of this research have the potential to improve how topic modeling techniques are used to predict, plan, and understand the evolution of software, and will inform the design of tools that support software engineering activities such as feature location, expertise identification, and bug detection.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {723–726},
numpages = {4},
keywords = {software evolution, topic modeling, software ecosys-tems, Latent Dirichlet Allocation, Mining software repositories},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1145/2047478.2047479,
author = {Zayas-Cab\'{a}n, Teresa and Chaney, Kevin},
title = {Improving Consumer Health IT Application Development: Lessons from Other Industries, a Summary},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/2047478.2047479},
doi = {10.1145/2047478.2047479},
abstract = {This paper is a summary of the Improving Consumer Health Information Technology (IT) Application Development: Lessons from Other Industries, Background Report (available at http://healthit.ahrq.gov/developmentmethodsbackgroundreport). originally published on July 2011. The report was commissioned by the Agency for Healthcare Research and Quality (AHRQ) as a key deliverable for the task order entitled "Understanding Development Methods from Other Industries to Improve the Design of Consumer Health IT" and is an outgrowth of the Building Bridges: Consumer Needs and the Design of Health IT workshop sponsored by AHRQ. The workshop was held at AHRQ's facilities on July 27--28, 2009.},
journal = {SIGHIT Rec.},
month = {sep},
pages = {4–12},
numpages = {9},
keywords = {design methods, product development, Consumer health IT, consumer products}
}

@inproceedings{10.1145/2517349.2522720,
author = {Elphinstone, Kevin and Heiser, Gernot},
title = {From L3 to SeL4 What Have We Learnt in 20 Years of L4 Microkernels?},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517349.2522720},
doi = {10.1145/2517349.2522720},
abstract = {The L4 microkernel has undergone 20 years of use and evolution. It has an active user and developer community, and there are commercial versions which are deployed on a large scale and in safety-critical systems. In this paper we examine the lessons learnt in those 20 years about microkernel design and implementation. We revisit the L4 design papers, and examine the evolution of design and implementation from the original L4 to the latest generation of L4 kernels, especially seL4, which has pushed the L4 model furthest and was the first OS kernel to undergo a complete formal verification of its implementation as well as a sound analysis of worst-case execution times. We demonstrate that while much has changed, the fundamental principles of minimality and high IPC performance remain the main drivers of design and implementation decisions.},
booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
pages = {133–150},
numpages = {18},
location = {Farminton, Pennsylvania},
series = {SOSP '13}
}

@inproceedings{10.1145/2330601.2330651,
author = {Bienkowski, Marie and Brecht, John and Klo, Jim},
title = {The Learning Registry: Building a Foundation for Learning Resource Analytics},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330651},
doi = {10.1145/2330601.2330651},
abstract = {We describe our experimentation with the current implementation of a distribution system used to share descriptive and social metadata about learning resources. The Learning Registry, developed and released in a beta version in October 2011, is intended to store and forward learning-resource metadata among a distributed, de-centralized network of nodes. The Learning Registry also accepts social/attention metadata---data about users of and activity around the learning resource. The Learning Registry open-source community has proposed a schema for sharing social metadata, and has experimented with a number of organizations representing their social metadata using that schema. This paper describes the results and challenges, and the learning-resource analytics applications that will use Learning Registry data as their foundation.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {208–211},
numpages = {4},
keywords = {social metadata, attention metadata, learning-resource analytics, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.5555/2048476.2048482,
author = {Tu, Zhiying and Zacharewicz, Gregory and Chen, David},
title = {Harmonized and Reversible Development Framework for HLA Based Interoperable Application},
year = {2011},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {This paper aims at improving the re-implementation of existing information systems when they are reused in a system of systems, i.e. a federation of enterprise information systems that interoperate. The idea is adapting the local experiences, coming from the development of the original information system, to be part of a HLA process, thanks to Model Discovery and Ontological approach. This paper will propose a new bi-directional development life cycle. In the first way, MDA (Model Driven Architecture) and HLA FEDEP (Federation Development and Execution Process) will be harmonized to implement distributed information systems from enterprise models. Conversely, model discovery will be used to help re-implement existing systems, in order to be interoperable without being fully reconstructed. Then, according to HLA 1516 evolved new features, this paper will propose a solution based on an open source RTI, PORTICO, to implement web enable federates.},
booktitle = {Proceedings of the 2011 Symposium on Theory of Modeling &amp; Simulation: DEVS Integrative M&amp;S Symposium},
pages = {51–58},
numpages = {8},
keywords = {model discovery, interoperability, HLA evolved, HLA},
location = {Boston, Massachusetts},
series = {TMS-DEVS '11}
}

@article{10.1145/1899928.1899948,
author = {Dumitra\c{s}, Tudor and Neamtiu, Iulian and Tilevich, Eli},
title = {Report on the Second ACM Workshop on Hot Topics in Software Upgrades (HotSWUp'09): Http://Www.Hotswup.Org/2009/},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1899928.1899948},
doi = {10.1145/1899928.1899948},
abstract = {The Second ACM SIGPLAN Workshop on Hot Topics in Software Upgrades (HotSWUp'09) was held on 25 October 2009 in Orlando, FL. The workshop was co-located with OOPSLA 2009 and was sponsored by ACM SIGPLAN. Twenty researchers and practitioners, from the programming languages, systems, software engineering and database communities, attended HotSWUp'09.The goal of HotSWUp is to identify, through interdisciplinary collaboration, cutting-edge research ideas for implementing software upgrades.The workshop combined presentations of peer-reviewed research papers with invited presentations from well-known experts and a keynote speech on the practical issues related to performing large-scale upgrades. The audience included researchers and practitioners from academia, the industry (Facebook, ABB, Oracle) and the open-source community (AppUpdater). In addition to the technical presentations, the program allowed ample time for discussions, which were driven by debate questions provided in advance by the presentersHotSWUp provides a premier forum for discussing problems that are often considered niche topics in the established research communities. For example, the technical discussions at HotSWUp'09 covered dynamic software updates, package management tools, database schema upgrades, upgrades of systems with real-time constraints, etc., and highlighted many synergies among these topics. Perhaps more interestingly, the industry presentations provided real-world examples of systems that a have strong requirement for online upgrades. These examples emphasized the magnitude of the software upgrade problems that the industry is facing today},
journal = {SIGOPS Oper. Syst. Rev.},
month = {dec},
pages = {146–152},
numpages = {7},
keywords = {software upgrades, package management, real-time upgrades, dynamic software update, database schema evolution}
}

@article{10.1145/2893177,
author = {Heiser, Gernot and Elphinstone, Kevin},
title = {L4 Microkernels: The Lessons from 20 Years of Research and Deployment},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/2893177},
doi = {10.1145/2893177},
abstract = {The L4 microkernel has undergone 20 years of use and evolution. It has an active user and developer community, and there are commercial versions that are deployed on a large scale and in safety-critical systems. In this article we examine the lessons learnt in those 20 years about microkernel design and implementation. We revisit the L4 design articles and examine the evolution of design and implementation from the original L4 to the latest generation of L4 kernels. We specifically look at seL4, which has pushed the L4 model furthest and was the first OS kernel to undergo a complete formal verification of its implementation as well as a sound analysis of worst-case execution times. We demonstrate that while much has changed, the fundamental principles of minimality, generality, and high inter-process communication (IPC) performance remain the main drivers of design and implementation decisions.},
journal = {ACM Trans. Comput. Syst.},
month = {apr},
articleno = {1},
numpages = {29},
keywords = {IPC, real time, virtualization, Microkernels, performance, message passing, L4, seL4, formal verification, minimality, worst-case execution time}
}

@inproceedings{10.1145/2448556.2448573,
author = {Othman, Abu Talib and Khan, Sohail and Nauman, Mohammad and Musa, Shahrulniza},
title = {Towards a High-Level Trusted Computing API for Android Software Stack},
year = {2013},
isbn = {9781450319584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2448556.2448573},
doi = {10.1145/2448556.2448573},
abstract = {Smartphone platforms are fast becoming the de-facto method of online communication. Android is one of the most promising smartphone platforms with backing of both the finances of industry leaders and the technical skills and expertise of the open source community. These two factors combined with the usability of application design on the Android platform has propelled the platform in the fore-front of technological innovations. However, along with this wide acceptance by the community come large risks associated with privacy, security and trust. Users share very sensitive data of a very personal nature on their smartphones. Protection of this data is of immense importance if the adoption of smartphones is to be continued. Similar threats on the pc have been countered using the concepts of Trusted Computing -- a highly flexible trust mechanism with strong security properties. The Android platform has yet to see any Trusted Computing applications primarily because of the difficulty in adopting the relatively new paradigm of security. In this paper, we present the design of a high-level api that allows existing Android developers to adopt Trusted Computing and use it in their applications without having to learn the intricate details of how Trusted Computing works. The api will help the developers to increase the security of their applications and the data that they consume.},
booktitle = {Proceedings of the 7th International Conference on Ubiquitous Information Management and Communication},
articleno = {17},
numpages = {9},
keywords = {trusted computing, Android, security, mobile platforms},
location = {Kota Kinabalu, Malaysia},
series = {ICUIMC '13}
}

@inproceedings{10.1145/958220.958245,
author = {R\"{o}dig, Peter and Borghoff, Uwe M. and Scheffczyk, Jan and Schmitz, Lothar},
title = {Preservation of Digital Publications: An OAIS Extension and Implementation},
year = {2003},
isbn = {1581137249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/958220.958245},
doi = {10.1145/958220.958245},
abstract = {Over the last decades, the amount of digital documents has increased exponentially. Nevertheless, traditional document engineering methods are applied. Even worse, the long-term preservation issues have been neglected in standard document life cycle implementations.Our digital (cultural) heritage is, therefore, highly endangered by the silent obsolescence of data formats, software and hardware. Severe losses of information already happened. It is high time to implement concrete solutions.Fortunately numerous institutions already target these issues. Moreover, with the OAIS reference model1 a rich standardized conceptual framework is available, which already serves as implementation basis.2This paper discusses an extension to the OAIS reference model and illustrates a prototype implementation of a document life cycle that is enriched by functions for long-term preservation.More precisely, this paper aims to provide first solutions to the following three problem areas:1. Detachment: OAIS defines no functions for the process of detaching digital documents prior to the ingest function. This detachment function is modeled in great detail and implemented for the provision of the so-called OAIS's submission information packages (SIP).2. DBMS: OAIS defines a very complex functionality. We show how a standard database management system (DBMS) can support a wide variety of required functionalities in an integrated and homogenous way. Among others OAIS's data management, archival storage, and access are supported.3. Metadata: So far, OAIS does not cover any aspects of the metadata generation. Here, we briefly discuss the (semi-)automatic generation of a metadata set.In order to evaluate the feasibility of our approach, we built a first prototype. We carried out our experiments in close cooperation with the Bavarian State Library, Munich, which is engaged in numerous international initiatives dealing with the problem of long-term preservation. Our University Library also supported us by delivering a representative test set of digital publications.3We conclude our paper by presenting some lessons learned from our conceptual work and from our real world experiments.},
booktitle = {Proceedings of the 2003 ACM Symposium on Document Engineering},
pages = {131–139},
numpages = {9},
keywords = {OAIS, archival systems, document management, database management, digital libraries, metadata, detachment of digital publications, long-term preservation},
location = {Grenoble, France},
series = {DocEng '03}
}

@inproceedings{10.5555/3213200.3213204,
author = {Chen, Wen and Sharieh, Salah and Blainey, Bob},
title = {A Security-as-a-Service Solution for Applications in Cloud Computing Environment},
year = {2018},
isbn = {9781510860155},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Cloud computing is achieving tremendous popularity these days, by sharing software, platform and infrastructure through virtualization across various organizations and individuals, it offers easy access to computing power that greatly exceeds what one can access in his old physical world. Although almost all cloud vendors claim that their clouds are safe, security concerns are still raised widely among cloud users. The concerns not only come from users who hold sensitive user data, but also from the security responsibilities that cloud users need to take for their applications and the cloud environment they own. However, the cloud vendors have not yet provided an unified security service that could cover all stages in the software development life cycle (SDLC), while conventional security protection mechanisms that may be effective and efficient for on-premise architecture may not be suitable for the new cloud architecture. Especially, for small businesses who wish to leverage cloud computing but usually do not have access to a complete list of security services throughout the entire SDLC, it becomes greatly desirable to design an integrated service that aims to meet the security needs from various aspects. This papers explores in-depth what specific security requirements / concerns that cloud environment has, and more importantly, a security-as-a-service (SECaaS) solution is proposed to provide an end-to-end solution for organizations or individuals who need to deploy their applications onto cloud.},
booktitle = {Proceedings of the Communications and Networking Symposium},
articleno = {4},
numpages = {9},
keywords = {security, security as a service, cloud computing},
location = {Baltimore, Maryland},
series = {CNS '18}
}

@inproceedings{10.1145/3447993.3448138,
author = {Li, Yuanjie and Peng, Chunyi and Zhang, Zhehui and Tan, Zhaowei and Deng, Haotian and Zhao, Jinghao and Li, Qianru and Guo, Yunqi and Ling, Kai and Ding, Boyan and Li, Hewu and Lu, Songwu},
title = {Experience: A Five-Year Retrospective of MobileInsight},
year = {2021},
isbn = {9781450383424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447993.3448138},
doi = {10.1145/3447993.3448138},
abstract = {This paper reports our five-year lessons of developing and using MobileInsight, an open-source community tool to enable software-defined full-stack, runtime mobile network analytics inside our phones. We present how MobileInsight evolves from a simple monitor to a community toolset with cross-layer analytics, energy-efficient real-time user-plane analytics, and extensible user-friendly analytics at the control and user planes. These features are enabled by various novel techniques, including cross-layer state machine tracking, missing data inference, and domain-specific cross-layer sampling. Their powerfulness is exemplified with a 5-year longitudinal study of operational mobile network latency using a 6.4TB dataset with 6.1 billion over-the-air messages. We further share lessons and insights of using MobileInsight by the community, as well as our visions of MobileInsight's past, present, and future.},
booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
pages = {28–41},
numpages = {14},
keywords = {MobileInsight, cellular network, mobile network, mobile data science and analysis},
location = {New Orleans, Louisiana},
series = {MobiCom '21}
}

@inproceedings{10.1145/3472301.3484358,
author = {Galv\~{a}o, Vinicius Ferreira and Maciel, Cristiano and Pereira, Vinicius Carvalho and Garcia, Ana Cristina Bicharra and Pereira, Roberto and Viterbo, Jos\'{e}},
title = {Posthumous Data at Stake: An Overview of Digital Immortality Issues},
year = {2021},
isbn = {9781450386173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472301.3484358},
doi = {10.1145/3472301.3484358},
abstract = {Who wants to be immortal? While this rhetorical question seems to haunt humans, digital immortality is already a reality. Social networks have allowed and facilitated our digital footprints to remain forever at reach of Internet users. We will all die, on the other hand, our digital self may remain forever. Social media technology has also allowed people to browse or even post messages to maintain the memory of a person alive. It seems spooky, but it is technically feasible to build a chatbot from a deceased person's digital legacy allowing a certain degree of presence of a deceased. This unaware digital immortality brings new cultural, technical, social, ethical, and legal challenges and implications that should be brought to light and discussed. There is an urge to recognize the limits for an adequate use of digital immortality. By means of an overview, this paper reports our literature survey where we identified some key issues regarding digital immortality, looking at the benefits, consequences and complications from its use. Our study identified differences when dealing with famous versus non-famous deceased, explicitly defined public versus private data, right and duties related to data preservation versus deletion, technological advances versus restrictions, cultural impedance, and differences from the reasons for desiring immortalization. With this article, we aim to encourage discussions on digital immortality, its current limits and points that need to be considered regarding such a sensitive theme that is already a current issue.},
booktitle = {Proceedings of the XX Brazilian Symposium on Human Factors in Computing Systems},
articleno = {43},
numpages = {8},
keywords = {Death, Posthumous Interaction, Digital Legacy, Digital Immortality},
location = {Virtual Event, Brazil},
series = {IHC '21}
}

@inproceedings{10.1145/2369220.2369236,
author = {Hartung, Carl and Lerer, Adam and Anokwa, Yaw and Tseng, Clint and Brunette, Waylon and Borriello, Gaetano},
title = {Open Data Kit: Tools to Build Information Services for Developing Regions},
year = {2010},
isbn = {9781450307871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2369220.2369236},
doi = {10.1145/2369220.2369236},
abstract = {This paper presents Open Data Kit (ODK), an extensible, open-source suite of tools designed to build information services for developing regions. ODK currently provides four tools to this end: Collect, Aggregate, Voice, and Build. Collect is a mobile platform that renders application logic and supports the manipulation of data. Aggregate provides a "click-to-deploy" server that supports data storage and transfer in the "cloud" or on local servers. Voice renders application logic using phone prompts that users respond to with keypad presses. Finally, Build is a application designer that generates the logic used by the tools. Designed to be used together or independently, ODK core tools build on existing open standards and are supported by an open-source community that has contributed additional tools. We describe four deployments that demonstrate how the decisions made in the system architecture of ODK enable services that can both push and pull information in developing regions.},
booktitle = {Proceedings of the 4th ACM/IEEE International Conference on Information and Communication Technologies and Development},
articleno = {18},
numpages = {12},
keywords = {mobile phones, mobile computing, client-server distributed systems, ICTD},
location = {London, United Kingdom},
series = {ICTD '10}
}

@inproceedings{10.1145/3560835.3564547,
author = {Jiang, Wenxin and Synovic, Nicholas and Sethi, Rohan and Indarapu, Aryan and Hyatt, Matt and Schorlemmer, Taylor R. and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Artifacts and Security Risks in the Pre-Trained Model Supply Chain},
year = {2022},
isbn = {9781450398855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560835.3564547},
doi = {10.1145/3560835.3564547},
abstract = {Deep neural networks achieve state-of-the-art performance on many tasks, but require increasingly complex architectures and costly training procedures. Engineers can reduce costs by reusing a pre-trained model (PTM) and fine-tuning it for their own tasks. To facilitate software reuse, engineers collaborate around model hubs, collections of PTMs and datasets organized by problem domain. Although model hubs are now comparable in popularity and size to other software ecosystems, the associated PTM supply chain has not yet been examined from a software engineering perspective. We present an empirical study of artifacts and security features in 8 model hubs. We indicate the potential threat models and show that the existing defenses are insufficient for ensuring the security of PTMs. We compare PTM and traditional supply chains, and propose directions for further measurements and tools to increase the reliability of the PTM supply chain.},
booktitle = {Proceedings of the 2022 ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {105–114},
numpages = {10},
keywords = {machine learning, software supply chain, deep neural networks, model hubs, empirical software engineering, software reuse},
location = {Los Angeles, CA, USA},
series = {SCORED'22}
}

@inproceedings{10.1145/2641580.2641599,
author = {Aaltonen, Timo and Mikkonen, Tommi and Peltola, Heikki and Salminen, Arto},
title = {From Mashup Applications to Open Data Ecosystems},
year = {2014},
isbn = {9781450330169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641580.2641599},
doi = {10.1145/2641580.2641599},
abstract = {Web-based software is available all over the world instantly after the online release. Applications can be used and updated without need to install anything, with natural support for collaboration, which allows users to interact and share the same applications over the Web. In addition, numerous web services allowing users to upload, download, store and modify private and public resources have emerged. However, as the amount of web services and devices used to consume as well as generate data has exploded, it is difficult to access and manage relevant data. In this paper, we start from the principles of mashups, reflect their use to the concepts of software ecosystems, and finally extend the discussion to open data generated by users themselves. As a technical contribution, we also introduce our proof-of-concept implementation of a mashup system built on wellness data, and discuss the main lessons we have learned in the process.},
booktitle = {Proceedings of The International Symposium on Open Collaboration},
pages = {1–8},
numpages = {8},
keywords = {ecosystem, open data, Web apps, cloud service},
location = {Berlin, Germany},
series = {OpenSym '14}
}

@inproceedings{10.1145/1936254.1936269,
author = {Yamakami, Toshihiko},
title = {A Two-Layer Ecosystem Evolution Model: Lessons from Stages of Mobile Data Services},
year = {2010},
isbn = {9781450300476},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1936254.1936269},
doi = {10.1145/1936254.1936269},
abstract = {The development of mobile services in Japan shows that there are three chasms: one for walled garden, one for SNS-based community services, and one for mobile open platforms. These chasms were discussed from the context of a series of discrete events. The author tries to provide a birds-eye view of this development. The author proposes a two-layer structure for mobile service ecosystems, where each chasm represents the lower-layer component and the upper layer represents the cross-stage ecosystem. The author maps the development of mobile services into this proposed model, and discusses the ecosystem factor for each layer, in order provide a universal basis for generating eco-system-aware mobile data services.},
booktitle = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems},
pages = {81–86},
numpages = {6},
keywords = {service evolution, digital ecosystem, mobile service},
location = {Bangkok, Thailand},
series = {MEDES '10}
}

@inproceedings{10.5555/1385089.1385095,
author = {Franco, Luis and Sahama, Tony and Croll, Peter},
title = {Security Enhanced Linux to Enforce Mandatory Access Control in Health Information Systems},
year = {2008},
isbn = {9781920682613},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {This paper introduces Security Enhanced Linux (SELinux) as the required Operating System (OS) to enforce Mandatory Access Control (MAC) mechanisms to protect Health Information. Health Information Systems (HIS) require an OS which can enforce MAC rules so that access to the resources does not rely on the discretion of the users, thus minimizing the damage when users' applications are compromised. SELinux provides a flexible and fine-grained MAC architecture implementing a combination of Type Enforcement (TE) and Role-Based Access Control (RBAC). SELinux however, is considered to be difficult to implement because of the complexity of SELinux policies required by the fine-grained access controls. To reduce the complexity to manage SELinux policies different tools and methods have been developed increasing the feasibility to use SELinux to create trusted systems.},
booktitle = {Proceedings of the Second Australasian Workshop on Health Data and Knowledge Management - Volume 80},
pages = {27–33},
numpages = {7},
keywords = {health information privacy, HIS, MAC, access control, SELinux, SELinux policy tools, RBAC},
location = {Wollongong, NSW, Australia},
series = {HDKM '08}
}

@inproceedings{10.1145/3447786.3456243,
author = {Hunt, Guerney D. H. and Pai, Ramachandra and Le, Michael V. and Jamjoom, Hani and Bhattiprolu, Sukadev and Boivie, Rick and Dufour, Laurent and Frey, Brad and Kapur, Mohit and Goldman, Kenneth A. and Grimm, Ryan and Janakirman, Janani and Ludden, John M. and Mackerras, Paul and May, Cathy and Palmer, Elaine R. and Rao, Bharata Bhasker and Roy, Lawrence and Starke, William A. and Stuecheli, Jeff and Valdez, Enriquillo and Voigt, Wendel},
title = {Confidential Computing for OpenPOWER},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456243},
doi = {10.1145/3447786.3456243},
abstract = {This paper presents Protected Execution Facility (PEF), a virtual machine-based Trusted Execution Environment (TEE) for confidential computing on Power ISA. PEF enables protected secure virtual machines (SVMs). Like other TEEs, PEF verifies the SVM prior to execution. PEF utilizes a Trusted Platform Module (TPM), secure boot, and trusted boot as well as newly introduced architectural changes for Power ISA systems. Exploiting these architectural changes requires new firmware, the Protected Execution Ultravisor. PEF is supported in the latest version of the POWER9 chip. PEF demonstrates that access control for isolation and cryptography for confidentiality is an effective approach to confidential computing. We particularly focus on how our design (i) balances between access control and cryptography, (ii) maximizes the use of existing security components, and (iii) simplifies the management of the SVM life cycle. Finally, we evaluate the performance of SVMs in comparison to normal virtual machines on OpenPOWER systems.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {294–310},
numpages = {17},
keywords = {secure computing, KVM, confidential computing, POWER9, trusted execution environment, Linux, enclave, firmware, TEE, ultravisor},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{10.5555/2050655.2050677,
author = {Palyart, Marc and Lugato, David and Ober, Ileana and Bruel, Jean-Michel},
title = {Improving Scalability and Maintenance of Software for High-Performance Scientific Computing by Combining MDE and Frameworks},
year = {2011},
isbn = {9783642244841},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, numerical simulation has attracted increasing interest within industry and among academics. Paradoxically, the development and maintenance of high performance scientific computing software has become more complex due to the diversification of hardware architectures and their related programming languages and libraries.In this paper, we share our experience in using model-driven development for numerical simulation software. Our approach called MDE4HPC proposes to tackle development complexity by using a domain specific modeling language to describe abstract views of the software. We present and analyse the results obtained with its implementation when deriving this abstract model to target Arcane, a development framework for 2D and 3D numerical simulation software.},
booktitle = {Proceedings of the 14th International Conference on Model Driven Engineering Languages and Systems},
pages = {213–227},
numpages = {15},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@article{10.1145/2930665,
author = {Bortolotti, Daniele and Marongiu, Andrea and Benini, Luca},
title = {VirtualSoC: A Research Tool for Modern MPSoCs},
year = {2016},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/2930665},
doi = {10.1145/2930665},
abstract = {Architectural heterogeneity has proven to be an effective design paradigm to cope with an ever-increasing demand for computational power within tight energy budgets, in virtually every computing domain. Programmable manycore accelerators are currently widely used not only in high-performance computing systems, but also in embedded devices, in which they operate as coprocessors under the control of a general-purpose CPU (the host processor). Clearly, such powerful hardware architectures are paired with sophisticated and complex software ecosystems, composed of operating systems, programming models plus associated runtime engines, and increasingly complex user applications with related libraries. System modeling has always played a key role in early architectural exploration or software development when the real hardware is not available. The necessity of efficiently coping with the huge HW/SW design space provided by the described heterogeneous Systems on Chip (SoCs) calls for advanced full-system simulation methodologies and tools, capable of assessing various metrics for the functional and nonfunctional properties of the target system. In this article, we describe VirtualSoC, a simulation tool targeting the full-system simulation of massively parallel heterogeneous SoCs. We also describe how VirtualSoC has been successfully adopted in several research projects.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {3},
numpages = {27},
keywords = {Virtual platforms, accuracy, SystemC modeling, full-system simulation, manycore accelerators}
}

@inproceedings{10.1145/3524842.3528492,
author = {Sousa, Bruno L. and Bigonha, Mariza A. S. and Ferreira, Kecia A. M. and Franco, Glaura C.},
title = {A Time Series-Based Dataset of Open-Source Software Evolution},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528492},
doi = {10.1145/3524842.3528492},
abstract = {Software evolution is the process of developing, maintaining, and updating software systems. It is known that the software systems tend to increase their complexity and size over their evolution to meet the demands required by the users. Due to this fact, researchers have increasingly carried out studies on software evolution to understand the systems' evolution pattern and propose techniques to overcome inherent problems in software evolution. Many of these works collect data but do not make them publicly available. Many datasets on software evolution are outdated, and/or are small, and some of them do not provide time series from software metrics. We propose an extensive software evolution dataset with temporal information about open-source Java systems. To build this dataset, we proposed a methodology of four steps: selecting the systems using a criterion, extracting and measuring their releases, and generating their time series. Our dataset contains time series of 46 software metrics extracted from 46 open-source Java systems, and we make it publicly available.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {702–706},
numpages = {5},
keywords = {open-source software, software metrics, dataset, software evolution, time series},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3597638.3614550,
author = {Seo, JooYoung and Rogge, Megan},
title = {Coding Non-Visually in Visual Studio Code: Collaboration Towards Accessible Development Environment for Blind Programmers},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3614550},
doi = {10.1145/3597638.3614550},
abstract = {This paper delineates a fruitful collaboration between blind and sighted developers, aiming to augment the accessibility of Visual Studio Code (VSCode). Our shared journey is portrayed through examples drawn from our interaction with GitHub issues, pull requests, review processes, and insider’s releases, each contributing to an improved VSCode experience for blind developers. One key milestone of our co-design process is the establishment of an accessible terminal buffer, a significant enhancement for blind developers using VSCode. Other innovative outcomes include Git Diff audio cues, adaptable verbosity settings, intuitive help menus, and a targeted accessibility testing initiative. These tailored improvements not only uplift the accessibility standards of VSCode but also provide a valuable blueprint for open-source developers at large. Through our shared dedication to promoting inclusivity in software development, we aim for the strategies and successes shared in this paper to inspire and guide the open-source community towards crafting more accessible software environments.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {102},
numpages = {9},
keywords = {integrated development environment, nonvisual programming, visual studio code, accessibility},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up Architectural SW Health Builds in a New Product Line Generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {software architecture, architectural technical debt, embedded software, technical debt, architectural checks, product line development, software erosion},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1145/3468264.3468535,
author = {Luo, Linghui and Sch\"{a}f, Martin and Sanchez, Daniel and Bodden, Eric},
title = {IDE Support for Cloud-Based Static Analyses},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468535},
doi = {10.1145/3468264.3468535},
abstract = {Integrating static analyses into continuous integration (CI) or continuous delivery (CD) has become the best practice for assuring code quality and security. Static Application Security Testing (SAST) tools fit well into CI/CD, because CI/CD allows time for deep static analyses on large code bases and prevents vulnerabilities in the early stages of the development lifecycle. In CI/CD, the SAST tools usually run in the cloud and provide findings via a web interface. Recent studies show that developers prefer seeing the findings of these tools directly in their IDEs. Most tools with IDE integration run lightweight static analyses and can give feedback at coding time, but SAST tools used in CI/CD take longer to run and usually are not able to do so. Can developers interact directly with a cloud-based SAST tool that is typically used in CI/CD through their IDE? We investigated if such a mechanism can integrate cloud-based SAST tools better into a developers’ workflow than web-based solutions. We interviewed developers to understand their expectations from an IDE solution. Guided by these interviews, we implemented an IDE prototype for an existing cloud-based SAST tool. With a usability test using this prototype, we found that the IDE solution promoted more frequent tool interactions. In particular, developers performed code scans three times more often. This indicates better integration of the cloud-based SAST tool into developers’ workflow. Furthermore, while our study did not show statistically significant improvement on developers’ code-fixing performance, it did show a promising reduction in time for fixing vulnerable code.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1178–1189},
numpages = {12},
keywords = {cloud service, IDE integration, SAST tools, static analysis, security testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/1987993.1988006,
author = {Helms, Eric and Williams, Laurie},
title = {Evaluating Access Control of Open Source Electronic Health Record Systems},
year = {2011},
isbn = {9781450305853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987993.1988006},
doi = {10.1145/1987993.1988006},
abstract = {Incentives and penalties for healthcare providers as laid out in the American Recovery and Reinvestment Act of 2009 have caused tremendous growth in the development and installation of electronic health record (EHR) systems in the US. For the benefit of protecting patient privacy, regulations and certification criteria related to EHR systems stipulate the use of access control of protected health information. The goal of this research is to guide development teams, regulators, and certification bodies by assessing the state of the practice in EHR access control. In this paper, we present a compilation of 25 criteria relative to access control in EHR systems found in the Health Insurance Portability and Accountability Act (HIPAA) regulation, meaningful use certification criteria, best practices embodied in the National Institute for Standards and Technology (NIST) role-based access control standard, and other best practices found in the literature. We then examine the state of the practice in access control by evaluating four open source EHR systems using these 25 evaluation criteria. Our research indicates that the NIST Meaningful Use criteria provide HIPAA compliance, but none of the regulatory and certification criteria address the implementation standards, and best practices related to access control. Additionally, our results indicate that open source EHR system designers are not implementing robust access control mechanisms for the adequate protection of patient data.},
booktitle = {Proceedings of the 3rd Workshop on Software Engineering in Health Care},
pages = {63–70},
numpages = {8},
keywords = {role-based access control, privacy, access control, healthcare, health it, electronic health records},
location = {Waikiki, Honolulu, HI, USA},
series = {SEHC '11}
}

@inproceedings{10.1145/3356317.3356325,
author = {da Silva, R\^{o}mulo Martins and Cruz, Cafer and de S. Campos, Heleno and Murta, Leonardo G. P. and de Oliveira Neves, V\^{a}nia},
title = {What is the Adoption Level of Automated Support for Testing in Open-Source Ecosystems?},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356325},
doi = {10.1145/3356317.3356325},
abstract = {In the last decades, different kinds of automated support for testing have emerged in the open-source community. However, we still have limited evidence about the adoption level of such automated support in practice, considering different programming language ecosystems. In this paper, we investigate the adoption of automated support for testing among 184 popular open-source projects. Besides, we also investigate test coverage and metrics correlations on 571 open-source projects. As results, we found that projects written in Go, PHP, and JavaScript are the ones that most adopt automated support and that JavaScript and Python projects have the largest test coverage, with, on average, 84% and 81%, respectively. Moreover, we also found overall negligible correlations between projects' amount of stars, commits and source lines of code and coverage. Knowing that an open-source project has a high test coverage may enhance users' confidence in using this project. Besides that, we also listed the testing tools, libraries or frameworks that are most adopted for each programming language ecosystem. It may help developers in choosing appropriate automated support. Finally, we established a research agenda on this topic that motivates deeper studies as future work.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {80–89},
numpages = {10},
keywords = {White-Box Coverage, Software Testing, Open Source, Testing Tools, Repository Mining},
location = {Salvador, Brazil},
series = {SAST '19}
}

@inproceedings{10.1145/2875913.2875924,
author = {Mo, Wenkai and Shen, Beijun and He, Yuming and Zhong, Hao},
title = {GEMiner: Mining Social and Programming Behaviors to Identify Experts in Github},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875924},
doi = {10.1145/2875913.2875924},
abstract = {Hosting over 10 million repositories, GitHub becomes the largest open source community in the world. Besides sharing code, Github is also a social network, in which developers can follow others or keep track of their interested projects. Considering the multi-roles of Github, integrating heterogenous data of each developer to identify experts is a challenging task. In this paper, we propose GEMiner, a novel approach to identify experts for some specific programming languages in Github. Different from previous approaches, GEMiner analyzes the social behaviors and programming behaviors of a developer to determine the expertise of the developer. When modeling social behaviors of developers, to integrate heterogenous social networks in Github, GEMiner implements a Multi-Sources PageRank algorithm. Also, GEMiner analyzes the behaviors of developers when they are programming (e.g., their commit activities and their preferred programming languages) to model programming behaviors of them. Based on our expertise models and our extracted programming languages data, GEMiner can then identify experts for some specific programming languages in Github. We conducted experiments on a real data set, and our results show that GEMiner identifies experts with 60% accuracy higher than the state-of-the-art algorithms.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {93–101},
numpages = {9},
keywords = {Github, Experts Identification, Social Network},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/3463274.3463808,
author = {S\"{o}derberg, Emma and Church, Luke and H\"{o}st, Martin},
title = {Open Data-Driven Usability Improvements of Static Code Analysis and Its Challenges},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463808},
doi = {10.1145/3463274.3463808},
abstract = {Context: Software development is moving towards a place where data about development is gathered in a systematic fashion in order to improve the practice, for example, in tuning of static code analysis. However, this kind of data gathering has so far primarily happened within organizations, which is unfortunate as it tends to favor larger organizations with more resources for maintenance of developer tools. Objective: Over the years, we have seen a lot of benefits from open source and recently there has been a lot of development in open data. We see this as an opportunity for cross-organisation community building and wonder to what extent the views on using and sharing open source software developer tools carry across to open data-driven tuning of software development tools. Method: An exploratory study with 11 participants divided into 3 focus groups discussing using and sharing of static code analyzers and data about these analyzers. Results: While using and sharing open-source code (analyzers in this case) is perceived in a positive light as part of the practice of modern software development, sharing data is met with skepticism and uncertainty. Developers are concerned about threats to the company brand, exposure of intellectual property, legal liabilities, and to what extent data is context-specific to a certain organisation. Conclusions: Sharing data in software development is different from sharing data about software development. We need to better understand how we can provide solutions for sharing of software development data in a fashion that reduces risk and enables openness.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {272–277},
numpages = {6},
keywords = {data-driven software development, open data, static code analysis},
location = {Trondheim, Norway},
series = {EASE '21}
}

@inproceedings{10.1145/1141277.1141716,
author = {Poritz, Jonathan A.},
title = {Trust[Ed | in] Computing, Signed Code and the Heat Death of the Internet},
year = {2006},
isbn = {1595931082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1141277.1141716},
doi = {10.1145/1141277.1141716},
abstract = {The Trusted Computing Group (TCG) is an industry consortium which has invested in the design of a small piece of hardware (roughly a smartcard), called a Trusted Platform Module (TPM), and associated APIs and protocols which are supposed to help increase the reliability of TPM-endowed computing platforms (trusted platforms). The TCG envisions that boot loaders, OSes and applications programs on trusted platforms will all collaborate in building a cryptographic hash chain which represents the current execution state of the platform, and which resides on the TPM. Remote sites can then verify that the platform in question is "in a trusted state" by requesting the TPM to produce a signed data blob containing the value of this hash chain, which can then be compared against a library of recognized ("trusted") values; this process is called remote attestation, and the whole picture is sometimes referred to as integrity-based computing (IBC).We argue that there is a fundamental gap between the stated goals of the TCG's IBC and the central technology that is intended to achieve these goals, which gap is simply that remote attestation asks the attesting platform to answer the wrong question - the platform is not attesting to its security state, but rather to its execution state, and this underlies all of the troublesome use cases, as well as a number of the practical difficulties, of the TCG world-view. One response to this is to replace standard TCG attestation with property-based attestation (PBA), which places the emphasis on deriving security properties from (potentially) elaborate trust models and conditional statements of security property dependencies. Herein the central r\^{o}le for IBC of trust and deriving consequences from precise trust models becomes clear.Finally, we claim that the TCG's own remote attestation is most properly viewed in fact as a form of PBA, with a certain simple trust model and database of security properties. From this point of view, it becomes clear that IBC can have a much less restrictive range of applications than envisioned merely by the TCG. In fact, with the right "trust infrastructure" and sufficiently open software using and relying upon this infrastructure, IBC could actually realize some of the portentous early promises of the TCG for truly increasing the reliability of individual users' platforms and pushing back the apocalyptic rise of malware, especially if platforms and OSes virtualize and enforce some kind of signed code contracts.},
booktitle = {Proceedings of the 2006 ACM Symposium on Applied Computing},
pages = {1855–1859},
numpages = {5},
keywords = {binary attestation, remote attestation, hypervisor, security properties, security property language, web of trust, signed code, trust language, OS virtualization, trust model, property-based attestation, trusted computing},
location = {Dijon, France},
series = {SAC '06}
}

@inproceedings{10.1145/3548606.3560654,
author = {Wichelmann, Jan and Sieck, Florian and P\"{a}tschke, Anna and Eisenbarth, Thomas},
title = {Microwalk-CI: Practical Side-Channel Analysis for JavaScript Applications},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560654},
doi = {10.1145/3548606.3560654},
abstract = {Secret-dependent timing behavior in cryptographic implementations has resulted in exploitable vulnerabilities, undermining their security. Over the years, numerous tools to automatically detect timing leakage or even to prove their absence have been proposed. However, a recent study at IEEE S&amp;P 2022 showed that, while many developers are aware of one or more analysis tools, they have major difficulties integrating these into their workflow, as existing tools are tedious to use and mapping discovered leakages to their originating code segments requires expert knowledge. In addition, existing tools focus on compiled languages like C, or analyze binaries, while the industry and open-source community moved to interpreted languages, most notably JavaScript.  In this work, we introduce Microwalk-CI, a novel side-channel analysis framework for easy integration into a JavaScript development workflow. First, we extend existing dynamic approaches with a new analysis algorithm, that allows efficient localization and quantification of leakages, making it suitable for use in practical development. We then present a technique for generating execution traces from JavaScript applications, which can be further analyzed with our and other algorithms originally designed for binary analysis. Finally, we discuss how Microwalk-CI can be integrated into a continuous integration (CI) pipeline for efficient and ongoing monitoring. We evaluate our analysis framework by conducting a thorough evaluation of several popular JavaScript cryptographic libraries, and uncover a number of critical leakages.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2915–2929},
numpages = {15},
keywords = {software development, leakage analysis, side-channel attacks},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo Variability! JHipster: A Playground for Web-Apps Analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {web-apps, case study, variability-related analyses},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1145/2636879.2636907,
author = {Pires, Durval and Alves, Valter and Roque, Licinio},
title = {A Software Architecture for Dynamic Enhancement of Soundscapes in Games},
year = {2014},
isbn = {9781450330329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2636879.2636907},
doi = {10.1145/2636879.2636907},
abstract = {A game soundscape often includes sounds that are triggered by the game logic according to the player's actions and to other real time occurrences. The dynamic nature of such triggering events can lead to a composition that is not necessarily interesting, at a given moment. Following principles from Acoustic Ecology, and specifically the notion of healthy soundscape, we propose a system aiming at the moderation of sounds generated during gameplay in a way that the composition retains its communicational meaningfulness. For instance, the system aims to ensure that the soundscape does not get overcrowded by the superimposition of whatever sounds might be triggered, and that the player can actually ear the relevant stimuli. The main component of the system is a module that implements heuristics that free designers from restating accepted sound design principles, and programmers from embedding such intelligence in the game logic. Thus, designers can focus on expressing their design intents, by using an API to create and characterize the sound sources to be handled by the sound engine. The use of this API also conveys a more readable expression of the game's sound design, easing communication and reuse. Hence, this proposal can be particularly relevant during fast prototyping phases, when it can constitute an expedite way to test and refine creative ideas, while avoiding extensive coding and the typical complexity and cost of existing middleware, which is particular relevant for small budget and sound novice practitioners. We finish by presenting results from a proof of concept implementation, and a game remake for evaluating the proposed system.},
booktitle = {Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound},
articleno = {27},
numpages = {8},
keywords = {sound design, game design, sound engine, dynamic soundscape composition, healthy soundscape, acoustic ecology, pattern language, game audio, middleware, soundscape, design patterns},
location = {Aalborg, Denmark},
series = {AM '14}
}

@inproceedings{10.1145/2010425.2010439,
author = {Franke, Tobias and Kahn, Svenja and Olbrich, Manuel and Jung, Yvonne},
title = {Enhancing Realism of Mixed Reality Applications through Real-Time Depth-Imaging Devices in X3D},
year = {2011},
isbn = {9781450307741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2010425.2010439},
doi = {10.1145/2010425.2010439},
abstract = {Until recently, depth sensing cameras have been used almost exclusively in research due to the high costs of such specialized equipment. With the introduction of the Microsoft Kinect device, realtime depth imaging is now available for the ordinary developer at low expenses and so far it has been received with great interest from both the research and hobby developer community. The underlying OpenNI framework not only allows to extract the depth image from the camera, but also provides tracking information of gestures or user skeletons. In this paper, we present a framework to include depth sensing devices into X3D in order to enhance visual fidelity of X3D Mixed Reality applications by introducing some extensions for advanced rendering techniques. We furthermore outline how to calibrate depth and image data in a meaningful way through calibration for devices that do not already come with precalibrated sensors, as well as a discussion of some of the OpenNI functionality that X3D can benefit from in the future.},
booktitle = {Proceedings of the 16th International Conference on 3D Web Technology},
pages = {71–79},
numpages = {9},
keywords = {rendering, augmented reality, mixed reality, X3D},
location = {Paris, France},
series = {Web3D '11}
}

@inproceedings{10.1109/ICSSP.2019.00033,
author = {Kruchten, Philippe},
title = {The End of Agile as We Know It},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00033},
doi = {10.1109/ICSSP.2019.00033},
abstract = {It's been 20 years or so, but the end is in sight. We have successfully placed the adjective 'agile' in front of about every important noun in our software development / IT world: agile design, agile testing, agile management, agile database, agile architecture, agile user-interaction.... Agile has won the war. "What is next?" is the question I've been asked again and again. What is the future of software engineering? The next best thing? Is it DevOps, cloud-something, micro-services, AI? The adjective agile has lost some of its weight and novelty, only a few laggards are still asking "what is it?"It is time to reflect on the fundamental aspects of agility: what does it really means, what are the fundamental principles behind it, that made its successes. The agile movement has had some tremendous impact in the way we work, putting the human being and human interaction more central in these processes, by using extensively iterations, direct interactions, and feedback loops. But at the same time, some aspects of agile have become dogmatic, fossilized, and the agile movement has not been always very agile in its application to itself. These dogmatic aspects have slowed the expansion of its own principles to some of the more complex or much larger software development endeavours.Now, the increasing need for speed, the availability of opensource software repositories, the shifts in technology, such as the cloud, the emergence of software ecosystems are creating new needs in terms of process and project management, that can exploit the fundamental principles of agile, beyond the dogma of this or that method, this or that practice. As the amount of software in use is growing and will outgrow the capacity of our industry to maintain and evolve it, the industry faces a massive amount of technical debt, which we do not know well how to mitigate or repay. Agile has been very valuable, but once its lessons are fully integrated in the way we work we have to look beyond and stop repeating it like a mantra. Agile is dead. Long live agility.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {104},
numpages = {1},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@inproceedings{10.1145/1238844.1238846,
author = {Ierusalimschy, Roberto and de Figueiredo, Luiz Henrique and Celes, Waldemar},
title = {The Evolution of Lua},
year = {2007},
isbn = {9781595937667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1238844.1238846},
doi = {10.1145/1238844.1238846},
abstract = {We report on the birth and evolution of Lua and discuss how it moved from a simple configuration language to a versatile, widely used language that supports extensible semantics, anonymous functions, full lexical scoping, proper tail calls, and coroutines.},
booktitle = {Proceedings of the Third ACM SIGPLAN Conference on History of Programming Languages},
pages = {2–1–2–26},
location = {San Diego, California},
series = {HOPL III}
}

@inproceedings{10.1145/1808920.1808924,
author = {Kumar, Manish and Ajmeri, Nirav and Ghaisas, Smita},
title = {Towards Knowledge Assisted Agile Requirements Evolution},
year = {2010},
isbn = {9781605589749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808920.1808924},
doi = {10.1145/1808920.1808924},
abstract = {This paper presents work on a recommendation system for Knowledge assisted Agile Requirements Evolution (K-gileRE). We treat requirements engineering as a special case of knowledge engineering and emphasize the fact that providing a domain knowledge edge can impart agility to the requirements definition exercise. The approach differs from existing agile methods in that it seamlessly incorporates a domain knowledge base into an agile requirements definition framework and explicitly provides to requirement analysts, relevant online domain specific recommendations based on underlying ontologies. The framework presents a 'domain knowledge seed' to requirement analysts. The seed provides a view of core features in a given domain and associated knowledge elements such as business processes, rules, policies, partial data models, use cases and test cases,. These in turn are mapped with agile requirements elements such as user stories, features, tasks, product backlog, sprints and prototype plans. The requirement analyst can evolve the seed to suit her specific project needs. As she modifies and evolves the seed specification, she receives domain-specific online recommendations to improve the correctness, consistency and completeness of her requirement specification documents and executable models. Using the domain knowledge seed as a point of departure provides a jump-start to her project. Each exercise of requirements definition thus becomes an evolution from the seed instead of the traditional 'clean slate' Requirements Engineering (RE) that typically starts from the scratch. Hence, the term K-gileRE. We elaborate how K-gileRE helps in practicing the essence of agile doctrines while defining software requirements by providing just-in-time recommendations.},
booktitle = {Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering},
pages = {16–20},
numpages = {5},
keywords = {domain-specific recommendations, collaborative and semantic requirements definition, knowledge assisted agile},
location = {Cape Town, South Africa},
series = {RSSE '10}
}

@inproceedings{10.1145/2442882.2442893,
author = {Underwood, Heather and Sterling, S. Revi and Bennett, John K.},
title = {The Design and Implementation of the PartoPen Maternal Health Monitoring System},
year = {2013},
isbn = {9781450318563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442882.2442893},
doi = {10.1145/2442882.2442893},
abstract = {The World Health Organization advocates the paper partograph as the single most effective tool for monitoring labor and reducing labor complications in developing countries. Used correctly, the partograph can serve as a tool for early detection of serious maternal and fetal complications during labor, allowing sufficient time for an appropriate response. However, in order to be effective, the partograph must be used correctly. Recent studies in Kenya reported that less than one fourth of partographs were completed in accordance with WHO guidelines. In developing countries, lack of training and continuing education, exacerbated by limited resources, represents a serious barrier to effective partograph use. The goal of the PartoPen project is to increase the effectiveness of the partograph using an interactive digital pen with custom software, together with partograph forms printed with a background dot pattern that is recognized by the pen. This paper describes the design and implementation of the PartoPen system, and the technical evolution of the PartoPen system during studies that evaluated the PartoPen in use in Nairobi, Kenya from June 2012 -- August 2012.},
booktitle = {Proceedings of the 3rd ACM Symposium on Computing for Development},
articleno = {8},
numpages = {10},
keywords = {ICTD, maternal health, partograph, Kenya, digital pens},
location = {Bangalore, India},
series = {ACM DEV '13}
}

@inproceedings{10.1109/MSR.2017.55,
author = {Kikas, Riivo and Gousios, Georgios and Dumas, Marlon and Pfahl, Dietmar},
title = {Structure and Evolution of Package Dependency Networks},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.55},
doi = {10.1109/MSR.2017.55},
abstract = {Software developers often include available open-source software packages into their projects to minimize redundant effort. However, adding a package to a project can also introduce risks, which can propagate through multiple levels of dependencies. Currently, not much is known about the structure of open-source package ecosystems of popular programming languages and the extent to which transitive bug propagation is possible. This paper analyzes the dependency network structure and evolution of the JavaScript, Ruby, and Rust ecosystems. The reported results reveal significant differences across language ecosystems. The results indicate that the number of transitive dependencies for JavaScript has grown 60% over the last year, suggesting that developers should look more carefully into their dependencies to understand what exactly is included. The study also reveals that vulnerability to a removal of the most popular package is increasing, yet most other packages have a decreasing impact on vulnerability. The findings of this study can inform the development of dependency management tools.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {102–112},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1145/3360611,
author = {Sergey, Ilya and Nagaraj, Vaivaswatha and Johannsen, Jacob and Kumar, Amrit and Trunov, Anton and Hao, Ken Chan Guan},
title = {Safer Smart Contract Programming with Scilla},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360611},
doi = {10.1145/3360611},
abstract = {The rise of programmable open distributed consensus platforms based on the blockchain technology has aroused a lot of interest in replicated stateful computations, aka smart contracts. As blockchains are used predominantly in financial applications, smart contracts frequently manage millions of dollars worth of virtual coins. Since smart contracts cannot be updated once deployed, the ability to reason about their correctness becomes a critical task. Yet, the de facto implementation standard, pioneered by the Ethereum platform, dictates smart contracts to be deployed in a low-level language, which renders independent audit and formal verification of deployed code infeasible in practice.  We report an ongoing experiment held with an industrial blockchain vendor on designing, evaluating, and deploying Scilla, a new programming language for safe smart contracts. Scilla is positioned as an intermediate-level language, suitable to serve as a compilation target and also as an independent programming framework. Taking System F as a foundational calculus, Scilla offers strong safety guarantees by means of type soundness. It provides a clean separation between pure computational, state-manipulating, and communication aspects of smart contracts, avoiding many known pitfalls due to execution in a byzantine environment. We describe the motivation, design principles, and semantics of Scilla, and we report on Scilla use cases provided by the developer community. Finally, we present a framework for lightweight verification of Scilla programs, and showcase it with two domain-specific analyses on a suite of real-world use cases.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {185},
numpages = {30},
keywords = {Domain-Specific Languages, Smart Contracts, Static Analysis, Blockchain}
}

@inproceedings{10.1145/2593850.2593855,
author = {Venkatasubramanyam, Radhika D. and G. R., Sowmya},
title = {Why is Dynamic Analysis Not Used as Extensively as Static Analysis: An Industrial Study},
year = {2014},
isbn = {9781450328593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593850.2593855},
doi = {10.1145/2593850.2593855},
abstract = {Code Assessments using static and dynamic analyses are important for the maintenance of code quality of software in the industry. These analyses, though understood to be beneficial, have several practical limitations. The intent of our study was to understand the usage of these analyses across various teams in Siemens during code assessments, reasons for the success of static analysis and the challenges in dynamic analysis. The evaluation was conducted through a survey of the practices of developers in different development teams. Additionally interviews were conducted with expert reviewers and their experiences documented. Our study has revealed that institutionalization of code quality initiatives and customized toolkit support for static analysis are some of the key reasons for the successful implementation of static analysis. Some of the problems hindering the widespread adoption of dynamic analysis include the direct dependency with test cases, scalability, and understandability of the results. Various implications of the results are discussed, including the possible improvements that could increase the acceptance of dynamic analysis on a regular basis.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering Research and Industrial Practices},
pages = {24–33},
numpages = {10},
keywords = {code assessment, static analysis, Code quality, dynamic analysis, software quality},
location = {Hyderabad, India},
series = {SER&amp;IPs 2014}
}

@inproceedings{10.1145/1985441.1985466,
author = {Hindle, Abram and Ernst, Neil A. and Godfrey, Michael W. and Mylopoulos, John},
title = {Automated Topic Naming to Support Cross-Project Analysis of Software Maintenance Activities},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985466},
doi = {10.1145/1985441.1985466},
abstract = {Researchers have employed a variety of techniques to extract underlying topics that relate to software development artifacts. Typically, these techniques use semi-unsupervised machine-learning algorithms to suggest candidate word-lists. However, word-lists are difficult to interpret in the absence of meaningful summary labels. Current topic modeling techniques assume manual labelling and do not use domainspecific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: automated labelled topic extraction. Topics are extracted using Latent Dirichlet Allocation (LDA) from commit-log comments recovered from source control systems such as CVS and Bit-Keeper. These topics are given labels from a generalizable cross-project taxonomy, consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on two large-scale RDBMS projects: MySQL and MaxDB. The case studies show that labelled topic extraction can produce appropriate, context-sensitive labels relevant to these projects, which provides fresh insight into their evolving software development activities.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {163–172},
numpages = {10},
keywords = {lda, non-functional requirements, topic analysis},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1109/ESEM.2017.57,
author = {Sharma, Tushar and Fragkoulis, Marios and Spinellis, Diomidis},
title = {House of Cards: Code Smells in Open-Source C# Repositories},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.57},
doi = {10.1109/ESEM.2017.57},
abstract = {Background: Code smells are indicators of quality problems that make a software hard to maintain and evolve. Given the importance of smells in the source code's maintainability, many studies have explored the characteristics of smells and analyzed their effects on the software's quality.Aim: We aim to investigate fundamental characteristics of code smells through an empirical study on frequently occurring smells that examines inter-category and intra-category correlation between design and implementation smells.Method: The study mines 19 design smells and 11 implementation smells in 1988 C# repositories containing more than 49 million lines of code. The mined data are statistically analyzed using methods such as Spearman's correlation and presented through hexbin and scatter plots.Results: We find that unutilized abstraction and magic number smells are the most frequently occurring smells in C# code. Our results also show that implementation and design smells exhibit strong inter-category correlation. The results of co-occurrence analysis imply that whenever unutilized abstraction or magic number smells are found, it is very likely to find other smells from the same smell category in the project.Conclusions: Our experiment shows high average smell density (14.7 and 55.8 for design and implementation smells respectively) for open source C# programs. Such high smell densities turn a software system into a house of cards reflecting the fragility introduced in the system. Our study advocates greater awareness of smells and the adoption of regular refactoring within the developer community to avoid turning software into a house of cards.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {424–429},
numpages = {6},
keywords = {maintainability, design smells, C#, code quality, code smells, implementation smells},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3551349.3560437,
author = {Wang, Chao and Wu, Rongxin and Song, Haohao and Shu, Jiwu and Li, Guoqing},
title = {SmartPip: A Smart Approach to Resolving Python Dependency Conflict Issues},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560437},
doi = {10.1145/3551349.3560437},
abstract = {As one of the representative software ecosystems, PyPI, together with the Python package management tool pip, greatly facilitates Python developers to automatically manage the reuse of third-party libraries, thus saving development time and cost. Despite its great success in practice, a recent empirical study revealed the risks of dependency conflict (DC) issues and then summarized the characteristics of DC issues. However, the dependency resolving strategy, which is the foundation of the prior study, has evolved to a new one, namely the backtracking strategy. To understand how the evolution of this dependency resolving strategy affects the prior findings, we conducted an empirical study to revisit the characteristics of DC issues under the new strategy. Our study revealed that, of the two previously discovered DC issue manifestation patterns, one has significantly changed (Pattern A), while the other remained the same (Pattern B). We also observed, the resolving strategy for the DC issues of Pattern A suffers from the efficiency issue, while the one for the DC issues of Pattern B would lead to a waste of time and space. Based on our findings, we propose a tool smartPip&nbsp; to overcome the limitations of the resolving strategies. To resolve the DC issues of Pattern A, instead of iteratively verifying each candidate dependency library, we leverage a pre-built knowledge base of library dependencies to collect version constraints for concerned libraries, and then convert the version constraints into the SMT expressions for solving. To resolve the DC issues of Pattern B, we improve the existing virtual environment solution to reuse the local libraries as far as possible. Finally, we evaluated smartPip&nbsp; in three benchmark datasets of open source projects. The results showed that, smartPip&nbsp; can outperform the existing Python package management tools including pip with the new strategy and Conda in resolving DC issues of Pattern A, and achieve 1.19X - 1.60X speedups over the best baseline approach. Compared with the built-in Python virtual environment (venv), smartPip&nbsp; reduced 34.55% - 80.26% of storage space and achieved up to 2.26X - 6.53X speedups in resolving the DC issues of Pattern B.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {93},
numpages = {12},
keywords = {dependency resolving, Python, dependency conflict},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.5555/2820518.2820588,
author = {Spinellis, Diomidis},
title = {A Repository with 44 Years of Unix Evolution},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {The evolution of the Unix operating system is made available as a version-control repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2015 as a widely-used 26 million line system. The repository contains 659 thousand commits and 2306 merges. The repository employs the commonly used Git system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, Berkeley University, and the 386BSD team, two legacy repositories, and the modern repository of the open source FreeBSD system. In total, 850 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {462–465},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1109/ICSE48619.2023.00039,
author = {Wang, Yuekun and Ye, Yuhang and Wu, Yueming and Zhang, Weiwei and Xue, Yinxing and Liu, Yang},
title = {Comparison and Evaluation of Clone Detection Techniques with Different Code Representations},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00039},
doi = {10.1109/ICSE48619.2023.00039},
abstract = {As one of bad smells in code, code clones may increase the cost of software maintenance and the risk of vulnerability propagation. In the past two decades, numerous clone detection technologies have been proposed. They can be divided into text-based, token-based, tree-based, and graph-based approaches according to their code representations. Different code representations abstract the code details from different perspectives. However, it is unclear which code representation is more effective in detecting code clones and how to combine different code representations to achieve ideal performance.In this paper, we present an empirical study to compare the clone detection ability of different code representations. Specifically, we reproduce 12 clone detection algorithms and divide them into different groups according to their code representations. After analyzing the empirical results, we find that token and tree representations can perform better than graph representation when detecting simple code clones. However, when the code complexity of a code pair increases, graph representation becomes more effective. To make our findings more practical, we perform manual analysis on open-source projects to seek a possible distribution of different clone types in the open-source community. Through the results, we observe that most clone pairs belong to simple code clones. Based on this observation, we discard heavyweight graph-based clone detection algorithms and conduct combination experiments to find out a suitable combination of token-based and tree-based approaches for achieving scalable and effective code clone detection. We develop the suitable combination into a tool called TACC and evaluate it with other state-of-the-art code clone detectors. Experimental results indicate that TACC performs better and has the ability to detect large-scale code clones.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {332–344},
numpages = {13},
keywords = {empirical study, large scale, clone detection, code representation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3548659.3561306,
author = {Fulcini, Tommaso and Garaccione, Giacomo and Coppola, Riccardo and Ardito, Luca and Torchiano, Marco},
title = {Guidelines for GUI Testing Maintenance: A Linter for Test Smell Detection},
year = {2022},
isbn = {9781450394529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548659.3561306},
doi = {10.1145/3548659.3561306},
abstract = {GUI Test suites suffer from high fragility, in fact modifications or redesigns of the user interface are commonly frequent and often invalidate the tests. This leads, for both DOM- and visual-based techniques, to frequent need for careful maintenance of test suites, which can be expensive and time-consuming.  
The goal of this work is to present a set of guidelines to write cleaner and more robust test code, reducing the cost of maintenance and producing more understandable code. Based on the provided recommendations, a static test suite analyzer and code linter has been developed.  
An ad-hoc grey literature research was conducted on the state of the practice, by performing a semi-systematic literature review. Authors' experience was coded into a set of recommendations, by applying the grounded theory methodology.  
Based on these results, we developed a linter in the form of a plugin for Visual Studio Code, implementing 17 of the provided guidelines. The plugin highlights test smells in the Java and Javascript languages.  
Finally, we conducted a preliminary validation of the tool against test suites from real GitHub projects.  
The preliminary evaluation, meant to be an attempt of application of the plugin to real test suites, detected three main smells, namely the usage of global variables, the lack of adoption of the Page Object design pattern, and the usage of fragile locator such as the XPath.},
booktitle = {Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation},
pages = {17–24},
numpages = {8},
keywords = {Software Engineering, Software Testing, GUI Testing, Test smell},
location = {Singapore, Singapore},
series = {A-TEST 2022}
}

@inproceedings{10.1145/3540250.3549114,
author = {Di Grazia, Luca and Pradel, Michael},
title = {The Evolution of Type Annotations in Python: An Empirical Study},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549114},
doi = {10.1145/3540250.3549114},
abstract = {Type annotations and gradual type checkers attempt to reveal errors and facilitate maintenance in dynamically typed programming languages. Despite the availability of these features and tools, it is currently unclear how quickly developers are adopting them, what strategies they follow when doing so, and whether adding type annotations reveals more type errors. This paper presents the first large-scale empirical study of the evolution of type annotations and type errors in Python. The study is based on an analysis of 1,414,936 type annotation changes, which we extract from 1,123,393 commits among 9,655 projects. Our results show that (i) type annotations are getting more popular, and once added, often remain unchanged in the projects for a long time, (ii) projects follow three evolution patterns for type annotation usage -- regular annotation, type sprints, and occasional uses -- and that the used pattern correlates with the number of contributors, (iii) more type annotations help find more type errors (0.704 correlation), but nevertheless, many commits (78.3%) are committed despite having such errors. Our findings show that better developer training and automated techniques for adding type annotations are needed, as most code still remains unannotated, and they call for a better integration of gradual type checking into the development process.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {209–220},
numpages = {12},
keywords = {empirical study, Type annotations, Python, type errors},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3453478,
author = {Dilhara, Malinda and Ketkar, Ameya and Dig, Danny},
title = {Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3453478},
doi = {10.1145/3453478},
abstract = {Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {55},
numpages = {42},
keywords = {Machine learning libraries, Software-2.0, empirical studies}
}

@inproceedings{10.5555/1999416.1999452,
author = {Sokolowski, John A. and Banks, Catherine M.},
title = {A Proposed Approach to Modeling and Simulation Education for the Medical and Health Sciences},
year = {2010},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Modeling and Simulation is at the forefront of a multi-disciplinary effort to integrate quantitative and qualitative research methods and diverse modeling paradigms. M&amp;S possesses a variety of modeling tools that can represent many aspects of life, including life itself. The Medical and Health Sciences are proof of that in that M&amp;S is providing practitioners in these fields the capability to better understand some of the fundamental aspects of healthcare such as human behavior, human systems, medical treatment, and disease proliferation. This paper presents a philosophical, theoretical, and practical approach to advancing M&amp;S education specific to medical and health sciences},
booktitle = {Proceedings of the 2010 Summer Computer Simulation Conference},
pages = {284–289},
numpages = {6},
keywords = {operating room, bio-medical engineering, virtual, human systems modeling, standardized patient, clinical engineering},
location = {Ottawa, Ontario, Canada},
series = {SCSC '10}
}

@article{10.1145/3477040,
author = {Weiss, Kevin and Rottleuthner, Michel and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {PHiLIP on the HiL: Automated Multi-Platform OS Testing With External Reference Devices},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3477040},
doi = {10.1145/3477040},
abstract = {Developing an operating systems (OSs) for low-end embedded devices requires continuous adaptation to new hardware architectures and components, while serviceability of features needs to be assured for each individual platform under tight resource constraints. It is challenging to design a versatile and accurate heterogeneous test environment that is agile enough to cover a continuous evolution of the code base and platforms. This mission is even more challenging when organized in an agile open-source community process with many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing and Continuous Integration (CI) are automatable approaches to verify functionality, prevent regressions, and improve the overall quality at development speed in large community projects.In this paper, we present PHiLIP (Primitive Hardware in the Loop Integration Product), an open-source external reference device together with tools that validate the system software while it controls hardware and interprets physical signals. Instead of focusing on a specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL test process, designed for continuous evolution and deployment cycles. We explain its design, describe how it supports HiL tests, evaluate performance metrics, and report on practical experiences of employing PHiLIP in an automated CI test infrastructure. Our initial deployment comprises 22 unique platforms, each of which executes 98 peripheral tests every night. PHiLIP allows for easy extension of low-cost, adaptive testing infrastructures but serves testing techniques and tools to a much wider range of applications.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {91},
numpages = {26},
keywords = {IoT, constrained devices, hardware in the loop, operating system}
}

@proceedings{10.1145/1595800,
title = {IWOCE '09: Proceedings of the 1st International Workshop on Open Component Ecosystems},
year = {2009},
isbn = {9781605586779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 1st International Workshop on Open Component Ecosystem -- IWOCE 2009, whose goal is to gather together both researchers and practitioners active in open source software engineering, software composition, algorithms, constraint programming, and model-driven engineering to discuss, debate, exchange ideas, and outline common solutions to the problem of static and dynamic component aggregations in software ecosystems.Over the last years, the composition of software parts is increasingly considered a crucial operation to build and maintain large software systems. The continuous and independent evolution of readily available components suggested that open platforms can better accommodate and manage them as normally happens in systems like open source software distributions, Eclipse, and J2EE, just to mention a few. The critical mass represented by such software components requires organizations, such as companies, research groups, and open source communities, to collaborate on custom software development, implementation and shared services. Such infrastructures can be regarded as ecosystems, i.e., collections of software projects that possibly belong to organizations, developed in parallel by the organizations, and able to integrate each other at assembly time, during the configuration, and/or dynamically after the deployment.The capability of modeling, analyzing, and predicting the component behavior during these stages is intrinsically difficult and requires techniques, algorithms, and methods which are both expressive and computationally convenient in order to be engineered and conveyed in practical projects. Moreover, when analyzing software ecosystems, exploration and visualization cannot be neglected because of the large amounts of information that are available about the ecosystem.The call for papers attracted 6 submissions from Canada, Europe, and the United States. The program committee accepted 5 papers that cover some of the central topics of IWOCE, focusing in particular on component ecosystem deployment, configuation and modeling.},
location = {Amsterdam, The Netherlands}
}

@article{10.1145/3386324,
author = {Monnier, Stefan and Sperber, Michael},
title = {Evolution of Emacs Lisp},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386324},
doi = {10.1145/3386324},
abstract = {While Emacs proponents largely agree that it is the world’s greatest text editor, it is almost as much a Lisp machine disguised as an editor. Indeed, one of its chief appeals is that it is programmable via its own programming language. Emacs Lisp is a Lisp in the classic tradition. In this article, we present the history of this language over its more than 30 years of evolution. Its core has remained remarkably stable since its inception in 1985, in large part to preserve compatibility with the many third-party packages providing a multitude of extensions. Still, Emacs Lisp has evolved and continues to do so. Important aspects of Emacs Lisp have been shaped by concrete requirements of the editor it supports as well as implementation constraints. These requirements led to the choice of a Lisp dialect as Emacs’s language in the first place, specifically its simplicity and dynamic nature: Loading additional Emacs packages or changing the ones in place occurs frequently, and having to restart the editor in order to re-compile or re-link the code would be unacceptable. Fulfilling this requirement in a more static language would have been difficult at best. One of Lisp’s chief characteristics is its malleability through its uniform syntax and the use of macros. This has allowed the language to evolve much more rapidly and substantively than the evolution of its core would suggest, by letting Emacs packages provide new surface syntax alongside new functions. In particular, Emacs Lisp can be customized to look much like Common Lisp, and additional packages provide multiple-dispatch object systems, legible regular expressions, programmable pattern-matching constructs, generalized variables, and more. Still, the core has also evolved, albeit slowly. Most notably, it acquired support for lexical scoping. The timeline of Emacs Lisp development is closely tied to the projects and people who have shaped it over the years: We document Emacs Lisp history through its predecessors, Mocklisp and MacLisp, its early development up to the “Emacs schism” and the fork of Lucid Emacs, the development of XEmacs, and the subsequent rennaissance of Emacs development.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {74},
numpages = {55},
keywords = {Lisp, Emacs Lisp, History of programming languages}
}

@inproceedings{10.1145/3293882.3330578,
author = {Lou, Yiling and Chen, Junjie and Zhang, Lingming and Hao, Dan and Zhang, Lu},
title = {History-Driven Build Failure Fixing: How Far Are We?},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330578},
doi = {10.1145/3293882.3330578},
abstract = {Build systems are essential for modern software development and maintenance since they are widely used to transform source code artifacts into executable software. Previous work shows that build systems break frequently during software evolution. Therefore, automated build-fixing techniques are in huge demand. In this paper we target a mainstream build system, Gradle, which has become the most widely used build system for Java projects in the open-source community (e.g., GitHub). HireBuild, state-of-the-art build-fixing tool for Gradle, has been recently proposed to fix Gradle build failures via mining the history of prior fixes. Although HireBuild has been shown to be effective for fixing real-world Gradle build failures, it was evaluated on only a limited set of build failures, and largely depends on the quality/availability of historical fix information. To investigate the efficacy and limitations of the history-driven build fix, we first construct a new and large build failure dataset from Top-1000 GitHub projects. Then, we evaluate HireBuild on the extended dataset both quantitatively and qualitatively. Inspired by the findings of the study, we propose a simplistic new technique that generates potential patches via searching from the present project under test and external resources rather than the historical fix information. According to our experimental results, the simplistic approach based on present information successfully fixes 2X more reproducible build failures than the state-of-art HireBuild based on historical fix information. Furthermore, our results also reveal various findings/guidelines for future advanced build failure fixing.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {43–54},
numpages = {12},
keywords = {Build System, Build Failure Fixing, {Automated Program Repair},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00035,
author = {Effendi, Sedick David Baker and \c{C}irisci, Berk and Mukherjee, Rajdeep and Nguyen, Hoan Anh and Tripp, Omer},
title = {A Language-Agnostic Framework for Mining Static Analysis Rules from Code Changes},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00035},
doi = {10.1109/ICSE-SEIP58684.2023.00035},
abstract = {Static analysis tools detect a wide range of code defects, including code quality issues, security vulnerabilities, operational risks, and best-practice violations. Creating and maintaining a set of high-quality static analysis rules that detect misuses of popular libraries and SDKs across multiple languages is challenging. One of the mechanisms for inferring static analysis rules is by leveraging frequently occurring bug-fix code changes in the wild that are committed by multiple developers and into different software repositories. The intuition is that code changes following a common pattern correspond to recurring mistakes, from which deriving best practices could likely be of high value and accepted by the community.Automating the process of mining and clustering code changes enables a scalable mechanism to source and generate best-practices rules. From a coverage standpoint, the rules are derived from real-world code changes, which ensures that popular libraries and application domains are accounted for.In this paper, we present a language-agnostic framework for mining and clustering code changes from software repositories using a graph-based representation dubbed MU (μ). Unlike language-specific ASTs, the MU representation generalizes across languages by modeling programs at a higher semantic level, which enables grouping of code changes that are semantically similar yet syntactically distinct. We have mined a total of 62 high-quality static analysis rules across Java, JavaScript, and Python from less than 600 code change clusters. These cover multiple libraries, including the AWS Java and Python SDKs, as well as libraries like pandas, React, Android libraries, Json parsing libraries, and many more. These rules are integrated into a cloud-based static analyzer, Amazon CodeGuru Reviewer. Developers have accepted 73% of recommendations from these rules during code review, which signifies the value of these rules to help improve developer productivity, make code secure, and improve code hygiene.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {327–339},
numpages = {13},
keywords = {coding best practices, mining software repository, clustering, program synthesis, static analysis},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/2597073.2597136,
author = {Baldassari, Boris and Preux, Philippe},
title = {Understanding Software Evolution: The Maisqual Ant Data Set},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597136},
doi = {10.1145/2597073.2597136},
abstract = {Software engineering is a maturing discipline which has seen many drastic advances in the last years. However, some studies still point to the lack of rigorous and mathematically grounded methods to raise the field to a new emerging science, with proper and reproducible foundations to build upon. Indeed, mathematicians and statisticians do not necessarily have software engineering knowledge, while software engineers and practitioners do not necessarily have a mathematical background.  The Maisqual research project intends to fill the gap between both fields by proposing a controlled and peer-reviewed data set series ready to use and study. These data sets feature metrics from different repositories, from source code to mail activity and configuration management meta data. Metrics are described and commented, and all the steps followed for their extraction and treatment are described with contextual information about the data and its meaning.  This article introduces the Apache Ant weekly data set, featuring 636 extracts of the project over 12 years at different levels of artefacts – application, files, functions. By associating community and process related information to code extracts, this data set unveils interesting perspectives on the evolution of one of the great success stories of open source.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {424–427},
numpages = {4},
keywords = {Metrics},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1109/ICSE48619.2023.00052,
author = {Vu, Duc-Ly and Newman, Zachary and Meyers, John Speed},
title = {Bad Snakes: Understanding and Improving Python Package Index Malware Scanning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00052},
doi = {10.1109/ICSE48619.2023.00052},
abstract = {Open-source, community-driven package repositories see thousands of malware packages each year, but do not currently run automated malware detection systems. In this work, we explore the security goals of the repository administrators and the requirements for deploying such malware scanners via a case study of the Python ecosystem and PyPI repository, including interviews with administrators and maintainers. Further, we evaluate existing malware detection techniques for deployment in this setting by creating a benchmark dataset and comparing several existing tools: the malware checks implemented in PyPI, Bandit4Mal, and OSSGadget's OSS Detect Backdoor.We find that repository administrators have exacting requirements for such malware detection tools. Specifically, they consider a false positive rate of even 0.1% to be unacceptably high, given the large number of package releases that might trigger false alerts. Measured tools have false positive rates between 15% and 97%; increasing thresholds for detection rules to reduce this rate renders the true positive rate useless.While automated tools are far from reaching these demands, we find that a socio-technical malware detection system has emerged to meet these needs: external security researchers perform repository malware scans, filter for useful results, and report the results to repository administrators. These parties face different incentives and constraints on their time and tooling. We conclude with recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software repository administrators.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {499–511},
numpages = {13},
keywords = {malware detection, PyPI, open-source software (OSS) supply chain, qualitative study, quantitative study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.5555/2591272.2591276,
author = {Lu, Lanyue and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H. and Lu, Shan},
title = {A Study of Linux File System Evolution},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux file-system changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools.},
booktitle = {Proceedings of the 11th USENIX Conference on File and Storage Technologies},
pages = {31–44},
numpages = {14},
location = {San Jose, CA},
series = {FAST'13}
}

@inproceedings{10.1145/1326304.1326319,
author = {Latry, Fabien and Mercadal, Julien and Consel, Charles},
title = {Staging Telephony Service Creation: A Language Approach},
year = {2007},
isbn = {9781605580067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1326304.1326319},
doi = {10.1145/1326304.1326319},
abstract = {The open-endedness of telephony platforms is creating expectations among users, ranging from end-users to administrators, to create services dedicated to their activities. Not only is the population of developers heterogeneous, but the technologies underlying modern telephony range over a variety of areas such as multimedia, databases, web services, and distributed systems. This situation drastically widens the expertise required for service creation.This paper proposes an approach to coping with the heterogeneity of both the service developers and the technologies underlying modern telephony. Our approach is based on programming languages. It consists of providing a language that is specific to each developer community with respect to its expertise (e.g., programming skills) and the target application area (e.g., administration). Such languages, called Domain-Specific Languages (DSLs), are organized in layers, accounting for abstraction levels.Our layered approach to telephony service creation is illustrated by two high-level DSLs for end-user service creation, requiring no programming skills, and an expressive DSL enabling the development of expert-level telephony services. We show that layering DSLs greatly facilitates their implementation and verification of telephony-specific properties by leveraging on high-level tools.},
booktitle = {Proceedings of the 1st International Conference on Principles, Systems and Applications of IP Telecommunications},
pages = {99–110},
numpages = {12},
location = {New York City, New York},
series = {IPTComm '07}
}

@inproceedings{10.1145/1138506.1138513,
author = {Herraiz, Israel and Robles, Gregorio and Amor, Juan Jos\'{E} and Romera, Te\'{o}filo and Gonz\'{a}lez Barahona, Jes\'{u}s M.},
title = {The Processes of Joining in Global Distributed Software Projects},
year = {2006},
isbn = {1595934049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138506.1138513},
doi = {10.1145/1138506.1138513},
abstract = {Libre (free / open source) software is a good example of global software development. Thousands of projects, in a wide range of domains which involve hundreds of thousands of developers and contributors from all around the world. Some large (both in size and developer community) libre software projects have shown evidence of producing code with complete functionality and fast evolution (with linear or superlinear growth), while maintaining low defect density. Many companies are exploring how to benefit from this situation, considering several approaches related to libre software development. For instance, some of them have hired full-time developers, focusing their work on some libre software projects they consider strategic.However, before joining the core development team of the project, these hired developers have to follow a process of software comprehension, and get used to the rules and communication mechanisms used in the project. We were interested in the differences between this case and that of volunteer developers working in the same project, Therefore, we studied the duration and basic characteristics of this joining process for the developers of GNOME (a well known, large, libre software project). In our analysis, we have found two groups with clearly different joining patterns. Moreover, we have related those patterns to the different behaviors of volunteer and hired developers: volunteers tend to follow a step-by-step joining process, while hired developers usually experiencea "sudden" integration. Some reasons for this different behavior are also discussed.},
booktitle = {Proceedings of the 2006 International Workshop on Global Software Development for the Practitioner},
pages = {27–33},
numpages = {7},
keywords = {global software development, empirical studies, onion model, libre software, membership integration},
location = {Shanghai, China},
series = {GSD '06}
}

@inproceedings{10.1145/2610384.2610419,
author = {Marinescu, Paul and Hosek, Petr and Cadar, Cristian},
title = {Covrig: A Framework for the Analysis of Code, Test, and Coverage Evolution in Real Software},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610419},
doi = {10.1145/2610384.2610419},
abstract = {Software repositories provide rich information about the construction and evolution of software systems. While static data that can be mined directly from version control systems has been extensively studied, dynamic metrics concerning the execution of the software have received much less attention, due to the inherent difficulty of running and monitoring a large number of software versions. In this paper, we present Covrig, a flexible infrastructure that can be used to run each version of a system in isolation and collect static and dynamic software metrics, using a lightweight virtual machine environment that can be deployed on a cluster of local or cloud machines. We use Covrig to conduct an empirical study examining how code and tests co-evolve in six popular open-source systems. We report the main characteristics of software patches, analyse the evolution of program and patch coverage, assess the impact of nondeterminism on the execution of test suites, and investigate whether the coverage of code containing bugs and bug fixes is higher than average.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {93–104},
numpages = {12},
keywords = {latent patch cover- age, coverage evolution, nondeterministic coverage, bugs and fixes, Patch characteristics},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3133841.3133843,
author = {Ungar, David and Grove, David and Franke, Hubertus},
title = {Dynamic Atomicity: Optimizing Swift Memory Management},
year = {2017},
isbn = {9781450355261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133841.3133843},
doi = {10.1145/3133841.3133843},
abstract = {Swift is a modern multi-paradigm programming language with an extensive developer community and open source ecosystem. Swift 3's memory management strategy is based on Automatic Reference Counting (ARC) augmented with unsafe APIs for manually-managed memory. We have seen ARC consume as much as 80% of program execution time. A significant portion of ARC's direct performance cost can be attributed to its use of atomic machine instructions to protect reference count updates from data races. Consequently, we have designed and implemented dynamic atomicity, an optimization which safely replaces atomic reference-counting operations with nonatomic ones where feasible. The optimization introduces a store barrier to detect possibly intra-thread references, compiler-generated recursive reference-tracers to find all affected objects, and a bit of state in each reference count to encode its atomicity requirements.  Using a suite of 171 microbenchmarks, 9 programs from the Computer Language Benchmarks Game, and the Richards benchmark, we performed a limit study by unsafely making all reference counting operations nonatomic. We measured potential speedups of up to 220% on the microbenchmarks, 120% on the Benchmarks Game and 70% on Richards.  By automatically reducing ARC overhead, our optimization both improves Swift 3's performance and reduces the temptation for performance-oriented programmers to resort to unsafe manual memory management. Furthermore, the machinery implemented for dynamic atomicity could also be employed to obtain cheaper thread-safe Swift data structures, or to augment ARC with optional cycle detection or a backup tracing garbage collector.},
booktitle = {Proceedings of the 13th ACM SIGPLAN International Symposium on on Dynamic Languages},
pages = {15–26},
numpages = {12},
keywords = {Swift, ARC, Reference Counting},
location = {Vancouver, BC, Canada},
series = {DLS 2017}
}

@inproceedings{10.5555/3172795.3172831,
author = {Masri, Samer Al and Bhuiyan, Nazim Uddin and Nadi, Sarah and Gaudet, Matthew},
title = {Software Variability through C++ Static Polymorphism: A Case Study of Challenges and Open Problems in Eclipse OMR},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software Product Line Engineering (SPLE) creates configurable platforms that can be used to efficiently produce similar, and yet different, product variants. SPLs are typically modular such that it is easy to connect different blocks of code together, creating different variations of the product. There are many variability implementation mechanisms to achieve an SPL. This paper shows how static polymorphism can be used to implement variability, through a case study of IBM's open-source Eclipse OMR project. We discuss the current open problems and challenges this variability implementation mechanism raises and highlight technology gaps for reasoning about variability in OMR. We then suggest steps to close these gaps.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {285–291},
numpages = {7},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@article{10.1145/2560012,
author = {Lu, Lanyue and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H. and Lu, Shan},
title = {A Study of Linux File System Evolution},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/2560012},
doi = {10.1145/2560012},
abstract = {We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux file-system changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools.},
journal = {ACM Trans. Storage},
month = {jan},
articleno = {3},
numpages = {32},
keywords = {reliability, failure, performance, bug, patch, File systems}
}

@inproceedings{10.1145/3035918.3064016,
author = {Huang, Jiamin and Mozafari, Barzan and Schoenebeck, Grant and Wenisch, Thomas F.},
title = {A Top-Down Approach to Achieving Performance Predictability in Database Systems},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3064016},
doi = {10.1145/3035918.3064016},
abstract = {While much of the research on transaction processing has focused on improving overall performance in terms of throughput and mean latency, surprisingly less attention has been given to performance predictability: how often individual transactions exhibit execution latency far from the mean. Performance predictability is increasingly important when transactions lie on the critical path of latency-sensitive applications, enterprise software, or interactive web services.In this paper, we focus on understanding and mitigating the sources of performance unpredictability in today's transactional databases. We conduct the first quantitative study of major sources of variance in MySQL, Postgres (two of the largest and most popular open-source products on the market), and VoltDB (a non-conventional database). We carry out our study with a tool called TProfiler that, given the source code of a database system and programmer annotations indicating the start and end of a transaction, is able to identify the dominant sources of variance in transaction latency. Based on our findings, we investigate alternative algorithms, implementations, and tuning strategies to reduce latency variance without compromising mean latency or throughput. Most notably, we propose a new lock scheduling algorithm, called Variance-Aware Transaction Scheduling (VATS), and a lazy buffer pool replacement policy. In particular, our modified MySQL exhibits significantly lower variance and 99th percentile latencies by up to 5.6\texttimes{} and 6.3\texttimes{}, respectively. Our proposal has been welcomed by the open-source community, and our VATS algorithm has already been adopted as of MySQL's 5.7.17 release (and been made the default scheduling policy in MariaDB).},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {745–758},
numpages = {14},
keywords = {variance, transactional databases, profiling, predictability, transaction processing},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@article{10.1145/3402524,
author = {M\"{a}kitalo, Niko and Flores-Martin, Daniel and Flores, Huber and Lagerspetz, Eemil and Christophe, Francois and Ihantola, Petri and Babazadeh, Masiar and Hui, Pan and Murillo, Juan Manuel and Tarkoma, Sasu and Mikkonen, Tommi},
title = {Human Data Model: Improving Programmability of Health and Well-Being Data for Enhanced Perception and Interaction},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3402524},
doi = {10.1145/3402524},
abstract = {Today, an increasing number of systems produce, process, and store personal and intimate data. Such data has plenty of potential for entirely new types of software applications, as well as for improving old applications, particularly in the domain of smart healthcare. However, utilizing this data, especially when it is continuously generated by sensors and other devices, with the current approaches is complex—data is often using proprietary formats and storage, and mixing and matching data of different origin is not easy. Furthermore, many of the systems are such that they should stimulate interactions with humans, which further complicates the systems. In this article, we introduce the Human Data Model—a new tool and a programming model for programmers and end users with scripting skills that help combine data from various sources, perform computations, and develop and schedule computer-human interactions. Written in JavaScript, the software implementing the model can be run on almost any computer either inside the browser or using Node.js. Its source code can be freely downloaded from GitHub, and the implementation can be used with the existing IoT platforms. As a whole, the work is inspired by several interviews with professionals, and an online survey among healthcare and education professionals, where the results show that the interviewed subjects almost entirely lack ideas on how to benefit the ever-increasing amount of data measured of the humans. We believe that this is because of the missing support for programming models for accessing and handling the data, which can be satisfied with the Human Data Model.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {26},
numpages = {39},
keywords = {Internet of Things, wearable computers, IoT, pervasive computing, Mobile computing, Human Data Model, programmable world, data management, ubiquitous computing, data mashups}
}

@inproceedings{10.1145/3333581.3333590,
author = {Aksoy, Ayberk and Desai, Bipin C.},
title = {Heimdallr1: A System Design for the next Generation of IoTs},
year = {2019},
isbn = {9781450366274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333581.3333590},
doi = {10.1145/3333581.3333590},
abstract = {In today's wired and interconnected world, a sheer number of devices are now able to be connected to the internet and expose the data generated by user inputs or the devices' built-in sensors. These growing numbers of internet of things (IoT) devices are called smart and they range from mobile phones, smart televisions, IP cameras, household and industrial appliances, to WiFi thermostats and thermometers. The reason for the avalanche of IoT devices is their convenience and remote accessibility over the traditional versions. However, as with other technological breakthroughs, IoT have a major issue regarding the security of access and control of the data generated and hence privacy.To address these issues, we propose in this paper a system based approach: the system consists of two parts to monitor users' IoTs. The first aspect of this system is a firewall monitoring and controlling the incoming and outgoing traffic to and from the IoT devices which are connected to the Internet via a new generation of routers called Heimdallr. The second aspect is to store locally in this router the user's IoT related data and allow a secure interaction by the user with the data and the IoTs. A third concept introduced in the system to ensure that any updates to the IoT software is verified and certified by a central not-for-profit organization. Heimdallr would not allow any updates to the software to be made unless the update has been certified by this certification agency. The certification agency has a role similar to CSA [1] or UL [2] organizations which provide testing, inspection and certification service and are involved in setting standards. It is worth pointing out that currently in the software domain, all this is done by the for profit corporation without any public oversight. The only beacon is the open source community where dedicated developers donate their time, talent and energy to produce open source software which is often free and the source code is accessible to anyone to investigate and verify their algorithms and functions.},
booktitle = {Proceedings of the International Conference on Industrial Control Network and System Engineering Research},
pages = {92–100},
numpages = {9},
keywords = {Data security, Data privacy, IoT device, Heimdallr, IoT security},
location = {Shenyang, China},
series = {ICNSER2019}
}

@article{10.1145/3144592.3144595,
author = {Brinkman, Bo and Flick, Catherine and Gotterbarn, Don and Miller, Keith and Vazansky, Kate and Wolf, Marty J.},
title = {Dynamic Technology Challenges Static Codes of Ethics: A Case Study},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/3144592.3144595},
doi = {10.1145/3144592.3144595},
abstract = {We describe the process of changing and the changes being suggested for the ACM Code of Ethics and Professional Conduct. In addition to addressing the technical and ethical basis for the proposed changes, we identify suggestions that commenters made in response to the first draft. We invite feedback on the proposed changes and on the suggestions that commenters made.},
journal = {SIGCAS Comput. Soc.},
month = {sep},
pages = {7–24},
numpages = {18},
keywords = {professional conduct, code of ethics, ACM}
}

@inproceedings{10.1145/3267183.3267191,
author = {de Sousa, D\^{e}mora Bruna Cunha and Maia, Paulo Henrique and Rocha, Lincoln Souza and Viana, Windson},
title = {Analysing the Evolution of Exception Handling Anti-Patterns in Large-Scale Projects: A Case Study},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267191},
doi = {10.1145/3267183.3267191},
abstract = {Previous studies have shown that exception handling bad practices may impact the overall software quality. We believe that quality of exception handling code is directly affected by (i) an absence, or lack of awareness, of an explicit exception handling policy; and (ii) a silent rising and spreading of exception handling anti-patterns. To investigate such phenomenon, we conducted a case study in a large-scale Java Web system, trying to better understand the relationship between (i) and (ii). The study takes into account technical and human aspects. We surveyed 21 developers regarding their perception about exception handling in the system's institution. Next, we analyse the evolution of exception handling anti-patterns across 15 releases of the target system. Finally, we conduct a semi-structured interview with three senior software architects. Our finds beneficiated the system's institution by making it aware of these problems and enabling it to take actions towards to combat them.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {73–82},
numpages = {10},
keywords = {Exception Handling Anti-Patterns, Exception Handling, Case Study},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@inproceedings{10.1145/3503229.3547061,
author = {Ghofrani, Javad and Heravi, Paria and Babaei, Kambiz A. and Soorati, Mohammad D.},
title = {Trust Challenges in Reusing Open Source Software: An Interview-Based Initial Study},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547061},
doi = {10.1145/3503229.3547061},
abstract = {Open source projects play a significant role in software production. Most of the software projects reuse and build upon the existing open source projects and libraries. While reusing is a time and cost saving strategy, some of the key factors are often neglected that create vulnerability in the software system. We look beyond the static code analysis and dependency chain tracing to prevent vulnerabilities at the human factors level. Literature lacks a comprehensive study of the human factors perspective to the issue of trust in reusing open source projects. We performed an interview-based initial study with software developers to get an understanding of the trust issue and limitations among the practitioners. We outline some of the key trust issues in this paper and layout the first steps towards a trustworthy reuse of software.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {110–116},
numpages = {7},
keywords = {trust, open source software, package dependency, systematic reuse, reusability, empirical study},
location = {Graz, Austria},
series = {SPLC '22}
}

@article{10.1145/3449249,
author = {Geiger, R. Stuart and Howard, Dorothy and Irani, Lilly},
title = {The Labor of Maintaining and Scaling Free and Open-Source Software Projects},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449249},
doi = {10.1145/3449249},
abstract = {Free and/or open-source software (or F/OSS) projects now play a major and dominant role in society, constituting critical digital infrastructure relied upon by companies, academics, non-profits, activists, and more. As F/OSS has become larger and more established, we investigate the labor of maintaining and sustaining those projects at various scales. We report findings from an interview-based study with contributors and maintainers working in a wide range of F/OSS projects. Maintainers of F/OSS projects do not just maintain software code in a more traditional software engineering understanding of the term: fixing bugs, patching security vulnerabilities, and updating dependencies. F/OSS maintainers also perform complex and often-invisible interpersonal and organizational work to keep their projects operating as active communities of users and contributors. We particularly focus on how this labor of maintaining and sustaining changes as projects and their software grow and scale across many dimensions. In understanding F/OSS to be as much about maintaining a communal project as it is maintaining software code, we discuss broadly applicable considerations for peer production communities and other socio-technical systems more broadly.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {175},
numpages = {28},
keywords = {maintenance, open source, infrastructure, free software, labor}
}

@inproceedings{10.1145/1639950.1640066,
author = {Bacon, David F. and Chen, Yiling and Parkes, David and Rao, Malvika},
title = {A Market-Based Approach to Software Evolution},
year = {2009},
isbn = {9781605587684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1639950.1640066},
doi = {10.1145/1639950.1640066},
abstract = {Software correctness has bedeviled the field of computer science since its inception. Software complexity has increased far more quickly than our ability to control it, reaching sizes that are many orders of magnitude beyond the reach of formal or automated verification techniques.We propose a new paradigm for evaluating "correctness" based on a rich market ecosystem in which coalitions of users bid for features and fixes. Developers, testers, bug reporters, and analysts share in the rewards for responding to those bids. In fact, we suggest that the entire software development process can be driven by a disintermediated market-based mechanism driven by the desires of users and the capabilities of developers.The abstract, unspecifiable, and unknowable notion of absolute correctness is then replaced by quantifiable notions of correctness demand (the sum of bids for bugs) and correctness potential (the sum of the available profit for fixing those bugs). We then sketch the components of a market design intended to identify bugs, elicit demand for fixing bugs, and source workers for fixing bugs. The ultimate goal is to achieve a more appropriate notion of correctness, in which market forces drive software towards a correctness equilibrium in which all bugs for which there is enough value, and with low enough cost to fix, are fixed.},
booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
pages = {973–980},
numpages = {8},
keywords = {software correctness, markets, mechanism design},
location = {Orlando, Florida, USA},
series = {OOPSLA '09}
}

@inproceedings{10.1145/1866914.1866916,
author = {Smith, Ben and Austin, Andrew and Brown, Matt and King, Jason T. and Lankford, Jerrod and Meneely, Andrew and Williams, Laurie},
title = {Challenges for Protecting the Privacy of Health Information: Required Certification Can Leave Common Vulnerabilities Undetected},
year = {2010},
isbn = {9781450300940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1866914.1866916},
doi = {10.1145/1866914.1866916},
abstract = {The use of electronic health record (EHR) systems by medical professionals enables the electronic exchange of patient data, yielding cost and quality of care benefits. The United States American Recovery and Reinvestment Act (ARRA) of 2009 provides up to $34 billion for meaningful use of certified EHR systems. But, will these certified EHR systems provide the infrastructure for secure patient data exchange? As a window into the ability of current and emerging certification criteria to expose security vulnerabilities, we performed exploratory security analysis on a proprietary and an open source EHR. We were able to exploit a range of common code-level and design-level vulnerabilities. These common vulnerabilities would have remained undetected by the 2011 security certification test scripts from the Certification Commission for Health Information Technology, the most widely used certification process for EHR systems. The consequences of these exploits included, but were not limited to: exposing all users' login information, the ability of any user to view or edit health records for any patient, and creating a denial of service for all users. Based upon our results, we suggest that an enhanced set of security test scripts be used as entry criteria to the EHR certification process. Before certification bodies spend the time to certify that an EHR application is functionally complete, they should have confidence that the software system meets a basic level of security competence.},
booktitle = {Proceedings of the Second Annual Workshop on Security and Privacy in Medical and Home-Care Systems},
pages = {1–12},
numpages = {12},
keywords = {ethical hacking, medical records, xss, emr, sql injection, cchit, exploit, openemr, attack, white hat, healthcare, vulnerability, security testing, meaningful use, man-in-the-middle, ehr, dos},
location = {Chicago, Illinois, USA},
series = {SPIMACS '10}
}

@inproceedings{10.1145/3580305.3599861,
author = {Yuan, Zhuoning and Zhu, Dixian and Qiu, Zi-Hao and Li, Gang and Wang, Xuanhui and Yang, Tianbao},
title = {LibAUC: A Deep Learning Library for X-Risk Optimization},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599861},
doi = {10.1145/3580305.3599861},
abstract = {This paper introduces the award-winning deep learning (DL) library called LibAUC for implementing state-of-the-art algorithms towards optimizing a family of risk functions named X-risks. X-risks refer to a family of compositional functions in which the loss function of each data point is defined in a way that contrasts the data point with a large number of others. They have broad applications in AI for solving classical and emerging problems, including but not limited to classification for imbalanced data (CID), learning to rank (LTR), and contrastive learning of representations (CLR). The motivation of developing LibAUC is to address the convergence issues of existing libraries for solving these problems. In particular, existing libraries may not converge or require very large mini-batch sizes in order to attain good performance for these problems, due to the usage of the standard mini-batch technique in the empirical risk minimization (ERM) framework. Our library is for deep X-risk optimization (DXO) that has achieved great success in solving a variety of tasks for CID, LTR and CLR. The contributions of this paper include: (1) It introduces a new mini-batch based pipeline for implementing DXO algorithms, which differs from existing DL pipeline in the design of controlled data samplers and dynamic mini-batch losses; (2) It provides extensive benchmarking experiments for ablation studies and comparison with existing libraries. The LibAUC library features scalable performance for millions of items to be contrasted, faster and better convergence than existing libraries for optimizing X-risks, seamless PyTorch deployment and versatile APIs for various loss optimization. Our library is available to the open source community at https://github.com/Optimization-AI/LibAUC, to facilitate further academic research and industrial applications.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5487–5499},
numpages = {13},
keywords = {x-risk, library, optimization, deep learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3411764.3445616,
author = {Tahaei, Mohammad and Vaniea, Kami and Beznosov, Konstantin (Kosta) and Wolters, Maria K},
title = {Security Notifications in Static Analysis Tools: Developers’ Attitudes, Comprehension, and Ability to Act on Them},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445616},
doi = {10.1145/3411764.3445616},
abstract = {Static analysis tools (SATs) have the potential to assist developers in finding and fixing vulnerabilities in the early stages of software development, requiring them to be able to understand and act on tools’ notifications. To understand how helpful such SAT guidance is to developers, we ran an online experiment (N=132) where participants were shown four vulnerable code samples (SQL injection, hard-coded credentials, encryption, and logging sensitive data) along with SAT guidance, and asked to indicate the appropriate fix. Participants had a positive attitude towards both SAT notifications and particularly liked the example solutions and vulnerable code. Seeing SAT notifications also led to more detailed open-ended answers and slightly improved code correction answers. Still, most SAT (SpotBugs 67%, SonarQube 86%) and Control (96%) participants answered at least one code-correction question incorrectly. Prior software development experience, perceived vulnerability severity, and answer confidence all positively impacted answer accuracy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {691},
numpages = {17},
keywords = {security notifications, software developers, static analysis tools, usable security},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3377811.3380397,
author = {Garcia, Joshua and Feng, Yang and Shen, Junjie and Almanee, Sumaya and Xia, Yuan and Chen, and Qi Alfred},
title = {A Comprehensive Study of Autonomous Vehicle Bugs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380397},
doi = {10.1145/3377811.3380397},
abstract = {Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the open-source community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {385–396},
numpages = {12},
keywords = {bugs, empirical software engineering, defects, autonomous vehicles},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3603269.3604866,
author = {Brown, Matt and Fogel, Ari and Halperin, Daniel and Heorhiadi, Victor and Mahajan, Ratul and Millstein, Todd},
title = {Lessons from the Evolution of the Batfish Configuration Analysis Tool},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603269.3604866},
doi = {10.1145/3603269.3604866},
abstract = {Batfish is a tool to analyze network configurations and forwarding. It has evolved from a research prototype to an industrial-strength product, guided by scalability, fidelity, and usability challenges encountered when analyzing complex, real-world networks. We share key lessons from this evolution, including how Datalog had significant limitations when generating and analyzing forwarding state and how binary decision diagrams (BDDs) proved highly versatile. We also describe our new techniques for addressing real-world challenges, which increase Batfish performance by three orders of magnitude and enable high-fidelity analysis of networks with thousands of nodes within minutes.},
booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference},
pages = {122–135},
numpages = {14},
keywords = {configuration analysis, network verification, batfish},
location = {New York, NY, USA},
series = {ACM SIGCOMM '23}
}

@article{10.1145/3539606,
author = {Carri\'{o}n, Carmen},
title = {Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3539606},
doi = {10.1145/3539606},
abstract = {Continuous integration enables the development of microservices-based applications using container virtualization technology. Container orchestration systems such as Kubernetes, which has become the de facto standard, simplify the deployment of container-based applications. However, developing efficient and well-defined orchestration systems is a challenge. This article focuses specifically on the scheduler, a key orchestrator task that assigns physical resources to containers. Scheduling approaches are designed based on different Quality of Service (QoS) parameters to provide limited response time, efficient energy consumption, better resource utilization, and other things. This article aims to establish insight knowledge into Kubernetes scheduling, find the main gaps, and thus guide future research in the area. Therefore, we conduct a study of empirical research on Kubernetes scheduling techniques and present a new taxonomy for Kubernetes scheduling. The challenges, future direction, and research opportunities are also discussed.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {138},
numpages = {37},
keywords = {Kubernetes, containers, orchestration, survey, scheduling}
}

@inproceedings{10.1109/MSR.2017.8,
author = {Gonzalez, Danielle and Santos, Joanna C. S. and Popovich, Andrew and Mirakhorli, Mehdi and Nagappan, Mei},
title = {A Large-Scale Study on the Usage of Testing Patterns That Address Maintainability Attributes: Patterns for Ease of Modification, Diagnoses, and Comprehension},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.8},
doi = {10.1109/MSR.2017.8},
abstract = {Test case maintainability is an important concern, especially in open source and distributed development environments where projects typically have high contributor turnover with varying backgrounds and experience, and where code ownership changes often. Similar to design patterns, patterns for unit testing promote maintainability quality attributes such as ease of diagnoses, modifiability, and comprehension. In this paper, we report the results of a large-scale study on the usage of four xUnit testing patterns which can be used to satisfy these maintainability attributes. This is a first-of-its-kind study which developed automated techniques to investigate these issues across 82,447 open source projects, and the findings provide more insight into testing practices in open source projects. Our results indicate that only 17% of projects had test cases, and from the 251 testing frameworks we studied, 93 of them were being used. We found 24% of projects with test files implemented patterns that could help with maintainability, while the remaining did not use these patterns. Multiple qualitative analyses indicate that usage of patterns was an ad-hoc decision by individual developers, rather than motivated by the characteristics of the project, and that developers sometimes used alternative techniques to address maintainability concerns.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {391–401},
numpages = {11},
keywords = {unit test frameworks, mining software repositories, unit test patterns, unit testing, open source, maintenance},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.1145/3359173,
author = {Okeke, Fabian and Wasunna, Beatrice and Amulele, Mercy and Holeman, Isaac and Dell, Nicola},
title = {Including the Voice of Care Recipients in Community Health Feedback Loops in Rural Kenya},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359173},
doi = {10.1145/3359173},
abstract = {Community health programs in low-resource settings (like rural Kenya) aim to provide essential health services to vulnerable populations. However, to date, there has been limited research that explores the design of mechanisms that enable care recipients to provide feedback regarding their satisfaction with the services they receive. Such feedback has the potential to increase the motivation of community health workers (CHWs), enhance training procedures, detect fraudulent behavior, and inform key performance indicators for health programs. Our paper explores the design and deployment of a USSD-based system that allows anyone who possesses a basic mobile phone to provide feedback regarding the health services and quality of care they received from a CHW or during a hospital visit. Our system was designed through iterative fieldwork in rural Kenya that engaged with multiple stakeholder groups, including care recipients, CHWs, and high-level decision makers. After designing and testing the system, we deployed it for seven weeks in Siaya, Kenya, collecting both quantitative system usage data and qualitative data from six focus groups with 42 participants. Findings from our deployment show that 168 care recipients engaged with the system, submitting 495 reports via USSD. We discuss the broader factors impacting deployment, including the feasibility of USSD, actionability of feedback, scalability, and sustainability. Taken together, our findings suggest that USSD is a promising approach for enabling care recipients to submit feedback in a way that balances privacy, equity, and sustainability.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {71},
numpages = {20},
keywords = {USSD, mHealth, beneficiary feedback, ICTD, HCI4D, QA}
}

@article{10.1145/3387111,
author = {Wang, Zhendong and Feng, Yang and Wang, Yi and Jones, James A. and Redmiles, David},
title = {Unveiling Elite Developers’ Activities in Open Source Projects},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3387111},
doi = {10.1145/3387111},
abstract = {Open source developers, particularly the elite developers who own the administrative privileges for a project, maintain a diverse portfolio of contributing activities. They not only commit source code but also exert significant efforts on other communicative, organizational, and supportive activities. However, almost all prior research focuses on specific activities and fails to analyze elite developers’ activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open source projects hosted on GITHUB. We investigate elite developers’ contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) elite developers participate in a variety of activities, of which technical contributions (e.g., coding) only account for a small proportion; (2) as the project grows, elite developers tend to put more effort into supportive and communicative activities and less effort into coding; and (3) elite developers’ efforts in nontechnical activities are negatively correlated with the project’s outcomes in terms of productivity and quality in general, except for a positive correlation with the bug fix rate (a quality indicator). These results provide an integrated view of elite developers’ activities and can inform an individual’s decision making about effort allocation, which could lead to improved project outcomes. The results also provide implications for supporting these elite developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {16},
numpages = {35},
keywords = {open source software (OSS), project outcomes, developers’ activity, Elite developers, productivity, software quality}
}

@article{10.1145/299157.299164,
author = {O'Reilly, Tim},
title = {Lessons from Open-Source Software Development},
year = {1999},
issue_date = {April 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/299157.299164},
doi = {10.1145/299157.299164},
journal = {Commun. ACM},
month = {apr},
pages = {32–37},
numpages = {6}
}

@article{10.1145/3386335,
author = {Ingalls, Daniel},
title = {The Evolution of Smalltalk: From Smalltalk-72 through Squeak},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386335},
doi = {10.1145/3386335},
abstract = {This paper presents a personal view of the evolution of six generations of Smalltalk in which the author played a part, starting with Smalltalk-72 and progressing through Smalltalk-80 to Squeak and Etoys. It describes the forces that brought each generation into existence, the technical innovations that characterized it, and the growth in understanding of object-orientation and personal computing that emerged. It summarizes what that generation achieved and how it affected the future, both within the evolving group of developers and users, and in the outside world.  The early Smalltalks were not widely accessible because they ran only on proprietary Xerox hardware; because of this, few people have experience with these important historical artifacts. To make them accessible, the paper provides links to live simulations that can be run in present-day web browsers. These simulations offer the ability to run pre-defined scripts, but also allow the user to go off-script, browse the details of the implementation, and try anything that could be done in the original system. An appendix includes anecdotal and technical aspects of how examples of each generation of Smalltalk were recovered, and how order was teased out of chaos to the point that these old systems could be brought back to life.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {85},
numpages = {101},
keywords = {Morphic, Interpreter, Squeak, Virtual Machine, Alto, BitBlt, Blocks, NoteTaker, OOZE, Objects, Bootstrap, Smalltalk, Bytecode, EToys}
}

@inproceedings{10.1145/3319008.3319029,
author = {Ruohonen, Jukka},
title = {A Demand-Side Viewpoint to Software Vulnerabilities in WordPress Plugins},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319029},
doi = {10.1145/3319008.3319029},
abstract = {WordPress has long been the most popular content management system (CMS). This CMS powers millions and millions of websites. Although WordPress has had a particularly bad track record in terms of security, in recent years many of the well-known security risks have transmuted from the core WordPress to the numerous plugins and themes written for the CMS. Given this background, the paper analyzes known software vulnerabilities discovered from WordPress plugins. A demand-side viewpoint was used to motivate the analysis; the basic hypothesis is that plugins with large installation bases have been affected by multiple vulnerabilities. As the hypothesis also holds according to the empirical results, the paper contributes to the recent discussion about common security folklore. A few general insights are also provided about the relation between software vulnerabilities and software maintenance.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {222–228},
numpages = {7},
keywords = {vulnerability, CMS, Web security, add-on, PHP, WPVDB, plug-in},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1109/ASE51524.2021.9678585,
author = {Ksontini, Emna and Kessentini, Marouane and Ferreira, Thiago do N. and Hassan, Foyzul},
title = {Refactorings and Technical Debt in Docker Projects: An Empirical Study},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678585},
doi = {10.1109/ASE51524.2021.9678585},
abstract = {Software containers, such as Docker, are recently considered as the mainstream technology of providing reusable software artifacts. Developers can easily build and deploy their applications based on the large number of reusable Docker images that are publicly available. Thus, a current popular trend in industry is to move towards the containerization of their applications. However, container-based projects compromise different components including the Docker and Docker-compose files, and several other dependencies to the source code combining different containers and facilitating the interactions with them. Similar to any other complex systems, container-based projects are prone to various quality and technical debt issues related to different artifacts: Docker and Docker-compose files, and regular source code ones. Unfortunately, there is a gap of knowledge in how container-based projects actually evolve and are maintained.In this paper, we address the above gap by studying refactorings, i.e., structural changes while preserving the behavior, applied in open-source Docker projects, and the technical debt issues they alleviate. We analyzed 68 projects, consisting of 19,5 MLOC, along with 193 manually examined commits. The results indicate that developers refactor these Docker projects for a variety of reasons that are specific to the configuration, combination and execution of containers, leading to several new technical debt categories and refactoring types compared to existing refactoring domains. For instance, refactorings for reducing the image size of Dockerfiles, improving the extensibility of Docker-compose files, and regular source code refactorings are mainly associated with the evolution of Docker and Docker-compose files. We also introduced 24 new Docker-specific refactorings and technical debt categories, respectively, and defined different best practices. The implications of this study will assist practitioners, tool builders, and educators in improving the quality of Docker projects.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {781–791},
numpages = {11},
keywords = {maintenance, containers, refactoring, docker, technical debt},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1145/1786774.1786775,
author = {Pino, Alexandros and Kouroupetroglou, Georgios},
title = {ITHACA: An Open Source Framework for Building Component-Based Augmentative and Alternative Communication Applications},
year = {2010},
issue_date = {June 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/1786774.1786775},
doi = {10.1145/1786774.1786775},
abstract = {As an answer to the disabled community’s odyssey to gain access to adaptable, modular, multilingual, cheap and sustainable Augmentative and Alternative Communication (AAC) products, we propose the use of the ITHACA framework. It is a software environment for building component-based AAC applications, grounded on the Design for All principles and a hybrid---community and commercial---Open Source development model. ITHACA addresses the developers, the vendors, as well as the people who use AAC. We introduce a new viewpoint on the AAC product design-develop-distribute lifecycle, and a novel way to search-select-modify-maintain the AAC aid. ITHACA provides programmers with a set of tools and reusable Open Source code for building AAC software components. It also facilitates AAC product vendors to put together sophisticated applications using the available on the Web, independently premanufactured, free or commercial software parts. Furthermore, it provides people who use AAC with a variety of compatible AAC software products which incorporate multimodal, user-tailored interfaces that can fulfill their changing needs. The ITHACA architecture and the proposed fusion of past and current approaches, trends and technologies are explained. ITHACA has been successfully applied by implementing a family of AAC products, based on interchangeable components. Several ready to use ITHACA-based components, including on-screen keyboards, Text-to-Speech, symbol selection sets, e-chatting, emailing, and scanning-based input, as well as four complete communication aids addressing different user cases have been developed. This demonstration showed good acceptance of the ITHACA applications and substantial improvement of the end users’ communication skills. Developers’ experience on working in ITHACA’s Open Source projects was also positively evaluated. More importantly, the potential contribution of the component-based framework and Open Source development model combination to the AAC community emerged.},
journal = {ACM Trans. Access. Comput.},
month = {jun},
articleno = {14},
numpages = {30},
keywords = {component, framework, Augmentative and alternative communication, design for all, open source}
}

@article{10.1145/3191737,
author = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
title = {Anatomy of a Vulnerable Fitness Tracking System: Dissecting the Fitbit Cloud, App, and Firmware},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191737},
doi = {10.1145/3191737},
abstract = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {5},
numpages = {24},
keywords = {health, Nexmon, Wearables, firmware reverse engineering}
}

@inproceedings{10.1145/3351556.3351585,
author = {Kov\'{a}cs, R\'{e}ka and Horv\'{a}th, G\'{a}bor and Porkol\'{a}b, Zolt\'{a}n},
title = {Detecting C++ Lifetime Errors with Symbolic Execution},
year = {2019},
isbn = {9781450371933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351556.3351585},
doi = {10.1145/3351556.3351585},
abstract = {One of the reasons why it is so hard to statically analyze C++ source code is because of its Standard Template Library (STL). The STL is a monstrous collection of complex code base whose semantics is hard for static analyzers to understand. Unfortunately, many of the most serious memory management bugs in C++ are connected to the lifetimes of STL containers. This paper describes a method of adding knowledge of STL ownership semantics to a static analysis engine. It was implemented in an open-source symbolic execution framework widely used in the industry, and produced new and serious lifetime-related error reports in popular open-source projects.},
booktitle = {Proceedings of the 9th Balkan Conference on Informatics},
articleno = {25},
numpages = {6},
keywords = {lifetime, C++, static analysis, STL, symbolic execution},
location = {Sofia, Bulgaria},
series = {BCI'19}
}

@article{10.1145/1400181.1400211,
author = {Darking, Mary and Whitley, Edgar A. and Dini, Paolo},
title = {Governing Diversity in the Digital Ecosystem},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/1400181.1400211},
doi = {10.1145/1400181.1400211},
journal = {Commun. ACM},
month = {oct},
pages = {137–140},
numpages = {4}
}

@inproceedings{10.1145/2110363.2110403,
author = {Li, Kejia and Warren, Steve and Hatcliff, John},
title = {Component-Based App Design for Platform-Oriented Devices in a Medical Device Coordination Framework},
year = {2012},
isbn = {9781450307819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110363.2110403},
doi = {10.1145/2110363.2110403},
abstract = {Frameworks that promote the intelligent coordination of medical devices are required in ubiquitous health care environments. Such environments are essential to achieve closed-loop behavior, and an emphasis on interoperability standards and reconfigurable hardware/software components will allow these systems to adapt to changing patient/provider needs and technologies. Intelligent medical system design often focuses on smart, complex on-device algorithms. This paper addresses the use of a Medical Device Coordination Framework from the complementary perspective, where devices stream data to an information infrastructure populated with rearrangeable components that can be cascaded to form complex processing apps, or "transformers."This approach enables a new paradigm for the design of medical devices that we refer to as "medical platform-oriented devices" (MPODs): devices constructed from sensors/actuators integrated with a medical acquisition and analysis platform that hosts sophisticated software component collections while providing a variety of safety and real-time features relevant to the medical device domain. These components provide capabilities that may have been unsuitable for direct microprocessor implementation, promote the use of parallel analyses (e.g., for parameter verification), and enable the construction of reconfigurable apps.To demonstrate this approach, a set of reconfigurable, component-based software apps was developed to address medical devices that utilize photoplethysmograms (PPGs). The first app builds upon the behavior of an existing wireless reflectance pulse oximeter that (a) processes raw PPGs to ascertain heart rate, blood oxygen saturation, and respiration rate and (b) makes decisions regarding PPG viability. Other apps address the calculation of pulse wave velocity using multiple PPGs and closed-loop control of PPG quality when determining blood pressure and stroke volume. These scenarios demonstrate the immense potential that a component pool and its related app library provide for biomedical signal analyses and the creation of reconfigurable virtual medical devices.},
booktitle = {Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
pages = {343–352},
numpages = {10},
keywords = {apps, photoplethysmogram, closed-loop, pulse oximeter, components, reconfigurability, interoperability, medical device coordination framework, medical platform-oriented device},
location = {Miami, Florida, USA},
series = {IHI '12}
}

@inproceedings{10.1145/1639950.1639953,
author = {Blanton, Jay and Leski, Steve and Nicks, Brian and Tirzaman, Traian},
title = {Making SOA Work in a Healthcare Company},
year = {2009},
isbn = {9781605587684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1639950.1639953},
doi = {10.1145/1639950.1639953},
abstract = {Making SOA work in a large and diverse healthcare company is not just about bridging the gap between business and IT. It is also about bridging the gap between the technologies of yesterday, today and tomorrow. As Health Net has grown by acquiring other entities, we have acquired a landscape of diverse assets written with many languages, hosted on many platforms. These range from Java on WebLogic to .Net to RPG on iSeries to CICS on zSeries to COBOL on OpenVMS. Integrating these systems goes beyond simple business services. Successful integration ultimately requires elevating IT teams to the vision of a SOA enterprise as defined by an enterprise reference architecture. Educating our IT project teams in the fundamentals of SOA design and development has involved special approaches and a commitment to mentoring and continuous education in the enterprise. This discussion covers some of the challenges, successes, and lessons learned that we have encountered in bringing SOA to Health Net.},
booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
pages = {589–596},
numpages = {8},
keywords = {ESB, healthcare, GSOAP, SOA, health net, services},
location = {Orlando, Florida, USA},
series = {OOPSLA '09}
}

@article{10.1145/3611058,
author = {Wallner, G\"{u}nter and Wang, Letian and Dormann, Claire},
title = {Visualizing the Spatio-Temporal Evolution of Gameplay Using Storyline Visualization: A Study with League of Legends},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611058},
doi = {10.1145/3611058},
abstract = {Players increasingly adopt a data-driven approach to review and improve their gaming skills. In the wake of this, spatio-temporal visualizations gained popularity but remain challenging to design. Storyline visualizations are unique in the way they integrate time and location information into a single view to show how entity relationships develop over time. We adopt the storyline visualization technique to summarize gameplay for the purpose of post-play review. We demonstrate the method by applying it to League of Legends matches and evaluated it with 39 players of the game in a task-based online study using the triad framework for spatio-temporal queries by Peuquet. Results indicate that players responded positively to the approach and could, by and large, solve tasks well but that time-based tasks proved most challenging and least efficient to solve. Based on our findings, we reflect on possibilities for enhancing the design of storyline visualizations for game-related data analysis.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {412},
numpages = {23},
keywords = {League of Legends, Player-centric visualization, storyline visualization, match analysis}
}

@inproceedings{10.1145/2998181.2998303,
author = {Kaziunas, Elizabeth and Ackerman, Mark S. and Lindtner, Silvia and Lee, Joyce M.},
title = {Caring through Data: Attending to the Social and Emotional Experiences of Health Datafication},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998303},
doi = {10.1145/2998181.2998303},
abstract = {Designing systems to support the social context of personal data is a topic of importance in CSCW, particularly in the area of health and wellness. The relational complexities and psychological consequences of living with health data, however, are still emerging. Drawing on a 12+ month ethnography and corroborating survey data, we detail the experiences of parents using Nightscout--an open source, DIY system for remotely monitoring blood glucose data-with their children who have type one diabetes. Managing diabetes with Nightscout is a deeply relational and (at times) contested activity for parent-caregivers, whose practices reveal the tensions and vulnerabilities of caregiving work enacted through data. As engagement with personal data becomes an increasingly powerful way people experience life, our findings call for alternative data narratives that reflect a multiplicity of emotional concerns and social arrangements. We propose the analytic lens of caring-through-data as a way forward.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {2260–2272},
numpages = {13},
keywords = {remote monitoring, diy health, diabetes, health and wellness, personal health informatics, chronic illness management, self-tracking, personal data, healthcare technology, emotion work, caregiving},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/3394514.3394516,
author = {Martignano, Maurizio},
title = {A: The Compiler},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/3394514.3394516},
doi = {10.1145/3394514.3394516},
abstract = {While in the past in the C/C++ world compilers and static analyzers took two separate paths and were two separate lines of tools, nowadays they are coming back together, especially the Clang compiler and its Clang/LLVM based static analyzers. The paper will show why and how this "reunion" is beneficial, especially when analyzing large codebases. In particular the paper first will present these relatively new analyzers, then it will show how these tools are currently integrated in code quality platforms - e.g. SonarQube; finally, the paper will describe the author's recent results in terms of improving the analyzers - code quality platforms integration and facilitating the adoption and execution of static analysis in software projects.},
journal = {Ada Lett.},
month = {apr},
pages = {25–28},
numpages = {4},
keywords = {sonarqube, clang, llvm, c/c++, libadalang, libclang, static analysis}
}

@inproceedings{10.1145/3545258.3545279,
author = {Wang, Kai and Yu, Ping},
title = {AUGraft: Graft New API Usage into Old Code},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545279},
doi = {10.1145/3545258.3545279},
abstract = {Software libraries are essential in software development process. When a library evolves, client applications that rely on the library APIs are supposed to migrate their code to utilize new or updated features. Code migration is a challenging task, as developers need to spend time and effort searching for and understanding alternate API usage. Some tools collected code migration instances by mining software repositories and guided old code to evolve according to heuristic migration rules. Most of them focused on statement level modification and relied on a high degree of matching between deprecated API usage and migration instances, thus the applicable scenarios were limited. We propose a new approach, AUGraft, which automatically grafts new API usage into old code by matching and editing fine-grained abstract syntax tree elements. By searching 1,881 projects on FDroid, AUGraft collected 442 unique migration instances with Android SDK, of which 418 are correct. We applied AUGraft on 15 projects and got 367 migration results, of which 294 usages are useful, and 225 usages in them are migrated perfectly. We demonstrate AUGraft can improve the utilization of migration instances with high code migration accuracy.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {55–64},
numpages = {10},
keywords = {API evolution, mining software repositories, software maintenance},
location = {Hohhot, China},
series = {Internetware '22}
}

@inproceedings{10.1145/2998181.2998301,
author = {Poretski, Lev and Arazy, Ofer},
title = {Placing Value on Community Co-Creations: A Study of a Video Game 'Modding' Community},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998301},
doi = {10.1145/2998181.2998301},
abstract = {Firms developing software -- and in particular, video game producers - seek to leverage the community of users/developers in enhancing product offering and increasing sales. Despite the practical importance of this phenomena, to date little research has investigated the actual value such communities add aside from few qualitative case studies of successful synergies between commercial enterprises and open-source communities. The objective of this study is to try and quantitatively assess the effectiveness of firms' efforts to increase sales of their product through inducing community's co-creation activity. Our empirical investigation focuses on producers of video games and their user/developer 'modding' community. An analysis of 45 games reveals that when firms are successful at engaging the community, the value added by the modding community contributes to an increase in sales of the base product. Implications for research on open innovation and for practitioners are discussed.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {480–491},
numpages = {12},
keywords = {open innovation, game modding, game development, digital culture, software development},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/2347696.2347698,
author = {Kruchten, Philippe and Nord, Robert L. and Ozkaya, Ipek and Visser, Joost},
title = {Technical Debt in Software Development: From Metaphor to Theory Report on the Third International Workshop on Managing Technical Debt},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2347696.2347698},
doi = {10.1145/2347696.2347698},
abstract = {The technical debt metaphor is gaining significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost. This is a report on a third workshop on managing technical debt, which took place as part of the 34rd International Conference on Software Engineering (ICSE 2012). The goal of this third workshop was to discuss managing technical debt as a part of the research agenda for the software engineering field, in particular focusing on eliciting and visualizing debt, and creating payback strategies.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {36–38},
numpages = {3},
keywords = {software economics, technical debt, software quality}
}

@inproceedings{10.1109/CHASE.2019.00032,
author = {Steglich, Caio and Marczak, Sabrina and de Souza, Cleidson R. B. and Guerra, Luiz Pedro and Mosmann, Luiz Henrique and Filho, Fernando Figueira and Perin, Marcelo},
title = {Social Aspects and How They Influence MSECO Developers},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00032},
doi = {10.1109/CHASE.2019.00032},
abstract = {Mobile software ecosystem (MSECO) is a new software development paradigm for mobile technologies, having three main dimensions, namely: Technical, Business and Social. The literature has a considerable number of studies on technical and business dimensions, but only a few studies focus on the social aspects of MSECOs. However, the literature has enough to provide evidence that the actors involved, such as developers, are crucial to an MSECO. This study aims to complement earlies studies by describing new social factors that influence developers to work in a MSECO. We conducted a systematic literature review in order to identify these new factors, and a field study in which 20 developers were interviewed to understand how these factors can influence them to join or keep participating in a MSECO. We found that developer become more rigorous to continue participating then to adopt a MSECO.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {99–106},
numpages = {8},
keywords = {mobile software ecosystem, developer's collaboration, social aspects},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@inproceedings{10.1145/3510003.3510074,
author = {Wang, Sinan and Wang, Yibo and Zhan, Xian and Wang, Ying and Liu, Yepang and Luo, Xiapu and Cheung, Shing-Chi},
title = {Aper: Evolution-Aware Runtime Permission Misuse Detection for Android Apps},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510074},
doi = {10.1145/3510003.3510074},
abstract = {The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPfix, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDroid and RevDroid, and an industrial tool, Lint, on ARPfix, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 open-source Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. We reported these bugs to the app developers. So far, 17 bugs have been confirmed and seven have been fixed.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {125–137},
numpages = {13},
keywords = {compatibility issues, Android runtime permission, static analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software Product Line Engineering and Variability Management: Achievements and Challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {Software product lines, design, variability management, variability modeling, quality assurance, requirements engineering},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/2536536.2536611,
author = {Montes, Rosana and Molina, Serafina and Gea, Miguel and Bergaz, Roberto and Bravo-Lupi\'{a}\~{n}ez, David and Ramos, Antonio},
title = {Turning out a Social Community into an E-Learning Platform for MOOC: The Case of AbiertaUGR},
year = {2013},
isbn = {9781450323451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536536.2536611},
doi = {10.1145/2536536.2536611},
abstract = {This paper considers the interest of traditional high educational institution to bounce into the world of Massive Open Online Courses (MOOC). This learning approach is defined over the concept of Open Educational Resources (OER) in a basis of a high number of users that interact with the learning materials (mostly multimedia resources). This approach implies some challenges to the software platform that lies behind the learning experience. In this work we are going to evaluate the Open Source Social Networking Engine, ELGG, and how it has strongly evolved to AbiertaUGR.},
booktitle = {Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality},
pages = {489–493},
numpages = {5},
keywords = {certification, open educational resources, MOOC, recognition},
location = {Salamanca, Spain},
series = {TEEM '13}
}

@inproceedings{10.1109/ICSE-NIER52604.2021.00027,
author = {Pashchenko, Ivan and Scandariato, Riccardo and Sabetta, Antonino and Massacci, Fabio},
title = {Secure Software Development in the Era of Fluid Multi-Party Open Software and Services},
year = {2021},
isbn = {9780738133249},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER52604.2021.00027},
doi = {10.1109/ICSE-NIER52604.2021.00027},
abstract = {Pushed by market forces, software development has become fast-paced. As a consequence, modern development projects are assembled from 3rd-party components. Security &amp; privacy assurance techniques once designed for large, controlled updates over months or years, must now cope with small, continuous changes taking place within a week, and happening in sub-components that are controlled by third-party developers one might not even know they existed. In this paper, we aim to provide an overview of the current software security approaches and evaluate their appropriateness in the face of the changed nature in software development. Software security assurance could benefit by switching from a process-based to an artefact-based approach. Further, security evaluation might need to be more incremental, automated and decentralized. We believe this can be achieved by supporting mechanisms for lightweight and scalable screenings that are applicable to the entire population of software components albeit there might be a price to pay.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {91–95},
numpages = {5},
keywords = {vision, open source software, software security},
location = {Virtual Event, Spain},
series = {ICSE-NIER '21}
}

@article{10.1145/3386323,
author = {Bright, Walter and Alexandrescu, Andrei and Parker, Michael},
title = {Origins of the D Programming Language},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386323},
doi = {10.1145/3386323},
abstract = {As its name suggests, the initial motivation for the D programming language was to improve on C and C++ while keeping their spirit. The D language was to preserve those languages' efficiency, low-level access, and Algol-style syntax. The areas D set out to improve focused initially on rapid development, convenience, and simplifying the syntax without hampering expressiveness.  The genesis of D has its peculiarities, as is the case with many other languages. Walter Bright, D's creator, is a mechanical engineer by education who started out working for Boeing designing gearboxes for the 757. He was programming games on the side, and in trying to make his game Empire run faster, became interested in compilers. Despite having no experience, Bright set out in 1982 to implement a compiler that produced better code than those on the market at the time.  This interest materialized into a C compiler, followed by compilers for C++, Java, and JavaScript. Best known of these would be the Zortech C++ compiler, the first (and to date only) C++-to-native compiler developed by a single person. The D programming language began in 1999 as an effort to pull the best features of these languages into a new one. Fittingly, D would use the by that time mature C/C++ back end (optimizer and code generator) that had been under continued development and maintenance since 1982.  Between 1999 and 2006, Bright worked alone on the D language definition and its implementation, although a steadily increasing volume of patches from users was incorporated. The new language would be based on the past successes of the languages he'd used and implemented, but would be clearly looking to the future. D started with choices that are obvious today but were less clear winners back in the 1990s: full support for Unicode, IEEE floating point, 2s complement arithmetic, and flat memory addressing (memory is treated as a linear address space with no segmentation). It would do away with certain compromises from past languages imposed by shortages of memory (for example, forward declarations would not be required). It would primarily appeal to C and C++ users, as expertise with those languages would be readily transferrable. The interface with C was designed to be zero cost.  The language design was begun in late 1999. An alpha version appeared in 2001 and the initial language was completed, somewhat arbitrarily, at version 1.0 in January 2007. During that time, the language evolved considerably, both in capability and in the accretion of a substantial worldwide community that became increasingly involved with contributing. The front end was open-sourced in April 2002, and the back end was donated by Symantec to the open source community in 2017. Meanwhile, two additional open-source back ends became mature in the 2010s: `gdc` (using the same back end as the GNU C++ compiler) and `ldc` (using the LLVM back end).  The increasing use of the D language in the 2010s created an impetus for formalization and development management. To that end, the D Language Foundation was created in September 2015 as a nonprofit corporation overseeing work on D's definition and implementation, publications, conferences, and collaborations with universities.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {73},
numpages = {38},
keywords = {Programming Languages}
}

@inproceedings{10.1145/2901739.2901745,
author = {Rahman, Md Tajmilur and Querel, Louis-Philippe and Rigby, Peter C. and Adams, Bram},
title = {Feature Toggles: Practitioner Practices and a Case Study},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901745},
doi = {10.1145/2901739.2901745},
abstract = {Continuous delivery and rapid releases have led to innovative techniques for integrating new features and bug fixes into a new release faster. To reduce the probability of integration conflicts, major software companies, including Google, Facebook and Netflix, use feature toggles to incrementally integrate and test new features instead of integrating the feature only when it's ready. Even after release, feature toggles allow operations managers to quickly disable a new feature that is behaving erratically or to enable certain features only for certain groups of customers. Since literature on feature toggles is surprisingly slim, this paper tries to understand the prevalence and impact of feature toggles. First, we conducted a quantitative analysis of feature toggle usage across 39 releases of Google Chrome (spanning five years of release history). Then, we studied the technical debt involved with feature toggles by mining a spreadsheet used by Google developers for feature toggle maintenance. Finally, we performed thematic analysis of videos and blog posts of release engineers at major software companies in order to further understand the strengths and drawbacks of feature toggles in practice. We also validated our findings with four Google developers. We find that toggles can reconcile rapid releases with long-term feature development and allow flexible control over which features to deploy. However they also introduce technical debt and additional maintenance for developers.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {201–211},
numpages = {11},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1145/3386320,
author = {Stroustrup, Bjarne},
title = {Thriving in a Crowded and Changing World: C++ 2006–2020},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386320},
doi = {10.1145/3386320},
abstract = {By 2006, C++ had been in widespread industrial use for 20 years. It contained parts that had survived unchanged since introduced into C in the early 1970s as well as features that were novel in the early 2000s. From 2006 to 2020, the C++ developer community grew from about 3 million to about 4.5 million. It was a period where new programming models emerged, hardware architectures evolved, new application domains gained massive importance, and quite a few well-financed and professionally marketed languages fought for dominance. How did C++ -- an older language without serious commercial backing -- manage to thrive in the face of all that?  This paper focuses on the major changes to the ISO C++ standard for the 2011, 2014, 2017, and 2020 revisions. The standard library is about 3/4 of the C++20 standard, but this paper's primary focus is on language features and the programming techniques they support.  The paper contains long lists of features documenting the growth of C++. Significant technical points are discussed and illustrated with short code fragments. In addition, it presents some failed proposals and the discussions that led to their failure. It offers a perspective on the bewildering flow of facts and features across the years. The emphasis is on the ideas, people, and processes that shaped the language.  Themes include efforts to preserve the essence of C++ through evolutionary changes, to simplify its use, to improve support for generic programming, to better support compile-time programming, to extend support for concurrency and parallel programming, and to maintain stable support for decades' old code.  The ISO C++ standard evolves through a consensus process. Inevitably, there is competition among proposals and clashes (usually polite ones) over direction, design philosophies, and principles. The committee is now larger and more active than ever, with as many as 250 people turning up to week-long meetings three times a year and many more taking part electronically. We try (not always successfully) to mitigate the effects of design by committee, bureaucratic paralysis, and excessive enthusiasm for a variety of language fashions.  Specific language-technical topics include the memory model, concurrency and parallelism, compile-time computation, move-semantics, exceptions, lambda expressions, and modules. Designing a mechanism for specifying a template's requirements on its arguments that is sufficiently flexible and precise yet doesn't impose run-time costs turned out to be hard. The repeated attempts to design ``concepts'' to do that have their roots back in the 1980s and touch upon many key design issues for C++ and for generic programming.  The description is based on personal participation in the key events and design decisions, backed by the thousands of papers and hundreds of meeting minutes in the ISO C++ standards committee's archives.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {70},
numpages = {168},
keywords = {resource management, C++, generic programming, simplification of language use, concurrency and parallelism, programming language design and evolution, standardization}
}

@inproceedings{10.1145/3597926.3598147,
author = {Jayasuriya, Dhanushka and Terragni, Valerio and Dietrich, Jens and Ou, Samuel and Blincoe, Kelly},
title = {Understanding Breaking Changes in the Wild},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598147},
doi = {10.1145/3597926.3598147},
abstract = {Modern software applications rely heavily on the usage of libraries, which provide reusable functionality, to accelerate the development process. As libraries evolve and release new versions, the software systems that depend on those libraries (the clients) should update their dependencies to use these new versions as the new release could, for example, include critical fixes for security vulnerabilities. However, updating is not always a smooth process, as it can result in software failures in the clients if the new version includes breaking changes. Yet, there is little research on how these breaking changes impact the client projects in the wild. To identify if changes between two library versions cause breaking changes at the client end, we perform an empirical study on Java projects built using Maven. For the analysis, we used 18,415 Maven artifacts, which declared 142,355 direct dependencies, of which 71.60% were not up-to-date. We updated these dependencies and found that 11.58% of the dependency updates contain breaking changes that impact the client. We further analyzed these changes in the library which impact the client projects and examine if libraries have adhered to the semantic versioning scheme when introducing breaking changes in their releases. Our results show that changes in transitive dependencies were a major factor in introducing breaking changes during dependency updates and almost half of the detected client impacting breaking changes violate the semantic versioning scheme by introducing breaking changes in non-Major updates.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1433–1444},
numpages = {12},
keywords = {software libraries, software evolution, software dependency, breaking changes},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3238147.3238197,
author = {Habchi, Sarra and Blanc, Xavier and Rouvoy, Romain},
title = {On Adopting Linters to Deal with Performance Concerns in Android Apps},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238197},
doi = {10.1145/3238147.3238197},
abstract = {With millions of applications (apps) distributed through mobile markets, engaging and retaining end-users challenge Android developers to deliver a nearly perfect user experience. As mobile apps run in resource-limited devices, performance is a critical criterion for the quality of experience. Therefore, developers are expected to pay much attention to limit performance bad practices. On the one hand, many studies already identified such performance bad practices and showed that they can heavily impact app performance. Hence, many static analysers, a.k.a. linters, have been proposed to detect and fix these bad practices. On the other hand, other studies have shown that Android developers tend to deal with performance reactively and they rarely build on linters to detect and fix performance bad practices. In this paper, we therefore perform a qualitative study to investigate this gap between research and development community. In particular, we performed interviews with 14 experienced Android developers to identify the perceived benefits and constraints of using linters to identify performance bad practices in Android apps. Our observations can have a direct impact on developers and the research community. Specifically, we describe why and how developers leverage static source code analysers to improve the performance of their apps. On top of that, we bring to light important challenges faced by developers when it comes to adopting static analysis for performance purposes.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {6–16},
numpages = {11},
keywords = {linters, Android, static analysis, performance},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.5555/2819289.2819293,
author = {Cerqueus, Thomas and de Almeida, Eduardo Cunha and Scherzinger, Stefanie},
title = {Safely Managing Data Variety in Big Data Software Development},
year = {2015},
publisher = {IEEE Press},
abstract = {We consider the task of building Big Data software systems, offered as software-as-a-service. These applications are commonly backed by NoSQL data stores that address the proverbial Vs of Big Data processing: NoSQL data stores can handle large volumes of data and many systems do not enforce a global schema, to account for structural variety in data. Thus, software engineers can design the data model on the go, a flexibility that is particularly crucial in agile software development. However, NoSQL data stores commonly do not yet account for the veracity of changes when it comes to changes in the structure of persisted data. Yet this is an inevitable consequence of agile software development. In most NoSQL-based application stacks, schema evolution is completely handled within the application code, usually involving object mapper libraries. Yet simple code refactorings, such as renaming a class attribute at the source code level, can cause data loss or runtime errors once the application has been deployed to production. We address this pain point by contributing type checking rules that we have implemented within an IDE plugin. Our plugin ControVol statically type checks the object mapper class declarations against the code release history. ControVol is thus capable of detecting common yet risky cases of mismatched data and schema, and can even suggest automatic fixes.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {4–10},
numpages = {7},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/1882486.1882491,
author = {Bos, Herbert and Huang, Kaiming},
title = {CacheCard: Caching Static and Dynamic Content on the NIC},
year = {2009},
isbn = {9781605586304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882486.1882491},
doi = {10.1145/1882486.1882491},
abstract = {CacheCard is a NIC-based cache for static and dynamic web content in a way that allows for implementation on simple devices like NICs. It requires neither understanding of the way dynamic data is generated, nor execution of scripts on the cache. By monitoring file system activity and potential non-determinism incurred by scripts, we determine all data sources for specific requests. For instance, if a deterministic script opens a set of files or a database tables, these files and tables, as well as the script itself will be in the set of data sources for this URL. Caching the dynamic data is possible, since we can invalidate cache entries when any of the sources changes. Non-deterministic scripts that produce content based on time or random values are automatically recognised and flagged as non-cacheable. We implemented CacheCard on Intel IXP2400 network processors.},
booktitle = {Proceedings of the 5th ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {1–10},
numpages = {10},
location = {Princeton, New Jersey},
series = {ANCS '09}
}

@article{10.1145/2542661.2544374,
author = {Chow, Fred},
title = {Intermediate Representation: The Increasing Significance of Intermediate Representations in Compilers},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {10},
issn = {1542-7730},
url = {https://doi.org/10.1145/2542661.2544374},
doi = {10.1145/2542661.2544374},
abstract = {Program compilation is a complicated process. A compiler is a software program that translates a high-level source language program into a form ready to execute on a computer. Early in the evolution of compilers, designers introduced IRs (intermediate representations, also commonly called intermediate languages) to manage the complexity of the compilation process. The use of an IR as the compiler’s internal representation of the program enables the compiler to be broken up into multiple phases and components, thus benefiting from modularity.},
journal = {Queue},
month = {oct},
pages = {30–37},
numpages = {8}
}

@inproceedings{10.1145/3474369.3486865,
author = {Deshpande, Chinmay and Gens, David and Franz, Michael},
title = {StackBERT: Machine Learning Assisted Static Stack Frame Size Recovery on Stripped and Optimized Binaries},
year = {2021},
isbn = {9781450386579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474369.3486865},
doi = {10.1145/3474369.3486865},
abstract = {The call stack represents one of the core abstractions that compiler-generated programs leverage to organize binary execution at runtime. For many use cases reasoning about stack accesses of binary functions is crucial: security-sensitive applications may require patching even after deployment, and binary instrumentation, rewriting, and lifting all necessitate detailed knowledge about the function frame layout of the affected program. As no comprehensive solution to the stack symbolization problem exists to date, existing approaches have to resort to workarounds like emulated stack environments, resulting in increased runtime overheads.In this paper we present StackBERT, a framework to statically reason about and reliably recover stack frame information of binary functions in stripped and highly optimized programs. The core idea behind our approach is to formulate binary analysis as a self-supervised learning problem by automatically generating ground truth data from a large corpus of open-source programs. We train a state-of-the-art Transformer model with self-attention and finetune for stack frame size prediction. We show that our finetuned model yields highly accurate estimates of a binary function's stack size from its function body alone across different instruction-set architectures, compiler toolchains, and optimization levels. We successfully verify the static estimates against runtime data through dynamic executions of standard benchmarks and additional studies, demonstrating that StackBERT's predictions generalize to 93.44% of stripped and highly optimized test binaries not seen during training. We envision these results to be useful for improving binary rewriting and lifting approaches in the future.},
booktitle = {Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security},
pages = {85–95},
numpages = {11},
keywords = {machine learning, binary lifting, stack symbolization, recompilation},
location = {Virtual Event, Republic of Korea},
series = {AISec '21}
}

@inproceedings{10.1145/2597073.2597079,
author = {Bloemen, Remco and Amrit, Chintan and Kuhlmann, Stefan and Ord\'{o}\~{n}ez–Matamoros, Gonzalo},
title = {Innovation Diffusion in Open Source Software: Preliminary Analysis of Dependency Changes in the Gentoo Portage Package Database},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597079},
doi = {10.1145/2597073.2597079},
abstract = {In this paper we make the case that software dependencies are a form of innovation adoption. We then test this on the time-evolution of the Gentoo package dependency graph. We find that the Bass model of innovation diffusion fits the growth of the number of packages depending on a given library. Interestingly, we also find that low-level packages have a primarily imitation driven adoption and multimedia libraries have primarily innovation driven growth.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {316–319},
numpages = {4},
keywords = {Innovation, Gentoo, graph, dependencies},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/126551.126556,
author = {Encontre, Vincent},
title = {SDL: A Standard Language for Ada Real-Time Applications},
year = {1991},
isbn = {0897914457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/126551.126556},
doi = {10.1145/126551.126556},
booktitle = {Proceedings of the Conference on TRI-Ada '91: Today's Accomplishments; Tomorrow's Expectations},
pages = {45–53},
numpages = {9},
location = {San Jose, California, USA},
series = {TRI-Ada '91}
}

@inproceedings{10.1145/571878.571887,
author = {Garc\'{\i}a, Pedro and Montal\`{a}, Oriol and Pairot, Carles and Rallo, Robert and Skarmeta, Antonio G\'{o}mez},
title = {MOVE: Component Groupware Foundations for Collaborative Virtual Environments},
year = {2002},
isbn = {1581134894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/571878.571887},
doi = {10.1145/571878.571887},
abstract = {The design of a Virtual Environment (VE) is a distributed problem of multi-user access to shared resources. Such problem requires careful design decisions in order to provide a seamless system infrastructure capable of supporting flexible interactions in the shared scenarios.The complexity of this domain has led to intricate software systems that provide ad-hoc solutions to specific problems. Furthermore, many of them have gone to a dead end, due to their non-extensible design and their lack of code and module reuse.This paper presents a VE that is constructed on top of a component groupware framework. Our major aim is to provide an extensible infrastructure offering a set of collaborative services in a seamless way. At the conceptual level, it provides essential collaborative services: shared sessions, support for synchronous and asynchronous components, security, coordination, and a server-side awareness infrastructure. At the architectural level, the framework is constructed on top of a middleware integration platform and uses high performance publish/subscribe notification services. Finally, we present the advantages and limitations of this approach.},
booktitle = {Proceedings of the 4th International Conference on Collaborative Virtual Environments},
pages = {55–62},
numpages = {8},
keywords = {virtual environments, frameworks, distributed systems, component groupware},
location = {Bonn, Germany},
series = {CVE '02}
}

@inproceedings{10.1145/2661136.2661159,
author = {Acher, Mathieu and Combemale, Benoit and Collet, Philippe},
title = {Metamorphic Domain-Specific Languages: A Journey into the Shapes of a Language},
year = {2014},
isbn = {9781450332101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661136.2661159},
doi = {10.1145/2661136.2661159},
abstract = {External or internal domain-specific languages (DSLs) or (fluent) APIs' Whoever you are - a developer or a user of a DSL - you usually have to choose side; you should not! What about metamorphic DSLs that change their shape according to your needs? Our 4-years journey of providing the "right" support (in the domain of feature modeling), led us to develop an external DSL, different shapes of an internal API, and maintain all these languages. A key insight is that there is no one-size-fits-all solution or no clear superiority of a solution compared to another. On the contrary, we found that it does make sense to continue the maintenance of an external and internal DSL. Based on our experience and on an analysis of the DSL engineering field, the vision that we foresee for the future of software languages is their ability to be self-adaptable to the most appropriate shape (including the corresponding integrated development environment) according to a particular usage or task. We call metamorphic DSL such a language, able to change from one shape to another shape.},
booktitle = {Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {243–253},
numpages = {11},
keywords = {programming, metamorphic, domain-specific languages},
location = {Portland, Oregon, USA},
series = {Onward! 2014}
}

@article{10.1145/3540202,
author = {Mashayekhi, Mehdi and Ajmeri, Nirav and List, George F. and Singh, Munindar P.},
title = {Prosocial Norm Emergence in Multi-Agent Systems},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1–2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3540202},
doi = {10.1145/3540202},
abstract = {Multi-agent systems provide a basis for developing systems of autonomous entities and thus find application in a variety of domains. We consider a setting where not only the member agents are adaptive but also the multi-agent system viewed as an entity in its own right is adaptive. Specifically, the social structure of a multi-agent system can be reflected in the social norms among its members. It is well recognized that the norms that arise in society are not always beneficial to its members. We focus on prosocial norms, which help achieve positive outcomes for society and often provide guidance to agents to act in a manner that takes into account the welfare of others.Specifically, we propose Cha, a framework for the emergence of prosocial norms. Unlike previous norm emergence approaches, Cha supports continual change to a system (agents may enter and leave) and dynamism (norms may change when the environment changes). Importantly, Cha agents incorporate prosocial decision-making based on inequity aversion theory, reflecting an intuition of guilt arising from being antisocial. In this manner, Cha brings together two important themes in prosociality: decision-making by individuals and fairness of system-level outcomes. We demonstrate via simulation that Cha can improve aggregate societal gains and fairness of outcomes.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {sep},
articleno = {3},
numpages = {24},
keywords = {prosociality, decentralized multi-agent systems, Fairness, socio-technical systems, ethics}
}

@inproceedings{10.1145/3520313.3534659,
author = {Misu, Md Rakib Hossain and Janjanin, Aleksandar Sa\v{s}a and Bian, Zhiqiang and Burlacu, Valentin-Sebastian and Anteski, Naum},
title = {ADA: A Tool for Visualizing the Architectural Overview of Open-Source Repositories},
year = {2022},
isbn = {9781450392747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520313.3534659},
doi = {10.1145/3520313.3534659},
abstract = {Writing highly maintainable and efficient software code is becoming increasingly difficult, especially while following the rapid, agile development process and working in a distributed team. One of the key indicators of that inefficient software design is a high degree of code coupling, which leads to unwanted side-effects during refactoring and acts as a burden during future development. To alleviate these problems, we developed a visualization tool, ADA, that statically analyzes an open-source repository and seeks to address the issue of code coupling by providing developers with a powerful graphic representation. ADA showcases the relationships and the degree of inter-connectivity between the classes. ADA will ultimately guide developers to instantly locate the coupled area and assist them in decoupling it.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
pages = {30–35},
numpages = {6},
keywords = {Visualization, Code Coupling, Static Analysis},
location = {San Diego, CA, USA},
series = {SOAP 2022}
}

@inproceedings{10.1145/2889160.2889251,
author = {Kulkarni, Vinay},
title = {Model Driven Development of Business Applications: A Practitioner's Perspective},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889251},
doi = {10.1145/2889160.2889251},
abstract = {We discuss our experience in use of models and model-driven techniques for developing large business applications. Benefits accrued and limitations observed are highlighted. We describe possible means of overcoming some of the limitations and experience thereof. A case for shift in focus of model driven engineering (MDE) community in the context of large enterprises is argued. Though emerging from a specific context, we think, the takeaways from this experience may have a more general appeal for MDE practitioners, tool vendors and researchers.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {260–269},
numpages = {10},
keywords = {separation of concerns, model driven engineering workbench, modeling, software product-lines, model transformation, meta modeling},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/3011286.3011291,
author = {Galster, Matthias and Zdun, Uwe and Weyns, Danny and Rabiser, Rick and Zhang, Bo and Goedicke, Michael and Perrouin, Gilles},
title = {Variability and Complexity in Software Design: Towards a Research Agenda},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011291},
doi = {10.1145/3011286.3011291},
abstract = {Many of today's software systems accommodate different usage and deployment scenarios. Intentional and unintentional variability in functionality or quality attributes (e.g., performance) of software significantly increases the complexity of the problem and design space of those systems. The complexity caused by variability becomes increasingly difficult to handle due to the increasing size of software systems, new and emerging application domains, dynamic operating conditions under which software systems have to operate, fast moving and highly competitive markets, and more powerful and versatile hardware. This paper reports results of the first International Workshop on Variability and Complexity in Software Design that brought together researchers and engineers interested in the topic of complexity and variability. It also outlines directions the field might move in the future},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {27–30},
numpages = {4},
keywords = {complexity, software design, Variability}
}

@inproceedings{10.1145/3524842.3528460,
author = {Kochanthara, Sangeeth and Dajsuren, Yanja and Cleophas, Loek and van den Brand, Mark},
title = {Painting the Landscape of Automotive Software in GitHub},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528460},
doi = {10.1145/3524842.3528460},
abstract = {The automotive industry has transitioned from being an electromechanical to a software-intensive industry. A current high-end production vehicle contains 100 million+ lines of code surpassing modern airplanes, the Large Hadron Collider, the Android OS, and Facebook's front-end software, in code size by a huge margin. Today, software companies worldwide, including Apple, Google, Huawei, Baidu, and Sony are reportedly working to bring their vehicles to the road. This paper ventures into the automotive software landscape in open source, providing a first glimpse into this multi-disciplinary industry with a long history of closed source development. We paint the landscape of automotive software on GitHub by describing its characteristics and development styles.The landscape is defined by 15,000+ users contributing to ≈600 actively-developed automotive software projects created in a span of 12 years from 2010 until 2021. These projects range from vehicle dynamics-related software; firmware and drivers for sensors like LiDAR and camera; algorithms for perception and motion control; to complete operating systems integrating the above. Developments in the field are spearheaded by industry and academia alike, with one in three actively developed automotive software repositories owned by an organization. We observe shifts along multiple dimensions, including preferred language from MATLAB to Python and prevalence of perception and decision-related software over traditional automotive software. This study witnesses open source automotive software boom in its infancy with many implications for future research and practice.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {215–226},
numpages = {12},
keywords = {automotive software, cyber-physical systems, mining software repositories, GitHub, open source, safety critical, software engineering},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@article{10.1145/2534706.2534720,
author = {Chow, Fred},
title = {Intermediate Representation},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/2534706.2534720},
doi = {10.1145/2534706.2534720},
abstract = {The increasing significance of intermediate representations in compilers.},
journal = {Commun. ACM},
month = {dec},
pages = {57–62},
numpages = {6}
}

@article{10.1145/1713254.1713267,
author = {Rakotoarivelo, Thierry and Ott, Maximilian and Jourjon, Guillaume and Seskar, Ivan},
title = {OMF: A Control and Management Framework for Networking Testbeds},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1713254.1713267},
doi = {10.1145/1713254.1713267},
abstract = {Networking testbeds are playing an increasingly important role in the development of new communication technologies. Testbeds are traditionally built for a particular project or to study a specific technology. An alternative approach is to federate existing testbeds to a) cater for experimenter needs which cannot be fullled by a single testbed, and b) provide a wider variety of environmental settings at different scales. These heterogenous settings allow the study of new approaches in environments similar to what one finds in the real world.This paper presents OMF, a control, measurement, and management framework for testbeds. It describes through some examples the versatility of OMF's current architecture and gives directions for federation of testbeds through OMF. In addition, this paper introduces a comprehensive experiment description language that allows an experimenter to describe resource requirements and their configurations, as well as experiment orchestration. Researchers would thus be able to reproduce their experiment on the same testbed or in a different environment with little changes. Along with the efficient support for large scale experiments, the use of testbeds and support for repeatable experiments will allow the networking field to build a culture of cross verification and therefore strengthen its scientific approach.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jan},
pages = {54–59},
numpages = {6}
}

@inproceedings{10.1145/3377813.3381348,
author = {Wang, Pei and Ding, Yu and Sun, Mingshen and Wang, Huibo and Li, Tongxin and Zhou, Rundong and Chen, Zhaofeng and Jing, Yiming},
title = {Building and Maintaining a Third-Party Library Supply Chain for Productive and Secure SGX Enclave Development},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381348},
doi = {10.1145/3377813.3381348},
abstract = {The big data industry is facing new challenges as concerns about privacy leakage soar. One of the remedies to privacy breach incidents is to encapsulate computations over sensitive data within hardware-assisted Trusted Execution Environments (TEE). Such TEE-powered software is called secure enclaves. Secure enclaves hold various advantages against competing for privacy-preserving computation solutions. However, enclaves are much more challenging to build compared with ordinary software. The reason is that the development of TEE software must follow a restrictive programming model to make effective use of strong memory encryption and segregation enforced by hardware. These constraints transitively apply to all third-party dependencies of the software. If these dependencies do not officially support TEE hardware, TEE developers have to spend additional engineering effort in porting them. High development and maintenance cost is one of the major obstacles against adopting TEE-based privacy protection solutions in production.In this paper, we present our experience and achievements with regard to constructing and continuously maintaining a third-party library supply chain for TEE developers. In particular, we port a large collection of Rust third-party libraries into Intel SGX, one of the most mature trusted computing platforms. Our supply chain accepts upstream patches in a timely manner with SGX-specific security auditing. We have been able to maintain the SGX ports of 159 open-source Rust libraries with reasonable operational costs. Our work can effectively reduce the engineering cost of developing SGX enclaves for privacy-preserving data processing and exchange.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {100–109},
numpages = {10},
keywords = {privacy-preserving computation, SGX, software supply chain, rust, third-party library},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/2642803.2647716,
author = {Axelsson, Jakob and Kobetski, Avenir},
title = {Architectural Concepts for Federated Embedded Systems},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2647716},
doi = {10.1145/2642803.2647716},
abstract = {Federated embedded systems (FES) is an approach for systems-of-systems engineering in the domain of cyber-physical systems. It is based on the idea to allow dynamic addition of plug-in software in the embedded system of a product, and through communication between the plug-ins in different products, it becomes possible to build services on the level of a federation of products. In this paper, architectural concerns for FES are elicited, and are used as rationale for a number of decisions in the architecture of products that are enabled for FES, as well as in the application architecture of a federation. A concrete implementation of a FES from the automotive domain is also described, as a validation of the architectural concepts presented.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {25},
numpages = {8},
keywords = {cyber-physical systems, federated embedded systems, system architecture, Systems-of-systems},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3546932.3547000,
author = {Zajdel, Stan and Costa, Diego Elias and Mili, Hafedh},
title = {Open Source Software: An Approach to Controlling Usage and Risk in Application Ecosystems},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547000},
doi = {10.1145/3546932.3547000},
abstract = {The Open Source Software movement has been growing exponentially for a number of years with no signs of slowing. Driving this growth is the wide-spread availability of libraries and frameworks that provide many functionalities. Developers are saving time and money incorporating this functionality into their applications resulting in faster more feature-rich releases. Despite the growing success and the advantages that open source software provides, there is a dark side. Due to its community construction and largely unregulated distribution, the majority of open source software contains bugs, vulnerabilities and other issues making it highly susceptible to exploits. The lack of oversight in general hinders the quality of this software resulting in a trickle down effect in the applications that use it. Additionally, developers who use open source tend to arbitrarily download the software into their build systems but rarely keep track of what they have downloaded resulting in an excessive amount of open source software in their applications and in their ecosystem. This paper discusses processes and practices that users of open source software can implement into their environments that can safely track and control the introduction and usage of open source software into their applications, and report on some preliminary results obtained in an industrial context. We conclude by discussing governance issues related to the disciplined use and reuse of open source and areas for further improvements.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {154–163},
numpages = {10},
keywords = {DevSecOps, dependencies, open source software, maven, NPM},
location = {Graz, Austria},
series = {SPLC '22}
}

@inproceedings{10.1145/3307630.3342398,
author = {Beek, Maurice H. ter and Schmid, Klaus and Eichelberger, Holger},
title = {Textual Variability Modeling Languages: An Overview and Considerations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342398},
doi = {10.1145/3307630.3342398},
abstract = {During the three decades since the invention of the first variability modeling approach [28], there have been multiple attempts to introduce advanced variability modeling capabilities. More recently, we have seen increased attention on textual variability modeling languages. In this paper, we summarize the main capabilities of state of the art textual variability modeling languages, based on [23], including updates regarding more recent work. Based on this integrated characterization, we provide a discussion of additional concerns, opportunities and challenges that are relevant for designing future (textual) variability modeling languages. The paper also summarizes relevant contributions by the authors as input to further discussions on future (textual) variability modeling languages.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {151–157},
numpages = {7},
keywords = {variability modeling, software product lines, textual specification languages},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1145/3624738,
author = {Zheng, Xiaoye and Wan, Zhiyuan and Zhang, Yun and Chang, Rui and Lo, David},
title = {A Closer Look at the Security Risks in the Rust Ecosystem},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624738},
doi = {10.1145/3624738},
abstract = {Rust is an emerging programming language designed for the development of systems software. To facilitate the reuse of Rust code, crates.io, as a central package registry of the Rust ecosystem, hosts thousands of third-party Rust packages. The openness of crates.io enables the growth of the Rust ecosystem but comes with security risks by severe security advisories. Although Rust guarantees a software program to be safe via programming language features and strict compile-time checking, the unsafe keyword in Rust allows developers to bypass compiler safety checks for certain regions of code. Prior studies empirically investigate the memory safety and concurrency bugs in the Rust ecosystem, as well as the usage of unsafe keywords in practice. Nonetheless, the literature lacks a systematic investigation of the security risks in the Rust ecosystem. In this paper, we perform a comprehensive investigation into the security risks present in the Rust ecosystem, asking “what are the characteristics of the vulnerabilities, what are the characteristics of the vulnerable packages, and how are the vulnerabilities fixed in practice?”. To facilitate the study, we first compile a dataset of 433 vulnerabilities, 300 vulnerable code repositories, and 218 vulnerability fix commits in the Rust ecosystem, spanning over 7 years. With the dataset, we characterize the types, life spans, and evolution of the disclosed vulnerabilities. We then characterize the popularity, categorization, and vulnerability density of the vulnerable Rust packages, as well as their versions and code regions affected by the disclosed vulnerabilities. Finally, we characterize the complexity of vulnerability fixes and localities of corresponding code changes, and inspect how practitioners fix vulnerabilities in Rust packages with various localities. We find that memory safety and concurrency issues account for nearly two thirds of the vulnerabilities in the Rust ecosystem. It takes over 2 years for the vulnerabilities to become publicly disclosed, and one third of the vulnerabilities have no fixes committed before their disclosure. In terms of vulnerability density, we observe a continuous upward trend at the package level over time, but a decreasing trend at the code level since August 2020. In the vulnerable Rust packages, the vulnerable code tends to be localized at the file level, and contains statistically significantly more unsafe functions and blocks than the rest of the code. More popular packages tend to have more vulnerabilities, while the less popular packages suffer from vulnerabilities for more versions. The vulnerability fix commits tend to be localized to a limited number of lines of code. Developers tend to address vulnerable safe functions by adding safe functions or lines to them, vulnerable unsafe blocks by removing them, and vulnerable unsafe functions by modifying unsafe trait implementations. Based on our findings, we discuss implications, provide recommendations for software practitioners, and outline directions for future research.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
keywords = {security risks, empirical study, ecosystem, Rust, vulnerability}
}

@inproceedings{10.1145/3180155.3180156,
author = {Banken, Herman and Meijer, Erik and Gousios, Georgios},
title = {Debugging Data Flows in Reactive Programs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180156},
doi = {10.1145/3180155.3180156},
abstract = {Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console.In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {752–763},
numpages = {12},
keywords = {program comprehension, visualization, reactive programming, debugging},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/3576037,
author = {Venturini, Daniel and Cogo, Filipe Roseiro and Polato, Ivanilton and Gerosa, Marco A. and Wiese, Igor Scaliante},
title = {I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3576037},
doi = {10.1145/3576037},
abstract = {Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages’ builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider’s version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {94},
numpages = {26},
keywords = {Breaking changes, dependency management, change impact, Semantic Version, npm}
}

@inproceedings{10.1145/3178248.3178252,
author = {Buchwald, Hagen},
title = {Subject-Orientation and Agility},
year = {2018},
isbn = {9781450353601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178248.3178252},
doi = {10.1145/3178248.3178252},
abstract = {This contribution sketches how S-BPM in the sense of a code generation approach could evolve to subject orientation in the sense of subject-oriented programming. It compares the existing agile approach of object-oriented software development with the subject-oriented approach. 10 theses are outlined of how the subject-oriented approach could benefit from the object-oriented approach and vice versa.},
booktitle = {Proceedings of the 10th International Conference on Subject-Oriented Business Process Management},
articleno = {2},
numpages = {7},
keywords = {Agile Software Engineering (ASE), Abstract State Machine (ASM), Subject-orientation, Agile Project Management (APM), Abstract Data Type(ADT), Domain Driven Design (DDD), Agile Requirements Engineering (ARE)},
location = {Linz, Austria},
series = {S-BPM One '18}
}

@inproceedings{10.1145/3427228.3427658,
author = {Nguyen, Duc Cuong and Derr, Erik and Backes, Michael and Bugiel, Sven},
title = {Up2Dep: Android Tool Support to Fix Insecure Code Dependencies},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427658},
doi = {10.1145/3427228.3427658},
abstract = {Third-party libraries, especially outdated versions, can introduce and multiply security &amp; privacy related issues to Android applications. While prior work has shown the need for tool support for developers to avoid libraries with security problems, no such a solution has yet been brought forward to Android. It is unclear how such a solution would work and which challenges need to be solved in realizing it. In this work, we want to make a step forward in this direction. We propose Up2Dep, an Android Studio extension that supports Android developers in keeping project dependencies up-to-date and in avoiding insecure libraries. To evaluate the technical feasibility of Up2Dep, we publicly released Up2Dep and tested it with Android developers (N=56) in their daily tasks. Up2Dep has delivered quick-fixes that mitigate 108 outdated dependencies and 8 outdated dependencies with security problems in 34 real projects. It was perceived by those developers as being helpful. Our results also highlight technical challenges in realizing such support, for which we provide solutions and new insights. Our results emphasize the urgent need for designated tool support to detect and update insecure outdated third-party libraries in Android apps. We believe that Up2Dep has provided a valuable step forward to improving the security of the Android ecosystem and encouraging results for tool support with a tangible impact as app developers have an easy means to fix their outdated and insecure dependencies.},
booktitle = {Annual Computer Security Applications Conference},
pages = {263–276},
numpages = {14},
keywords = {Third-party Library Updatability, Vulnerable Third-party Libraries, Cryptographic API Misuse in Android, Mobile Security},
location = {Austin, USA},
series = {ACSAC '20}
}

@inproceedings{10.1145/3316781.3323477,
author = {Huang, Tsung-Wei and Lin, Chun-Xun and Guo, Guannan and Wong, Martin D. F.},
title = {Essential Building Blocks for Creating an Open-Source EDA Project},
year = {2019},
isbn = {9781450367257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316781.3323477},
doi = {10.1145/3316781.3323477},
abstract = {Open source has started energizing both industrial and academic research and development in electronic design automation (EDA) systems. By moving to open source, we can speed up our effort and work with others who are working toward the same goals, while reducing costs and improving end products. However, building an open-source project is much more than placing the codebase on the web. In this paper, we will talk about essential building blocks to create an impactful open-source project, including source repository, project landing page, documentation, and continuous integration. We will also cover the use of web-based frameworks to design a showcase project to bring community's attention. We will then share our experience in developing an open-source timing analyzer (OpenTimer) and a parallel task programming library (Cpp-Taskflow), both of which are being used in many industrial and academic EDA research projects.},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {78},
numpages = {4},
keywords = {electronic design automation, Open source},
location = {Las Vegas, NV, USA},
series = {DAC '19}
}

@inproceedings{10.5555/3511065.3511087,
author = {Hon\'{\i}\v{S}ek, Patrik and Vrani\'{c}, Valentino},
title = {Mining Drama Patterns in Dramatic Situations},
year = {2022},
isbn = {9781941652169},
publisher = {The Hillside Group},
address = {USA},
abstract = {Drama patterns can be recognized in plays and movies as they are performed, but also in writing, i.e., in dramatic texts, scripts, stories, novels, etc. This process involves extensive text analysis. It requires insight and skill, but this doesn't mean that it exhibits no regularities. This paper brings an approach to collaborative drama pattern mining in dramatic situations based on layered text annotations. The approach is supported by a prototype tool. Two newly identified drama patterns are presented in an example based form. The approach is potentially applicable to all patterns that have some textual manifestation.},
booktitle = {Proceedings of the 27th Conference on Pattern Languages of Programs},
articleno = {16},
numpages = {14},
keywords = {drama patterns, annotations, mining, roles, forces, patterns},
location = {Virtual Event},
series = {PLoP '20}
}

@inproceedings{10.1145/2723372.2742790,
author = {Saha, Bikas and Shah, Hitesh and Seth, Siddharth and Vijayaraghavan, Gopal and Murthy, Arun and Curino, Carlo},
title = {Apache Tez: A Unifying Framework for Modeling and Building Data Processing Applications},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742790},
doi = {10.1145/2723372.2742790},
abstract = {The broad success of Hadoop has led to a fast-evolving and diverse ecosystem of application engines that are building upon the YARN resource management layer. The open-source implementation of MapReduce is being slowly replaced by a collection of engines dedicated to specific verticals. This has led to growing fragmentation and repeated efforts with each new vertical engine re-implementing fundamental features (e.g. fault-tolerance, security, stragglers mitigation, etc.) from scratch.In this paper, we introduce Apache Tez, an open-source framework designed to build data-flow driven processing runtimes. Tez provides a scaffolding and library components that can be used to quickly build scalable and efficient data-flow centric engines. Central to our design is fostering component re-use, without hindering customizability of the performance-critical data plane. This is in fact the key differentiator with respect to the previous generation of systems (e.g. Dryad, MapReduce) and even emerging ones (e.g. Spark), that provided and mandated a fixed data plane implementation. Furthermore, Tez provides native support to build runtime optimizations, such as dynamic partition pruning for Hive.Tez is deployed at Yahoo!, Microsoft Azure, LinkedIn and numerous Hortonworks customer sites, and a growing number of engines are being integrated with it. This confirms our intuition that most of the popular vertical engines can leverage a core set of building blocks. We complement qualitative accounts of real-world adoption with quantitative experimental evidence that Tez-based implementations of Hive, Pig, Spark, and Cascading on YARN outperform their original YARN implementation on popular benchmarks (TPC-DS, TPC-H) and production workloads.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1357–1369},
numpages = {13},
keywords = {open source, big data, apache hadoop, distributed data processing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/1509096.1509142,
author = {Kirschner, Bryan},
title = {Building a Balanced Scorecard for Open Source Policy and Strategy: A Case Study of the Microsoft Experience},
year = {2008},
isbn = {9781605583860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509096.1509142},
doi = {10.1145/1509096.1509142},
abstract = {The impact of open source software development has challenged commercial companies and government bodies alike to develop new strategies for a changing information and communication technology (ICT) environment. The balanced scorecard---already a widely recognized management best-practice---is well-suited both as a framework for this imperative, and as a mechanism for knowledge-sharing between industry and public sector managers.First, a balanced scorecard initiative must embrace a multi-disciplinary, multi-stakeholder and community-oriented approach, taking into account the needs of diverse constituencies. Second, public sector e-Government leaders can benefit from the same robust perspective on learning, innovation, and customer satisfaction in addition to financial measures that the balanced scorecard has brought to private sector managers.This paper shares the experience of one commercial company (Microsoft Corporation) taking a balanced scorecard approach to open source policy and strategy development, and offers an analysis of implications and opportunities for e-Government leaders.},
booktitle = {Proceedings of the 2nd International Conference on Theory and Practice of Electronic Governance},
pages = {226–231},
numpages = {6},
keywords = {balanced scorecard, strategy, open source, Microsoft},
location = {Cairo, Egypt},
series = {ICEGOV '08}
}

@inproceedings{10.1145/3106237.3106246,
author = {Coelho, Jailton and Valente, Marco Tulio},
title = {Why Modern Open Source Projects Fail},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106246},
doi = {10.1145/3106237.3106246},
abstract = {Open source is experiencing a renaissance period, due to the appearance of modern platforms and workflows for developing and maintaining public code. As a result, developers are creating open source software at speeds never seen before. Consequently, these projects are also facing unprecedented mortality rates. To better understand the reasons for the failure of modern open source projects, this paper describes the results of a survey with the maintainers of 104 popular GitHub systems that have been deprecated. We provide a set of nine reasons for the failure of these open source projects. We also show that some maintenance practices---specifically the adoption of contributing guidelines and continuous integration---have an important association with a project failure or success. Finally, we discuss and reveal the principal strategies developers have tried to overcome the failure of the studied projects.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {186–196},
numpages = {11},
keywords = {Open Source Software, GitHub, Project failure},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/383535.383553,
author = {Hall, Maria Jean J. and Zeleznikow, John},
title = {Acknowledging Insufficiency in the Evaluation of Legal Knowledge-Based Systems: Strategies towards a Broadbased Evaluation Model},
year = {2001},
isbn = {1581133685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383535.383553},
doi = {10.1145/383535.383553},
abstract = {This paper considers the need for evaluation of knowledge-based systems in general and legal knowledge-based systems in particular. Some special features of legal knowledge-based systems pertinent to their evaluation are presented. The expected benefits of such evaluations are discussed. and some of the difficulties likely to be encountered are outlined.The proceedings of four International Conferences on Artificial Intelligence and Law are analysed to determine the rate of reporting evaluations in non-theoretical papers. These papers had a low rate of consideration of evaluation issues reflecting common practice in research biased development environments. These results confirm that more attention to evaluation is needed in the legal knowledge based systems domain.This paper foreshadows the development of an evaluation methodology tailored specifically for legal knowledge-based systems. Evaluation strategies beyond verification and validation are drawn upon, both from the international ISO/IEC 14598 and 9126 standards and also from previous work on evaluation models for knowledge-based systems.},
booktitle = {Proceedings of the 8th International Conference on Artificial Intelligence and Law},
pages = {147–156},
numpages = {10},
keywords = {legal knowledge-based systems, evaluation, verification, validation},
location = {St. Louis, Missouri, USA},
series = {ICAIL '01}
}

@inproceedings{10.1145/2901739.2901762,
author = {Ahmad, Waqar and K\"{a}stner, Christian and Sunshine, Joshua and Aldrich, Jonathan},
title = {Inter-App Communication in Android: Developer Challenges},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901762},
doi = {10.1145/2901739.2901762},
abstract = {The Android platform is designed to support mutually untrusted third-party apps, which run as isolated processes but may interact via platform-controlled mechanisms, called Intents. Interactions among third-party apps are intended and can contribute to a rich user experience, for example, the ability to share pictures from one app with another. The Android platform presents an interesting point in a design space of module systems that is biased toward isolation, extensibility, and untrusted contributions. The Intent mechanism essentially provides message channels among modules, in which the set of message types is extensible. However, the module system has design limitations including the lack of consistent mechanisms to document message types, very limited checking that a message conforms to its specifications, the inability to explicitly declare dependencies on other modules, and the lack of checks for backward compatibility as message types evolve over time. In order to understand the degree to which these design limitations result in real issues, we studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums. Our findings suggest that design limitations do indeed cause development problems. Based on our results, we outline further research questions and propose possible mitigation strategies.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {177–188},
numpages = {12},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1145/2659118.2659122,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659122},
doi = {10.1145/2659118.2659122},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {6–13},
numpages = {8}
}

@article{10.1145/1107458.1107461,
author = {Desouza, Kevin C. and Awazu, Yukika and Tiwana, Amrit},
title = {Four Dynamics for Bringing Use Back into Software Reuse},
year = {2006},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1107458.1107461},
doi = {10.1145/1107458.1107461},
abstract = {Reuse is such a powerful tool---so why isn't it more popular?},
journal = {Commun. ACM},
month = {jan},
pages = {96–100},
numpages = {5}
}

@article{10.5555/1060081.1060089,
author = {Olan, Michael},
title = {Dr. J vs. the Bird: Java IDE's One-on-One},
year = {2004},
issue_date = {May 2004},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {19},
number = {5},
issn = {1937-4771},
abstract = {An important decision facing instructors of introductory programming courses is the choice of supporting software development tools. Usually this involves selecting an integrated development environment (IDE). BlueJ has received widespread adoption for first year courses that use the Java programming language; however, DrJava is emerging as an alternative. This paper features a comparison of the pedagogical approaches used by BlueJ and DrJava as a guideline for selecting the tool best suited to the teaching style used in the introductory course.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {44–52},
numpages = {9}
}

@inproceedings{10.1145/2384716.2384768,
author = {McDaniel, Brian and Back, Godmar},
title = {The CloudBrowser Web Application Framework},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384768},
doi = {10.1145/2384716.2384768},
abstract = {CloudBrowser is a web application framework that supports the development of rich Internet applications whose entire user interface and application logic resides on the server, while all client/server communication is provided by the framework. CloudBrowser thus hides the distributed nature of these applications from the developer, creating an environment similar to that provided by a desktop user interface library. CloudBrowser preserves the user interface state in a server-side virtual browser that is maintained across visits. Unlike other server-centric frameworks, CloudBrowser's exclusive use of the HTML document model and associated JavaScript execution environment allows it to exploit existing client-side user interface libraries and toolkits while transparently providing access to other application tiers. We have implemented a prototype of CloudBrowser as well as several example applications to demonstrate the benefits of its server-centric design.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {141–156},
numpages = {16},
keywords = {web application framework, remote display, server-centric, paas, ajax, cloud applications},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@inproceedings{10.1145/1274000.1274093,
author = {Cook, Thomas E.},
title = {GAUGUIN: Generating Art Using Genetic Algorithms and User Input Naturally},
year = {2007},
isbn = {9781595936981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1274000.1274093},
doi = {10.1145/1274000.1274093},
abstract = {This paper outlines an undergraduate research project demonstrating an application of evolutionary computation in the context of computer art. The project combines the visual impact of modern computer graphics with the computational power of genetic algorithms. GAUGUIN allows the user to become a creator of art, without requiring any technical or artistic training. By using an intuitive and easily comprehensible process like evolution to create the composition, all the user needs to do is evaluate a number of possible "solutions", which trains the system to recognize his or her specific taste. The act of evaluating and scoring is inherent in all of us; this project simply takes advantage of that behavior in a creative way.},
booktitle = {Proceedings of the 9th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {2647–2650},
numpages = {4},
keywords = {evolutionary art, interactive art, computer art},
location = {London, United Kingdom},
series = {GECCO '07}
}

@inproceedings{10.1145/3196398.3196426,
author = {Russo, Barbara},
title = {Profiling Call Changes via Motif Mining},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196426},
doi = {10.1145/3196398.3196426},
abstract = {Components' interactions in software systems evolve over time increasing in complexity and size. Developers might have hard time to master such complexity during their maintenance activities incrementing the risk to make mistakes. Understanding changes of such interactions helps developer plan their re-factoring activities. In this study, we propose a method to study the occurrence of motifs in call graphs and their role in the evolution of a system. In our settings, motifs are patterns of class calls that can arise for many reasons as, for example, by implementing design choices. By mining motifs of the call graph obtained from each system's release, we were able to profile the evolution of 68 releases of five open source systems and show that 1) systems have common motifs that occur non-randomly and persistently over their releases, 2) motifs can be used to describe the evolution of calls, compare systems and eventually reveal releases that underwent major changes, 3) there are no specific motif types that include design patterns in all systems under study, but each system has motifs that likely include them, motifs that do not include them at all, and motifs that include a design pattern and occur only once in every release. Some of the findings resemble the ones for biological / physical systems and, as such, path the way to study the evolution of call graphs as dynamical systems (i.e., as system regulated by analytic functions).},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {203–214},
numpages = {12},
keywords = {design patterns, call graphs, motifs mining, software evolution},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/1646353.1646365,
author = {Maughan, Douglas},
title = {The Need for a National Cybersecurity Research and Development Agenda},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/1646353.1646365},
doi = {10.1145/1646353.1646365},
abstract = {Government-funded initiatives, in cooperation with private-sector partners in key technology areas, are fundamental to cybersecurity technical transformation.},
journal = {Commun. ACM},
month = {feb},
pages = {29–31},
numpages = {3}
}

@article{10.1145/2756542,
author = {Lee, Melissa and Almirall, Esteve and Wareham, Jonathan},
title = {Open Data and Civic Apps: First-Generation Failures, Second-Generation Improvements},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2756542},
doi = {10.1145/2756542},
abstract = {Developers first need compelling incentives and committed management.},
journal = {Commun. ACM},
month = {dec},
pages = {82–89},
numpages = {8}
}

@inproceedings{10.5555/2008503.2008506,
author = {Bergmayr, Alexander},
title = {ReuseMe - towards Aspect-Driven Reuse in Modelling Method Development},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today, the construction of individual modelling methods is a commonly accepted practice in different application domains. Method engineers are, however, faced with complexity and high effort involved, especially during modelling language development, considered as one major task when developing methods. To alleviate this, one obvious step is to promote reuse, thereby increasing productivity and quality similar to what can be expected from reuse in software and information systems engineering. Although considerable progress in language modularization and composition is observable, the reuse principle is still rarely adopted in practice. Therefore, in this work, a research roadmap for ReuseMe (Reuse Methods), a novel aspect-oriented reuse approach is proposed. By involving artefacts generated during a method's conceptualization down to its implementation and putting forth fundamental ingredients for a comprehensive method reuse process on top of an Open Model Repository, method reuse becomes leveraged. This paves the way for establishing a library, populated with potential reusable aspects that modularize method artefacts based on separating language concerns.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {4–18},
numpages = {15},
keywords = {aspect-orientation, reuse, modelling method development},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1145/3194164.3194165,
author = {Alfayez, Reem and Behnamghader, Pooyan and Srisopha, Kamonphop and Boehm, Barry},
title = {An Exploratory Study on the Influence of Developers in Technical Debt},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194165},
doi = {10.1145/3194164.3194165},
abstract = {Software systems are often developed by many developers who have a varying range of skills and habits. These developers have a big impact on software quality. Understanding how different developers and developer characteristics impact the quality of a software is crucial to properly deploy human resources and help managers improve quality outcomes which is essential for software systems success. Addressing this concern, we conduct a study on how different developers and developer characteristics such as developer seniority in a system, frequency of commits, and interval between commits relate to Technical Debt (TD). We performed a large-scale analysis on 19,088 commits from 38 Apache Java systems and applied multiple statistical analysis tests to evaluate our hypotheses. Our empirical evaluation suggests that developers unequally increase and decrease TD, a developer seniority in a software system and frequency of commits are negatively correlated with the TD the developer induces, and a developer commit interval has a positive correlation with the TD the developer induces.},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {1–10},
numpages = {10},
keywords = {developer experience, technical debt, software maintenance, project management, developer contribution, software engineering, human factors},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/2739482.2768424,
author = {Landsborough, Jason and Harding, Stephen and Fugate, Sunny},
title = {Removing the Kitchen Sink from Software},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768424},
doi = {10.1145/2739482.2768424},
abstract = {We would all benefit if software were slimmer, thinner, and generally only did what we needed and nothing more. To this end, our research team has been exploring methods for removing unused and undesirable features from compiled programs. Our primary goal is to improve software security by removing rarely used features in order to decrease a pro- gram's attack surface. We describe two different approaches for "thinning" binary images of compiled programs. The first approach removes specific program features using dynamic tracing as a guide. This approach is safer than many alterna- tives, but is limited to removing code which is reachable in a trace when an undesirable feature is enabled. The second ap- proach uses a genetic algorithm (GA) to mutate a program until a suitable variant is found. Our GA-based approach can potentially remove any code that is not strictly required for proper execution, but may break program semantics in unpredictable ways. We show results of these approaches on a simple program and real-world software and explore some of the implications for software security.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {833–838},
numpages = {6},
keywords = {feature removal, genetic algorithm, tracing},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@inproceedings{10.1145/3551349.3556896,
author = {Latendresse, Jasmine and Mujahid, Suhaib and Costa, Diego Elias and Shihab, Emad},
title = {Not All Dependencies Are Equal: An Empirical Study on Production Dependencies in NPM},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556896},
doi = {10.1145/3551349.3556896},
abstract = {Modern software systems are often built by leveraging code written by others in the form of libraries and packages to accelerate their development. While there are many benefits to using third-party packages, software projects often become dependent on a large number of software packages. Consequently, developers are faced with the difficult challenge of maintaining their project dependencies by keeping them up-to-date and free of security vulnerabilities. However, how often are project dependencies used in production where they could pose a threat to their project’s security? We conduct an empirical study on 100 JavaScript projects using the Node Package Manager (npm) to quantify how often project dependencies are released to production and analyze their characteristics and their impact on security. Our results indicate that less than 1% of the installed dependencies are released to production. Our analysis reveals that the functionality of a package is not enough to determine if it will be released to production or not. In fact, 59% of the installed dependencies configured as runtime dependencies are not used in production, and 28.2% of the dependencies configured as development dependencies are used in production, debunking two common assumptions of dependency management. Findings also indicate that most security alerts target dependencies not used in production, making them highly unlikely to be a risk for the security of the software. Our study unveils a more complex side of dependency management: not all dependencies are equal. Dependencies used in production are more sensitive to security exposure and should be prioritized. However, current tools lack the appropriate support in identifying production dependencies.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {73},
numpages = {12},
keywords = {dependencies, security, npm, third-party packages},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/2000259.2000291,
author = {Durdik, Zoya},
title = {Towards a Process for Architectural Modelling in Agile Software Development},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000291},
doi = {10.1145/2000259.2000291},
abstract = {Agile methods and architectural modelling have been considered to be mutually exclusive. On the one hand, agile methods try to reduce overheads by avoiding activities that do not directly contribute to the immediate needs of the current project. This often leads to bad cross-project reuse. On the other hand, architectural modelling is considered a pre requisite for the systematic cross-project reuse and for the resulting increase in software developer productivity. The theme of this paper is to address the relationship between agile methods and architectural modelling and to propose a novel process for agile architectural modelling, which drives requirements elicitation through the use of patterns and components. This process is in-line with agile principles and is illustrated on an example application. Additionally, the paper points out the challenges connected to the process validation and proposes an approach for the empirical validation addressing these challenges.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {183–192},
numpages = {10},
keywords = {agile methods, software architecture, component selection, scrum, design patterns, architectural modelling, development process},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/3219104.3219144,
author = {Breen, Joe and Bryant, Lincoln and Carcassi, Gabriele and Chen, Jiahui and Gardner, Robert W. and Harden, Ryan and Izdimirski, Martin and Killen, Robert and Kulbertis, Ben and McKee, Shawn and Riedel, Benedikt and Stidd, Jason and Truong, Luan and Vukotic, Ilija},
title = {Building the SLATE Platform},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219144},
doi = {10.1145/3219104.3219144},
abstract = {We describe progress on building the SLATE (Services Layer at the Edge) platform. The high level goal of SLATE is to facilitate creation of multi-institutional science computing systems by augmenting the canonical Science DMZ pattern with a generic, "programmable", secure and trusted underlayment platform. This platform permits hosting of advanced container-centric services needed for higher-level capabilities such as data transfer nodes, software and data caches, workflow services and science gateway components. SLATE uses best-of-breed data center virtualization and containerization components, and where available, software defined networking, to enable distributed automation of deployment and service lifecycle management tasks by domain experts. As such it will simplify creation of scalable platforms that connect research teams, institutions and resources to accelerate science while reducing operational costs and development cycle times.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {5},
numpages = {7},
keywords = {Containerization, Distributed computing, Edge computing},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/3563657.3595983,
author = {Rivera, Michael L. and Bae, S. Sandra and Hudson, Scott E.},
title = {Designing a Sustainable Material for 3D Printing with Spent Coffee Grounds},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3595983},
doi = {10.1145/3563657.3595983},
abstract = {The widespread adoption of 3D printers exacerbates existing environmental challenges as these machines increase energy consumption, waste output, and the use of plastics. Material choice for 3D printing is tightly connected to these challenges, and as such researchers and designers are exploring sustainable alternatives. Building on these efforts, this work explores using spent coffee grounds as a sustainable material for prototyping with 3D printing. This material, in addition to being compostable and recyclable, can be easily made and printed at home. We describe the material in detail, including the process of making it from readily available ingredients, its material characteristics and its printing parameters. We then explore how it can support sustainable prototyping practices as well as HCI applications. In reflecting on our design process, we discuss challenges and opportunities for the HCI community to support sustainable prototyping and personal fabrication. We conclude with a set of design considerations for others to weigh when exploring sustainable materials for 3D printing and prototyping.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {294–311},
numpages = {18},
keywords = {environmental sustainability, personal fabrication, bio-based materials, 3D printing, zero-waste prototyping},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/2785592.2785593,
author = {Aalst, Wil van der},
title = {Big Software on the Run: In Vivo Software Analytics Based on Process Mining (Keynote)},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2785593},
doi = {10.1145/2785592.2785593},
abstract = {Software-related problems have an incredible impact on society, organizations, and users that increasingly rely on information technology. Specification, verification and testing techniques aim to avoid such problems. However, the growing complexity, scale, and diversity of software complicate matters. Since software is evolving and operates in a changing environment, one cannot anticipate all problems at design-time. Hence, we propose to analyze software "in vivo", i.e., we study systems in their natural habitat rather than through testing or software design. We propose to observe running systems, collect and analyze data on them, generate descriptive models, and use these to respond to failures. We focus on process mining as a tool for in vivo software analytics. Process discovery techniques can be used to capture the real behavior of software. Conformance checking techniques can be used to spot deviations. The alignment of models and real software behavior can be used to predict problems related to performance or conformance. Recent developments in process mining and instrumentation of software make this possible. This keynote paper provides pointers to process mining literature and introduces the "Big Software on the Run" (BSR) research program that just started.},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {1–5},
numpages = {5},
keywords = {conformance checking, software engineering, process discovery, Process mining, software analytics, event logs},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@article{10.1145/3274326,
author = {Germonprez, Matt and Link, Georg J.P. and Lumbard, Kevin and Goggins, Sean},
title = {Eight Observations and 24 Research Questions About Open Source Projects: Illuminating New Realities},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274326},
doi = {10.1145/3274326},
abstract = {The rapid acceleration of corporate engagement with open source projects is drawing out new ways for CSCW researchers to consider the dynamics of these projects. Research must now consider the complex ecosystems within which open source projects are situated, including issues of for-profit motivations, brokering foundations, and corporate collaboration. Localized project considerations cannot reveal broader workings of an open source ecosystem, yet much empirical work is constrained to a local context. In response, we present eight observations from our eight-year engaged field study about the changing nature of open source projects. We ground these observations through 24 research questions that serve as primers to spark research ideas in this new reality of open source projects. This paper contributes to CSCW in social and crowd computing by delivering a rich and fresh look at corporately-engaged open source projects with a call for renewed focus and research into newly emergent areas of interest.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {57},
numpages = {22},
keywords = {corporate-communal engagement, open source, tales from the field}
}

@inproceedings{10.1145/3184558.3191656,
author = {Vu, Henry and Fertig, Tobias and Braun, Peter},
title = {Verification of Hypermedia Characteristic of RESTful Finite-State Machines},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191656},
doi = {10.1145/3184558.3191656},
abstract = {Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1881–1886},
numpages = {6},
keywords = {MDSD, REST, RESTful systems, RESTful applications, MDT, hypermedia testing, hypermedia},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2509578.2514738,
author = {Aldrich, Jonathan},
title = {The Power of Interoperability: Why Objects Are Inevitable},
year = {2013},
isbn = {9781450324724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509578.2514738},
doi = {10.1145/2509578.2514738},
abstract = {Three years ago in this venue, Cook argued that in their essence, objects are what Reynolds called procedural data structures. His observation raises a natural question: if procedural data structures are the essence of objects, has this contributed to the empirical success of objects, and if so, how?This essay attempts to answer that question. After reviewing Cook's definition, I propose the term service abstractions to capture the essential nature of objects. This terminology emphasizes, following Kay, that objects are not primarily about representing and manipulating data, but are more about providing services in support of higher-level goals. Using examples taken from object-oriented frameworks, I illustrate the unique design leverage that service abstractions provide: the ability to define abstractions that can be extended, and whose extensions are interoperable in a first-class way. The essay argues that the form of interoperable extension supported by service abstractions is essential to modern software: many modern frameworks and ecosystems could not have been built without service abstractions. In this sense, the success of objects was not a coincidence: it was an inevitable consequence of their service abstraction nature.},
booktitle = {Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software},
pages = {101–116},
numpages = {16},
keywords = {service abstractions, object-oriented programming, frameworks, interoperability},
location = {Indianapolis, Indiana, USA},
series = {Onward! 2013}
}

@inproceedings{10.1145/1294948.1294974,
author = {Lungu, Mircea and Girba, Tudor},
title = {A Small Observatory for Super-Repositories},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294974},
doi = {10.1145/1294948.1294974},
abstract = {Software evolution research has been focused mostly on analyzing the evolution of single software systems. However, it is rarely the case that a project exists as standalone, independent of others. Rather, projects exist in parallel within larger contexts in companies, research groups or even the open-source communities, contexts that we call super-repositories. In this paper, we argue that visualization of super-repositories is useful in a range of situations, and we describe The Small Project Observatory, a prototype tool which aims to visualize super-repositories.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {106–109},
numpages = {4},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1145/3214907.3233972,
author = {Yamamoto, Takashi and Nishino, Tamaki and Kajima, Hideki and Ohta, Mitsunori and Ikeda, Koichi},
title = {Human Support Robot (HSR)},
year = {2018},
isbn = {9781450358101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214907.3233972},
doi = {10.1145/3214907.3233972},
abstract = {There has been an increasing interest in mobile manipulators that is capable of performing physical work in living spaces worldwide, corresponding to population aging with declining birth rates with the expectation of improving quality of life (QOL). Research and development is a must in intelligent sensing and software which enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R&amp;D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. In this paper, we introduce HSR design and its utilization.},
booktitle = {ACM SIGGRAPH 2018 Emerging Technologies},
articleno = {11},
numpages = {2},
keywords = {mobile manipulator},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3549034.3561177,
author = {Yeow, Matthew Yit Hang and Chong, Chun Yong and Lim, Mei Kuan},
title = {On the Application of Machine Learning Models to Assess and Predict Software Reusability},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549034.3561177},
doi = {10.1145/3549034.3561177},
abstract = {Software reuse has proven to be an effective strategy for developers to significantly increase software quality, reduce costs and increase the effectiveness of software development. Research in software reuse typically addresses two main hurdles: reduce the time and effort required to identify reusable candidates, and avoid selecting low-quality software components that may lead to higher cost of development (i.e., solving bugs, errors, refactoring). Inherently, human judgment falls short in the aspect of reliability and effectiveness. Hence this paper investigates the applicability of Machine Learning (ML) algorithms in assessing software reuse. We collected more than 32k open-source projects and employed GitHub fork as the ground truth to its reuse. We developed ML classification pipelines based on both internal and external software metrics to perform software reuse prediction. Our best-performing ML classification model achieved an accuracy of 86%, outperforming existing research in prediction performance and data coverage. Subsequently, we leverage our results by identifying key software characteristics that make software highly reusable. Our results show that size-related metrics (i.e., number of setters, methods, attributes) are the most impactful in contributing to the reuse of the software.},
booktitle = {Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {17–22},
numpages = {6},
keywords = {Software Metrics, Machine Learning, Software Reusability},
location = {Singapore, Singapore},
series = {MaLTeSQuE 2022}
}

@inproceedings{10.1145/3368089.3409688,
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed},
title = {Beware the Evolving ‘Intelligent’ Web Service! An Integration Architecture Tactic to Guard AI-First Components},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409688},
doi = {10.1145/3368089.3409688},
abstract = {Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services---such as computer vision---continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–280},
numpages = {12},
keywords = {software architecture, software evolution, intelligent web services},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3485730.3493375,
author = {Ranathunga, Tharindu and McGibney, Alan and Rea, Susan},
title = {The Convergence of Blockchain and Machine Learning for Decentralized Trust Management in IoT Ecosystems},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493375},
doi = {10.1145/3485730.3493375},
abstract = {The EU data strategy postulates that by 2025 there will be a paradigm shift towards more decentralized intelligence and data processing at the edge. The convergence of a large number of nodes at the IoT edge along with multiple service providers and network operators exposes data owners and resource providers to potential threats. To address cloud-edge risks, trust-based decentralized management is needed. Blockchain technology has created an opportunity to decentralize IoT ecosystems, through its intrinsic properties and together with machine learning (ML) it can be used to provide a trusted backbone for managing IoT ecosystems to support automated and adaptive trust management. This paper presents a novel approach for crosslayer intelligent trust computation modelling leveraging ML and Blockchain for decentralized trust management in IoT ecosystems. The effectiveness of the proposed approach for flow-based trust assessment is demonstrated using the Hyperledger Framework and the Cooja-based simulation environment. Finally, an initial evaluation is presented to understand the performance in terms of scalability and trust convergence of the proposed model.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {499–504},
numpages = {6},
keywords = {Hyperledger, Blockchain, Internet of things, Trust, Machine Learning, IoT Ecosystems},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.5555/2820518.2820589,
author = {Zacchiroli, Stefano},
title = {The Debsources Dataset: Two Decades of Debian Source Code Metadata},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {We present the Debsources Dataset: distribution metadata and source code metrics spanning two decades of Free and Open Source Software (FOSS) history, seen through the lens of the Debian distribution.Debsources is a software platform used to gather, search, and publish on the Web the full source code of the Debian operating system, as well as measures about it. A notable public instance of Debsources is available at http://sources.debian.net; it includes both current and historical releases of Debian. Plugins to compute popular source code metrics (lines of code, defined symbols, disk usage) and other derived data (e.g., checksums) have been written, integrated, and run on all the source code available on sources.debian.net.The Debsources Dataset is a PostgreSQL database dump of sources.debian.net metadata, as of February 10th, 2015. The dataset contains both Debian-specific metadata---e.g., which software packages are available in which release, which source code file belong to which package, release dates, etc.---and source code information gathered by running Debsources plugins.The Debsources Dataset offer a very long-term historical view of the macro-level evolution and constitution of FOSS through the lens of popular, representative FOSS projects of their times.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {466–469},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2675133.2675247,
author = {Webster, Gemma and Nguyen, Hai and Beel, David E. and Mellish, Chris and Wallace, Claire D. and Pan, Jeff},
title = {CURIOS: Connecting Community Heritage through Linked Data},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675133.2675247},
doi = {10.1145/2675133.2675247},
abstract = {The CURIOS project explores how digital archives for rural community heritage groups can be made more sustainable so that volunteer members can maintain a lasting digital presence. It is developing software tools to help remote rural communities to collaboratively maintain and present information about their cultural heritage. The objective is to investigate the use of semantic web/linked data technology to build a general, flexible and "future proof" software platform that could help such projects to develop digital archives and to be sustainable over time. As an interdisciplinary project we aim to synthesise a narrative that draws from both social science and computer science perspectives by critically reflecting upon the novel approach taken and the on-going results that are being produced.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {639–648},
numpages = {10},
keywords = {digital archives, cultural heritage, community heritage, open linked data},
location = {Vancouver, BC, Canada},
series = {CSCW '15}
}

@inproceedings{10.1145/3530019.3530031,
author = {Das, Debeshee and Mathews, Noble Saji and Chimalakonda, Sridhar},
title = {Exploring Security Vulnerabilities in Competitive Programming: An Empirical Study},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3530031},
doi = {10.1145/3530019.3530031},
abstract = {Insecure code leading to software vulnerabilities can result in damages of the order of millions of dollars, and in critical systems, the loss of life. Hence, developing secure systems free of exploitable vulnerabilities has been a thrust area of research in recent years. Understanding developers’ approach towards vulnerabilities in their code can pave the way for improvements in insecure coding practices. Recent studies have explored online Q&amp;A forums, open-source code repositories, and other code information sources to gain important insights into the pervasiveness of security vulnerabilities. However, to the best of our knowledge, competitive programming (CP) data, a rich source of information about coding practices, has not been explored from the perspective of insecure coding practices. The evaluation and assessment of coding practices used in CP is particularly intriguing because it has become a key player in developer recruitment in recent times. In this paper, we make one of the first attempts to draw the attention of the community to the emergent concern of insecure coding practices in CP. We use static analysis tools to identify the prevalence and nature of vulnerabilities in a large amount of CP data (6.1 million submissions) obtained from a top-rated CP platform, CodeChef, and find that 34.2% of submissions contain vulnerabilities. We observe that many programmers consistently follow insecure coding practices and most of the detected vulnerabilities are characterized by security standards (CWE, CVSS) based on real-world software.},
booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
pages = {110–119},
numpages = {10},
keywords = {Security Vulnerabilities, Empirical Study, Competitive Programming, Static Analysis, Software Security},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3528227.3528565,
author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
title = {"If Security is Required": Engineering and Security Practices for Machine Learning-Based IoT Devices},
year = {2023},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528227.3528565},
doi = {10.1145/3528227.3528565},
abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices.This research seeks to characterize engineering processes and security practices for ML-enabled IoT systems through the lens of the engineering lifecycle. We collected data from practitioners through a survey (N=25) and interviews (N=4). We found that security processes and engineering methods vary by company. Respondents emphasized the engineering cost of security analysis and threat modeling, and trade-offs with business needs. Engineers reduce their security investment if it is not an explicit requirement. The threats of IP theft and reverse engineering were a consistent concern among practitioners when deploying ML for IoT devices. Based on our findings, we recommend further research into understanding engineering cost, compliance, and security trade-offs.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
pages = {1–8},
numpages = {8},
keywords = {machine learning, internet of things, embedded systems, software engineering, security and privacy, cyber-physical systems},
location = {Pittsburgh, Pennsylvania},
series = {SERP4IoT '22}
}

@inproceedings{10.1145/3609437.3609447,
author = {Yang, Haolin and Chen, Lin and Cao, Yulu and Li, Yanhui and Zhou, Yuming},
title = {Towards Better Dependency Scope Settings in Maven Projects},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609447},
doi = {10.1145/3609437.3609447},
abstract = {The emergence of build automation tools with dependency management features has significantly impacted software development. However, in the configuration process, improper settings of some configuration items, such as the dependency scope setting, may cause severe problems in the development process. Improper setting of dependency scope may cause problems such as missing dependencies and redundant dependencies, and may even spread the problem to the downstream of the software ecosystem. We conduct the first comprehensive empirical study of dependency scope settings in Maven projects to investigate the current state of dependency scope settings. We collect 5,433 commits from 65 popular open-source projects on GitHub, including 20,076 dependency scope settings. We also manually analyze 124 improper scope setting issues sampled from 2,609 Java projects. By analyzing these data, we reveal the typical symptoms and root causes of problems caused by improper dependency scope settings, and summarize 5 patterns of dependency scope modification. We provide suggestions for developers to better set and manage the dependency scope, and provide some ideas and experiences for the development of tools related to dependency scope setting.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {90–100},
numpages = {11},
keywords = {Dependency Management, Empirical Study, Dependency Scope, Maven},
location = {Hangzhou, China},
series = {Internetware '23}
}

@inproceedings{10.1145/2556288.2557348,
author = {Zhu, Haiyi and Chen, Jilin and Matthews, Tara and Pal, Aditya and Badenes, Hernan and Kraut, Robert E.},
title = {Selecting an Effective Niche: An Ecological View of the Success of Online Communities},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557348},
doi = {10.1145/2556288.2557348},
abstract = {Online communities serve various important functions, but many fail to thrive. Research on community success has traditionally focused on internal factors. In contrast, we take an ecological view to understand how the success of a community is influenced by other communities. We measured a community's relationship with other communities - its "niche" - through four dimensions: topic overlap, shared members, content linking, and shared offline organizational affiliation. We used a mixed-method approach, combining the quantitative analysis of 9495 online enterprise communities and interviews with community members. Our results show that too little or too much overlap in topic with other communities causes a community's activity to suffer. We also show that this main result is moderated in predictable ways by whether the community shares members with, links to content in, or shares an organizational affiliation with other communities. These findings provide new insight on community success, guiding online community designers on how to effectively position their community in relation to others.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {301–310},
numpages = {10},
keywords = {success, topic overlap, workplace, online communities},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3267183.3267186,
author = {Flauzino, Matheus and Ver\'{\i}ssimo, J\'{u}lio and Terra, Ricardo and Cirilo, Elder and Durelli, Vinicius H. S. and Durelli, Rafael S.},
title = {Are You Still Smelling It? A Comparative Study between Java and Kotlin Language},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267186},
doi = {10.1145/3267183.3267186},
abstract = {Java is one of the most widely used programming languages. However, Java is a verbose language, thus one of the main drawbacks of the language is that even simple tasks often entail writing a significant amount of code. In some cases, writing too much code might lead to certain code smells, which are violations of fundamental design that can negatively impact the overall quality of programs. To allow programmers to write concise code, JetBrains created a new language named Kotlin. Nevertheless, few studies have evaluated whether Kotlin leads to concise and clearer code in comparison to Java. We conjecture that due to Java's verbosity, programs written in Java are likely to have more code smells than Kotlin programs. Therefore, we set out to evaluate whether some types of code smells are more common in Java programs. To this end, we carried out a large-scale empirical study involving more than 6 million lines of code from programs available in 100 repositories. We found that on average Kotlin programs have less code smells than Java programs.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {23–32},
numpages = {10},
keywords = {refactoring, Bad Smell, Kotlin Language, Code Smell},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@article{10.1145/3582697,
author = {Momotaz, Farhani and Ehtesham-Ul-Haque, Md and Billah, Syed Masum},
title = {Understanding the Usages, Lifecycle, and Opportunities of Screen Readers’ Plugins},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3582697},
doi = {10.1145/3582697},
abstract = {Screen reader plugins are small pieces of code that blind users can download and install to enhance the capabilities of their screen readers. This article aims to understand why blind users use these plugins, as well as how these plugins are developed, deployed, and maintained. To this end, we conducted an interview study with 14 blind users to gain individual perspectives and analyzed 2,000 online posts scraped from three plugin-related forums to gain the community perspective. Our study revealed that screen reader users rely on plugins for various reasons, such as to improve the usability of screen readers and application software, to make partially accessible applications accessible, and to receive custom auditory feedback. Furthermore, installing plugins is easy; uninstalling them is unlikely; and finding them online is ad hoc, challenging, and sometimes poses security threats. In addition, developing screen reader plugins is technically demanding; only a handful of people develop plugins. Unfortunately, most plugins do not receive updates once distributed and become obsolete. The lack of financial incentives plays in the slow growth of the plugin ecosystem. Further, we outlined the complex, tripartite collaboration among individual blind users, their online communities, and developer communities in creating a plugin. Additionally, we reported several phenomena within and between these communities that are likely to influence a plugin’s development. Based on our findings, we recommend creating a community-driven repository for all plugins hosted on a peer-to-peer infrastructure, engaging third-party developers, and raising general awareness about the benefits and dangers of plugins. We believe our findings will inspire HCI researchers to embrace the plugin-based distribution model as an effective way to combat accessibility and usability problems in non-visual interaction and to investigate potential ways to improve the collaboration between blind users and developer communities.},
journal = {ACM Trans. Access. Comput.},
month = {jul},
articleno = {17},
numpages = {35},
keywords = {extension, scripts, assistive technology, plugin, NVDA, visual impairments, screen reader}
}

@article{10.1145/1824760.1824763,
author = {Singh, Param Vir},
title = {The Small-World Effect: The Influence of Macro-Level Properties of Developer Collaboration Networks on Open-Source Project Success},
year = {2010},
issue_date = {August 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1824760.1824763},
doi = {10.1145/1824760.1824763},
abstract = {In this study we investigate the impact of community-level networks—relationships that exist among developers in an OSS community—on the productivity of member developers. Specifically, we argue that OSS community networks characterized by small-world properties would positively influence the productivity of the member developers by providing them with speedy and reliable access to more quantity and variety of information and knowledge resources. Specific hypotheses are developed and tested using longitudinal data on a large panel of 4,279 projects from 15 different OSS communities hosted at Sourceforge. Our results suggest that significant variation exists in small-world properties of OSS communities at Sourceforge. After accounting for project, foundry, and time-specific observed and unobserved effects, we found a statistically significant relationship between small-world properties of a community and the technical and commercial success of the software produced by its members. In contrast to the findings of prior research, we also found the lack of a significant relationship between closeness and betweenness centralities of the project teams and their success. These results were robust to a number of controls and model specifications.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {6},
numpages = {27},
keywords = {social networks, team formation, collaborative software development, online community, productivity, small world networks, Open source software development}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {6},
numpages = {45},
keywords = {Product-line analysis, static analysis, program family, type checking, software product line, theorem proving, model checking, software analysis}
}

@inproceedings{10.1145/3477314.3507126,
author = {Abadi, Aharon and Makovitzki, Bar and Shemer, Ron and Tyszberowicz, Shmuel},
title = {A Lightweight Approach for Sound Call Graph Approximation},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507126},
doi = {10.1145/3477314.3507126},
abstract = {Interprocedural analysis refers to gathering information about the entire program rather than for a single procedure only, as in intraprocedural analysis. It enables a more precise analysis; however, it is complicated due to the difficulty of constructing an accurate program call graph. Algorithms for constructing sound call graphs must trade-off precision against scalability. Many precise call graph techniques are complex and are difficult to scale due to the kind of type-inference analysis they use, in particular the use of some variations of points-to analysis. This forces use cases that require both soundness and scale such as vulnerability propagation analysis to resort to simpler variants such as Class Hierarchy Analysis. These kinds of analyses have no sound equivalent for dynamically typed languages such as Python and JavaScript that gained more popularity over recent years. To address this problem, we propose NoCFG, a new sound and scalable method for approximating a call graph that supports a wide variety of programming languages. A key property of NoCFG is that it works on a coarse abstraction of the program, discarding many of the programming language constructs. Due to the coarse program abstraction, extending it to support also other languages is easy. We evaluate NoCFG for real-world projects written in both Python and C# and the results demonstrate a high precision rate of ≥ 89% and scalability through a security use-case over projects with up to 2 million lines of code.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1837–1844},
numpages = {8},
keywords = {abstract interpretation, python, static analysis, static call graph, type inference},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3569951.3597546,
author = {Wisniewski, Leonard and Bujeda, Aday and Duncan, Sarah and Horka, William and Lawrence, Emily and Reekie, Michael and Sarmiento, Evan and Schlatter, Tania and Chalker, Alan and Ohrstrom, Jeffrey},
title = {Augmenting the User Experience in Open OnDemand},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597546},
doi = {10.1145/3569951.3597546},
abstract = {The user experience has always been a hallmark of the Open OnDemand open-source software. User experience research has identified a number of features that would augment it further. Quick-launch buttons allow applications to start with a single click. Custom profiles offer the researcher a choice among dashboards with unique branding and sets of applications. Other single buttons enable restarting a previous job and direct submission of a support request ticket. These features have been integrated and released in Open OnDemand 3.0.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {264–268},
numpages = {5},
keywords = {user experience (UX), dashboard, profile},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@inproceedings{10.1145/2851613.2851792,
author = {Al-omari, Farouq and Roy, Chanchal K.},
title = {Is Code Cloning in Games Really Different?},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851792},
doi = {10.1145/2851613.2851792},
abstract = {Since there are a tremendous number of similar functionalities related to images, 3D graphics, sounds, and script in games software, there is a common wisdom that there might be more cloned code in games compared to traditional software. Also, there might be more cloned code across games since many of these games share similar strategies and libraries. In this study, we attempt to investigate whether such statements are true by conducting a large empirical study using 32 games and 9 non-games software, written in three different programming languages C, Java, and C#, for the case of both exact and near-miss clones. Using a hybrid clone detection tool NiCad and a visualization tool VisCad, we examine and compare the cloning status in them and compare it to the non-games, and examine the cloned methods across game engines. The results show that code reuse in open source games is much different from that of other software systems. Specifically, in contrast to the common wisdom, there are fewer function clones in game open source comparing to non-game open source software systems. Similar to non-games open source, we observed that cloning status changes between different programming languages of the games. In addition, there are very fewer clones across games and mostly no clones (no code reuse) across different game engines. But clones exist heavily across recreated (cloned) games.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1512–1519},
numpages = {8},
keywords = {open source games, game clones, software clones},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00009,
author = {Lim, Geunsik and Ham, MyungJoo and Moon, Jijoong and Song, Wook},
title = {LightSys: Lightweight and Efficient CI System for Improving Integration Speed of Software},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00009},
doi = {10.1109/ICSE-SEIP52600.2021.00009},
abstract = {The complexity and size increase of software has extended the delay for developers as they wait for code analysis and code merge. With the larger and more complex software, more developers nowadays are developing software with large source code repositories. The tendency for software platforms to immediately update software packages with feature updates and bug-fixes is a significant obstacle. Continuous integration systems may help prevent software flaws during the active development of software packages, even when they are deployed and updated frequently. Herein, we present a portable and modular code review automation system that inspects incoming code changes such as code format and style, performance regression, static analysis, build and deployment tests, and dynamic analysis before merging and changing code. The proposed mechanisms are sufficiently lightweight to be hosted on a regular desktop computer even for numerous developers. The resulting reduced costs allow developers to apply the proposed mechanism to many source code repositories. Experimental results demonstrate that the proposed mechanism drastically reduces overheads and improves usability compared with conventional mechanisms: execution time (6x faster), CPU usage (40% lower), memory consumption (1/180), and no out-of-memory occurrence.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {1–10},
numpages = {10},
keywords = {software update, code review automation, software regression, continuous integration, continuous test},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3485557.3485564,
author = {Habuza, Tetiana and Zaki, Nazar and Statsenko, Yauhen and Alnajjar, Fady and Elyassami, Sanaa},
title = {Predicting the Diagnosis of Dementia from MRI Data: Added Value to Cognitive Tests},
year = {2021},
isbn = {9781450384186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485557.3485564},
doi = {10.1145/3485557.3485564},
abstract = {Neuroimaging data may reflect the mental status of both cognitively preserved individuals and patients with neurodegenerative diseases. To find the relationship between cognitive performance and the difference between predicted and observed functional test results, we developed a Convolutional Neural Network (CNN) based regression model to estimate the level of cognitive decline from pre-processed T1-weighted MRI images. In this study, we considered the Predicted Cognitive Gap (PCG) as the measure to accurately segregate Cognitively Normal (CN) versus Alzheimer disease (AD) subjects. The proposed model was tested on a dataset that includes 422 CN and 377 AD cases. The performance of the proposed solution was measured using Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) and achieved 0.987 (ADAS-cog), 0.978 (MMSE), 0.898 (RAVLT), 0.848 (TMT), 0.829 (DSST) for averaged brain images; and 0.985 (ADAS-cog), 0.987 (MMSE), 0.901 (RAVLT), 0.8474 (TMT), 0.796 (DSST) for middle slice skull stripped brain images. The results achieved indicate that PCG can accurately separate healthy subjects from demented ones. The structure of the brain contributes to the level of human cognition and their functional abilities. Proposed PCG may aid in diagnostics of dementia.},
booktitle = {The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research},
articleno = {7},
numpages = {7},
keywords = {Alzheimer’s disease, cognitive decline, Predicted Cognitive Gap marker, Convolutional Neural Network, dementia, aging.},
location = {Sharjah, United Arab Emirates},
series = {ArabWIC 2021}
}

@inproceedings{10.1145/3219104.3219129,
author = {Dooley, Rion and Brandt, Steven R. and Fonner, John},
title = {The Agave Platform: An Open, Science-as-a-Service Platform for Digital Science},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219129},
doi = {10.1145/3219104.3219129},
abstract = {The Agave Platform first appeared in 2011 as a pilot project for the iPlant Collaborative [11]. In its first two years, Foundation saw over 40% growth per month, supporting 1000+ clients, 600+ applications, 4 HPC systems at 3 centers across the US. It also gained users outside of plant biology. To better serve the needs of the general open science community, we rewrote Foundation as a scalable, cloud native application and named it the Agave Platform. In this paper we present the Agave Platform, a Science-as-a-Service (ScaaS) platform for reproducible science. We provide a brief history and technical overview of the project, and highlight three case studies leveraging the platform to create synergistic value for their users.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {28},
numpages = {8},
keywords = {saas, API, Docker, HTC, web application, data management, REST, big data, job submission, paas, platform-as-a-service, science gateway, software-as-a-service, Agave, HPC, container, data science, devops, Singularity, cloud, microservice, platform, cloud service, web service, Science-as-a-Service},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1109/ICSE48619.2023.00047,
author = {Liang, Jenny T. and Arab, Maryam and Ko, Minhyuk and Ko, Amy J. and LaToza, Thomas D.},
title = {A Qualitative Study on the Implementation Design Decisions of Developers},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00047},
doi = {10.1109/ICSE48619.2023.00047},
abstract = {Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {435–447},
numpages = {13},
keywords = {implementation design decisions, software design},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3395363.3397381,
author = {Nie, Pengyu and Celik, Ahmet and Coley, Matthew and Milicevic, Aleksandar and Bell, Jonathan and Gligoric, Milos},
title = {Debugging the Performance of Maven’s Test Isolation: Experience Report},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397381},
doi = {10.1145/3395363.3397381},
abstract = {Testing is the most common approach used in industry for checking software correctness. Developers frequently practice reliable testing-executing individual tests in isolation from each other-to avoid test failures caused by test-order dependencies and shared state pollution (e.g., when tests mutate static fields). A common way of doing this is by running each test as a separate process. Unfortunately, this is known to introduce substantial overhead. This experience report describes our efforts to better understand the sources of this overhead and to create a system to confirm the minimal overhead possible. We found that different build systems use different mechanisms for communicating between these multiple processes, and that because of this design decision, running tests with some build systems could be faster than with others. Through this inquiry we discovered a significant performance bug in Apache Maven’s test running code, which slowed down test execution by on average 350 milliseconds per-test when compared to a competing build system, Ant. When used for testing real projects, this can result in a significant reduction in testing time. We submitted a patch for this bug which has been integrated into the Apache Maven build system, and describe our ongoing efforts to improve Maven’s test execution tooling.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {249–259},
numpages = {11},
keywords = {test isolation, Maven, Build system},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1145/1005062.1005067,
author = {Decrem, Bart},
title = {Desktop Linux: Where Art Thou? Catching up, Meeting New Challenges, Moving Ahead},
year = {2004},
issue_date = {May 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/1005062.1005067},
doi = {10.1145/1005062.1005067},
abstract = {Linux on the desktop has come a long way - and it’s been a roller-coaster ride. At the height of the dot-com boom, around the time of Red Hat’s initial public offering, people expected Linux to take off on the desktop in short order. A few years later, after the stock market crash and the failure of a couple of high-profile Linux companies, pundits were quick to proclaim the stillborn death of Linux on the desktop.},
journal = {Queue},
month = {may},
pages = {48–56},
numpages = {9}
}

@inproceedings{10.1145/3338906.3340443,
author = {Olsson, Thomas and Franke, Ulrik},
title = {Risks and Assets: A Qualitative Study of a Software Ecosystem in the Mining Industry},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340443},
doi = {10.1145/3338906.3340443},
abstract = {Digitalization and servitization are impacting many domains, including the mining industry. As the equipment becomes connected and technical infrastructure evolves, business models and risk management need to adapt. In this paper, we present a study on how changes in asset and risk distribution are evolving for the actors in a software ecosystem (SECO) and system-of-systems (SoS) around a mining operation. We have performed a survey to understand how Service Level Agreements (SLAs) -- a common mechanism for managing risk -- are used in other domains. Furthermore, we have performed a focus group study with companies. There is an overall trend in the mining industry to move the investment cost (CAPEX) from the mining operator to the vendors. Hence, the mining operator instead leases the equipment (as operational expense, OPEX) or even acquires a service. This change in business model impacts operation, as knowledge is moved from the mining operator to the suppliers. Furthermore, as the infrastructure becomes more complex, this implies that the mining operator is more and more reliant on the suppliers for the operation and maintenance. As this change is still in an early stage, there is no formalized risk management, e.g. through SLAs, in place. Rather, at present, the companies in the ecosystem rely more on trust and the incentives created by the promise of mutual future benefits of innovation activities. We believe there is a need to better understand how to manage risk in SECO as it is established and evolves. At the same time, in a SECO, the focus is on cooperation and innovation, the companies do not have incentives to address this unless there is an incident. Therefore, industry need, we believe, help in systematically understanding risk and defining quality aspects such as reliability and performance in the new business environment.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {895–904},
numpages = {10},
keywords = {Software ecosystem, Survey, Case study, Service Level Agreement, Risk Management},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3594806.3594810,
author = {Flynn, Garrett and Brewster, Joshua and Song, Dong and Gotsis, Marientina},
title = {Developing Brain-Computer Interfaces with Everyone},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3594810},
doi = {10.1145/3594806.3594810},
abstract = {Throughout its history, the field of brain-computer interfaces (BCIs) has offered people with severe motor disabilities the opportunity to engage with their environments using brain activity alone. Contemporary solutions, however, lack support for reliable evaluation by researchers, independent use by patients and their caregivers, or creative extension by students, artists, and software developers at home. This paper provides preliminary guidance on the integration of research engagement activities into BCI research to enable use at home. Alongside key principles for enabling Research Engagement Always And With Everyone, we present the initial specification for a standardized software ecosystem that could enable the rapid development of high-performance BCI applications on the Open Web. By integrating Open Web technologies alongside engagement activities in current research programs, we argue that participation in the development of a new generation of at-home BCI systems can be widened.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {113–119},
numpages = {7},
keywords = {browser, brain-computer interface (BCI), research engagement, open-source, web, JavaScript},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1145/1137983.1138009,
author = {Canfora, Gerardo and Cerulo, Luigi},
title = {Fine Grained Indexing of Software Repositories to Support Impact Analysis},
year = {2006},
isbn = {1595933972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137983.1138009},
doi = {10.1145/1137983.1138009},
abstract = {Versioned and bug-tracked software systems provide a huge amount of historical data regarding source code changes and issues management. In this paper we deal with impact analysis of a change request and show that data stored in software repositories are a good descriptor on how past change requests have been resolved. A fine grained analysis method of software repositories is used to index code at different levels of granularity, such as lines of code and source files, with free text contained in software repositories. The method exploits information retrieval algorithms to link the change request description and code entities impacted by similar past change requests. We evaluate such approach on a set of three open-source projects.},
booktitle = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
pages = {105–111},
numpages = {7},
keywords = {mining software repositories, impact analysis},
location = {Shanghai, China},
series = {MSR '06}
}

@inproceedings{10.1145/2901739.2901742,
author = {Bavota, Gabriele and Russo, Barbara},
title = {A Large-Scale Empirical Study on Self-Admitted Technical Debt},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901742},
doi = {10.1145/2901739.2901742},
abstract = {Technical debt is a metaphor introduced by Cunningham to indicate "not quite right code which we postpone making it right". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects.This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding.Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {315–326},
numpages = {12},
keywords = {empirical software engineering, mining software repositories, technical debt},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3493649.3493655,
author = {Nadgowda, Shripad and Luan, Laura},
title = {Tapiser\'{\i}: Blueprint to Modernize DevSecOps for Real World},
year = {2021},
isbn = {9781450391719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493649.3493655},
doi = {10.1145/3493649.3493655},
abstract = {Micro-service application pattern has revolutionize the overall software delivery lifecycle. Modularization has allowed breaking monolithic application into independent components that can be developed faster and automation in CICD has enabled high velocity deployment of applications to the cloud. Such a modernization has mandated a need to put security at the center of the workflow from code to container, giving rise to the DevSecOps paradigms. Although effectiveness of the existing DevSecOps solutions is limited by lack of good development practices and narrow scope where it is applied for security analytic only around code hygiene, like vulnerability scanning, license auditing, etc. We discuss our survey on these challenges and highlight their security implications. In tapiser\'{\i} we then present wider perspective to design a DevSecOps solution that addresses prevalent challenges around supply chain security, build security for micro-services, ensures integrity of the pipelines themselves and brings transparency and auditability to the process.},
booktitle = {Proceedings of the Seventh International Workshop on Container Technologies and Container Clouds},
pages = {13–18},
numpages = {6},
keywords = {DevSecOps, Software Bill-of-Material, Supply Chain Security, CyberSecurity},
location = {Virtual Event, Canada},
series = {WoC '21}
}

@inproceedings{10.1145/3368089.3409705,
author = {Lamba, Hemank and Trockman, Asher and Armanios, Daniel and K\"{a}stner, Christian and Miller, Heather and Vasilescu, Bogdan},
title = {Heard It through the Gitvine: An Empirical Study of Tool Diffusion across the Npm Ecosystem},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409705},
doi = {10.1145/3368089.3409705},
abstract = {Automation tools like continuous integration services, code coverage reporters, style checkers, dependency managers, etc. are all known to provide significant improvements in developer productivity and software quality. Some of these tools are widespread, others are not. How do these automation "best practices" spread? And how might we facilitate the diffusion process for those that have seen slower adoption? In this paper, we rely on a recent innovation in transparency on code hosting platforms like GitHub---the use of repository badges---to track how automation tools spread in open-source ecosystems through different social and technical mechanisms over time. Using a large longitudinal data set, multivariate network science techniques, and survival analysis, we study which socio-technical factors can best explain the observed diffusion process of a number of popular automation tools. Our results show that factors such as social exposure, competition, and observability affect the adoption of tools significantly, and they provide a roadmap for software engineers and researchers seeking to propagate best practices and tools.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {505–517},
numpages = {13},
keywords = {innovations, diffusion, software tools, open source ecosystem},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/3291291.3291315,
author = {Baddreddin, Omar and Rahad, Khandoker},
title = {The Impact of Design and UML Modeling on Codebase Quality and Sustainability},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {The general consensus of researchers and practitioners is that up-front and continuous software design using modeling languages such as UML improve code quality and reliability particularly as the software evolves over time. Software designs and models help in managing the underlying code complexities which are crucial for sustainability. Recently, there has been increasing evidence suggesting broader adoption of modeling languages such as UML. However, our understanding of the impact of using such modeling and design languages remains limited. This paper reports on a study that aims to characterize this impact on code quality and sustainability. We identify a sample of open source software repositories with extensive use of designs and modeling and compare their code qualities with similar code-centric repositories. Our evaluation focuses on various code quality attributes such as code smells and technical debt. We also conduct code evolution analysis over five-year period and collect additional data from questionnaires and interviews with active repository contributors. This study finds that repositories with significant use of models and design activities are associated with reduced critical code smells but are also associated with increase in non-critical code smells. The study also finds that modeling and design activities are associated with significant reduction in measures of technical debt. Analyzing code evolution over five year period reveals that UML repositories start with significantly lower technical debt density measures but tend to decline over time.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {236–244},
numpages = {9},
keywords = {software design, code-centric development, UML, code smells, open source repositories, model driven software development, empirical investigation, technical debt, sustainability, software maintenance},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/1533057.1533075,
author = {Kursawe, Klaus and Schellekens, Dries},
title = {Flexible ΜTPMs through Disembedding},
year = {2009},
isbn = {9781605583945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1533057.1533075},
doi = {10.1145/1533057.1533075},
abstract = {With the utilization of TPM-based trusted platforms in real applications, and the subsequent adaption of the specification to the experience gained from such utilization, it increasingly appears that the TPM architecture has some fundamental flaws that result in more and more complex and expensive hardware requirements. In this paper, we propose a new architecture that resets the trust boundary to a much smaller scale, thus allowing for much simpler and more flexible TPM implementations, without sacrificing the security gains from a classical TPM.},
booktitle = {Proceedings of the 4th International Symposium on Information, Computer, and Communications Security},
pages = {116–124},
numpages = {9},
keywords = {trusted computing, mobile trusted module, trusted platform module, secure coprocessor},
location = {Sydney, Australia},
series = {ASIACCS '09}
}

@proceedings{10.1145/3604930,
title = {HotCarbon '23: Proceedings of the 2nd Workshop on Sustainable Computer Systems},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Hot Carbon focuses on understanding and addressing the negative environmental impacts of computing's success and computing's proliferation. The objective of the workshop is to foster insights and discussions as well as a growing community that focuses on sustainability of computer systems. We expect this includes innovative approaches to how we build, deploy, operate, and retire our creations, but perhaps even more. For example, software-driven hardware obsolescence that increases E-waste and embodied carbon suggests we must challenge computing's endemic upgrade and throwaway practices, and mindset.},
location = {Boston, MA, USA}
}

@inproceedings{10.1145/3497775.3503951,
author = {Appel, Andrew W.},
title = {Coq’s Vibrant Ecosystem for Verification Engineering (Invited Talk)},
year = {2022},
isbn = {9781450391825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497775.3503951},
doi = {10.1145/3497775.3503951},
abstract = {Program verification in the large is not only a matter of mechanizing a program logic to handle the semantics of your programming language. You must reason in the mathematics of your application domain--and there are many application domains, each with their own community of domain experts. So you will need to import mechanized proof theories from many domains, and they must all interoperate. Such an ecosystem is not only a matter of mathematics, it is a matter of software process engineering and social engineering. Coq's ecosystem has been maturing nicely in these senses.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {2–11},
numpages = {10},
keywords = {None, nil},
location = {Philadelphia, PA, USA},
series = {CPP 2022}
}

@inproceedings{10.1145/3338906.3340445,
author = {Gamez-Diaz, Antonio and Fernandez, Pablo and Ruiz-Cort\'{e}s, Antonio and Molina, Pedro J. and Kolekar, Nikhil and Bhogill, Prithpal and Mohaan, Madhurranjan and M\'{e}ndez, Francisco},
title = {The Role of Limitations and SLAs in the API Industry},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340445},
doi = {10.1145/3338906.3340445},
abstract = {As software architecture design is evolving to a microservice paradigm, RESTful APIs are being established as the preferred choice to build applications. In such a scenario, there is a shift towards a growing market of APIs where providers offer different service levels with tailored limitations typically based on the cost.  In this context, while there are well established standards to describe the functional elements of APIs (such as the OpenAPI Specification), having a standard model for Service Level Agreements (SLAs) for APIs may boost an open ecosystem of tools that would represent an improvement for the industry by automating certain tasks during the development such as: SLA-aware scaffolding, SLA-aware testing, or SLA-aware requesters.  Unfortunately, despite there have been several proposals to describe SLAs for software in general and web services in particular during the past decades, there is an actual lack of a widely used standard due to the complex landscape of concepts surrounding the notion of SLAs and the multiple perspectives that can be addressed.  In this paper, we aim to analyze the landscape for SLAs for APIs in two different directions: i) Clarifying the SLA-driven API development lifecycle: its activities and participants; 2) Developing a catalog of relevant concepts and an ulterior prioritization based on different perspectives from both Industry and Academia.  As a main result, we present a scored list of concepts that paves the way to establish a concrete road-map for a standard industry-aligned specification to describe SLAs in APIs.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1006–1014},
numpages = {9},
keywords = {API Gateways, RESTful APIs, SLA-driven APIs, SLA, OpenAPI Specification},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3106028.3106033,
author = {Wittern, Erik and Ying, Annie and Zheng, Yunhui and Laredo, Jim A. and Dolby, Julian and Young, Christopher C. and Slominski, Aleksander A.},
title = {Opportunities in Software Engineering Research for Web API Consumption},
year = {2017},
isbn = {9781538628058S},
publisher = {IEEE Press},
abstract = {Nowadays, invoking third party code increasingly involves calling web services via their web APIs, as opposed to the more traditional scenario of downloading a library and invoking the library's API. However, there are also new challenges for developers calling these web APIs. In this paper, we highlight a broad set of these challenges and argue for resulting opportunities for software engineering research to support developers in consuming web APIs. We outline two specific research threads in this context: (1) web API specification curation, which enables us to know the signatures of web APIs, and (2) static analysis that is capable of extracting URLs, HTTP methods etc. of web API calls. Furthermore, we present new work on how we combine (1) and (2) to provide IDE support for application developers consuming web APIs. As web APIs are used broadly, research in supporting the consumption of web APIs offers exciting opportunities.},
booktitle = {Proceedings of the 1st International Workshop on API Usage and Evolution},
pages = {7–10},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {WAPI '17}
}

@inproceedings{10.1145/2642803.2642832,
author = {da Silva Amorim, Simone and de Almeida, Eduardo Santana and McGregor, John D. and von Flach G. Chavez, Christina},
title = {When Ecosystems Collide: Making Systems of Systems Work},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642832},
doi = {10.1145/2642803.2642832},
abstract = {The objective of this research is to identify issues that arise when software systems from different ecosystems are composed into a system of systems. Whether it is an explicit, managed ecosystem or the implicit ecosystem of collaborators, competitors, and users that surrounds any organization, the community of organizations has momentum in a particular direction. When a system of systems is composed using systems from each of several ecosystems the differences in communities can prevent satisfactory integration. We examine the case of a portion of the infrastructure needed to support automated driving. We use Maier's criteria that identify challenges for systems of systems as starting points for investigations into differences among ecosystems. We found points of commonality that tie together the ecosystems include use of common standards, membership by large organizations in multiple ecosystems, and influences of the outside business environment. We also found points of variation including whether the systems have compatible real-time expectations, terms of licenses, frequency of new releases, and architectural assumptions. Developers of a system of systems benefit from being aware of the similarities and differences among the ecosystems from which their systems originate because both the similarities and differences help developers anticipate the evolutionary trajectories of the individual systems.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {29},
numpages = {4},
keywords = {system of systems, software ecosystem},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3487664.3487729,
author = {Alharbe, Nawaf},
title = {Security and Privacy of Telehealth Apps in the Era of COVID-19 Pandemic: Saudi Arabia Perspective},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487729},
doi = {10.1145/3487664.3487729},
abstract = {As the novel COVID-19 infection spreads across nations, there is a requirement for novel methods aimed at providing high-quality patient treatment, and managing its spread becomes increasingly pressing. Software-based solutions, such as telehealth apps, could deliver appreciated suggestions on health-centric data to clinicians to improve quality of living, particularly for patients, such as the aged individuals, and pregnant women. In the fight against COVID-19, the utilization of telehealth and digital applications has tremendous possibilities in Saudi Arabia. Data privacy and security concerns have taken a particularly heavy toll on the healthcare business. Mobile based applications privacy and security have emerged as the most important challenges as telemedicine technologies account for revolutionary developments in telehealth. Because of an accelerated research demand, this paper investigates the prospects, applications, and constraints of telehealth apps in Saudi Arabia. Mobile health apps already being used as suitable approaches for minimizing COVID-19 dissemination. To maintain the advantages and risk involved with user information collecting, processing, and sharing, completely transparent and responsible privacy-preserving methods should be built in from the start. Records must only be kept for the period reasonable for the purposes for which they were obtained.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {467–472},
numpages = {6},
keywords = {Threat, Cybersecurity, COVID-19, Pandemic, Healthcare mobile app, telehealth},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/2897683.2897693,
author = {Byrd, Kenny and Mansurov, Alisher and Baysal, Olga},
title = {Mining Twitter Data for Influenza Detection and Surveillance},
year = {2016},
isbn = {9781450341684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897683.2897693},
doi = {10.1145/2897683.2897693},
abstract = {Twitter --- a social media platform --- has gained phenomenal popularity among researchers who have explored its massive volumes of data to offer meaningful insights into many aspects of modern life. Twitter has also drawn great interest from public health community to answer many health-related questions regarding the detection and spread of certain diseases. However, despite the growing popularity of Twitter as an influenza detection source among researchers, healthcare officials do not seem to be as intrigued by the opportunities that social media offers for detecting and monitoring diseases. In this paper, we demonstrate that 1) Twitter messages (tweets) can be reliably classified based on influenza related keywords; 2) the spread of influenza can be predicted with high accuracy; and, 3) there is a way to monitor the spread of influenza in selected cities in real-time. We propose an approach to efficiently mine and extract data from Twitter streams, reliably classify tweets based on their sentiment, and visualize data via a real-time interactive map. Our study benefits not only aspiring researchers who are interested in conducting a study involving the analysis of Twitter data but also health sectors officials who are encouraged to incorporate the analysis of vast information from social media data sources, in particular, Twitter.},
booktitle = {Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
pages = {43–49},
numpages = {7},
keywords = {cold symptoms, flu, data mining, visualization tool, social media, sentiment analysis, public health surveillance},
location = {Austin, Texas},
series = {SEHS '16}
}

@inproceedings{10.1145/2882903.2903741,
author = {Dageville, Benoit and Cruanes, Thierry and Zukowski, Marcin and Antonov, Vadim and Avanes, Artin and Bock, Jon and Claybaugh, Jonathan and Engovatov, Daniel and Hentschel, Martin and Huang, Jiansheng and Lee, Allison W. and Motivala, Ashish and Munir, Abdul Q. and Pelley, Steven and Povinec, Peter and Rahn, Greg and Triantafyllis, Spyridon and Unterbrunner, Philipp},
title = {The Snowflake Elastic Data Warehouse},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903741},
doi = {10.1145/2882903.2903741},
abstract = {We live in the golden age of distributed computing. Public cloud platforms now offer virtually unlimited compute and storage resources on demand. At the same time, the Software-as-a-Service (SaaS) model brings enterprise-class systems to users who previously could not afford such systems due to their cost and complexity. Alas, traditional data warehousing systems are struggling to fit into this new environment. For one thing, they have been designed for fixed resources and are thus unable to leverage the cloud's elasticity. For another thing, their dependence on complex ETL pipelines and physical tuning is at odds with the flexibility and freshness requirements of the cloud's new types of semi-structured data and rapidly evolving workloads. We decided a fundamental redesign was in order. Our mission was to build an enterprise-ready data warehousing solution for the cloud. The result is the Snowflake Elastic Data Warehouse, or "Snowflake" for short. Snowflake is a multi-tenant, transactional, secure, highly scalable and elastic system with full SQL support and built-in extensions for semi-structured and schema-less data. The system is offered as a pay-as-you-go service in the Amazon cloud. Users upload their data to the cloud and can immediately manage and query it using familiar tools and interfaces. Implementation began in late 2012 and Snowflake has been generally available since June 2015. Today, Snowflake is used in production by a growing number of small and large organizations alike. The system runs several million queries per day over multiple petabytes of data.In this paper, we describe the design of Snowflake and its novel multi-cluster, shared-data architecture. The paper highlights some of the key features of Snowflake: extreme elasticity and availability, semi-structured and schema-less data, time travel, and end-to-end security. It concludes with lessons learned and an outlook on ongoing work.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {215–226},
numpages = {12},
keywords = {data warehousing, database as a service, multi-cluster shared data architecture},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.5555/3586210.3586427,
author = {Huddleston, Joshua and Galgoczy, Michael C. and Ghumrawi, Kareem A. and Giabbanelli, Philippe J. and Rice, Ketra L. and Nataraj, Nisha and Brown, Margaret M. and Harper, Christopher R. and Florence, Curtis S.},
title = {Design and Deployment of a Simulation Platform: Case Study of an Agent-Based Model for Youth Suicide Prevention},
year = {2023},
publisher = {IEEE Press},
abstract = {Research has examined the process of engaging end-users in co-designing a simulation model and using it within workplace settings. In contrast, few studies document the software development aspect of creating, packaging, and deploying a simulation platform that satisfies end-users' needs. In this case study, we detail these aspects regarding a platform involving Agent-Based Modeling for youth suicide prevention. Goals and constraints are detailed in three categories of needs, each showing how competing demands are addressed via software solutions: 1) data, encompassing data security (e.g., no direct access to individual-level data) and data updates (e.g., changing a model's initialization without coding expertise); 2) deployment, to satisfy security protocols (e.g., secure intranet communications) and the end-users' experience (e.g., ease of installation); 3) accessibility, ensuring that individuals with impairments are not excluded from using simulations. By presenting practical solutions to each category, our case study supports modelers in addressing their own deployment needs.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2582–2593},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3180155.3180224,
author = {Ren, Zhilei and Jiang, He and Xuan, Jifeng and Yang, Zijiang},
title = {Automated Localization for Unreproducible Builds},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180224},
doi = {10.1145/3180155.3180224},
abstract = {Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries.In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {71–81},
numpages = {11},
keywords = {unreproducible build, software maintenance, localization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1007/978-3-642-25821-3_22,
author = {Rellermeyer, Jan S. and K\"{u}pfer, Ramon},
title = {Co-Managing Software and Hardware Modules through the Juggle Middleware},
year = {2011},
isbn = {9783642258206},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25821-3_22},
doi = {10.1007/978-3-642-25821-3_22},
abstract = {Reprogrammable hardware like Field-Programmable Gate Arrays (FPGAs) is becoming increasingly powerful and affordable. Modern FPGA chips can be reprogrammed at runtime and with low latency which makes them attractive to be used as a dynamic resource in systems. For instance, on mobile devices FPGAs can help to accelerate the performance of critical tasks and at the same time increase the energy-efficiency of the device. The integration of FPGA resources into commodity software, however, is a highly involved task. On the one hand, there is an impedance mismatch between the hardware description languages in which FPGAs are programmed and the high-level languages in which many mobile applications are nowadays developed. On the other hand, the FPGA is a limited and shared resource and as such requires explicit resource management. In this paper, we present the Juggle middleware which leverages the ideas of modularity and service-orientation to facilitate a seamless exchange of hardware and software implementations at runtime. Juggle is built around the well-established OSGi standard for software modules in Java and extends it with support for services implemented in reprogrammable hardware, thereby leveraging the same level of management for both worlds. We show that hardware-accelerated services implemented with Juggle can help to increase the performance of applications and reduce power consumption on mobile devices without requiring any changes to existing program code.},
booktitle = {Proceedings of the 12th ACM/IFIP/USENIX International Conference on Middleware},
pages = {431–450},
numpages = {20},
keywords = {FPGA, OSGi, hardware acceleration},
location = {Lisbon, Portugal},
series = {Middleware'11}
}

@inproceedings{10.1109/ICSE-NIER.2019.00011,
author = {Cioroaica, Emilia and Kuhn, Thomas and Buhnova, Barbora},
title = {(Do Not) Trust in Ecosystems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00011},
doi = {10.1109/ICSE-NIER.2019.00011},
abstract = {In the context of Smart Ecosystems, systems engage in dynamic cooperation with other systems to achieve their goals. Expedient operation is only possible when all systems cooperate as expected. This requires a level of trust between the components of the ecosystem. New systems that join the ecosystem therefore first need to build up a level of trust. Humans derive trust from behavioral reputation in key situations. In Smart Ecosystems (SES), the reputation of a system or system component can also be based on observation of its behavior.In this paper, we introduce a method and a test platform that support virtual evaluation of decisions at runtime, thereby supporting trust building within SES. The key idea behind the platform is that it employs and evaluates Digital Twins, which are executable models of system components, to learn about component behavior in observed situations. The trust in the Digital Twin then builds up over time based on the behavioral compliance of the real system component with its Digital Twin. In this paper, we use the context of automotive ecosystems and examine the concepts for building up reputation on control algorithms of smart agents dynamically downloaded at runtime to individual autonomous vehicles within the ecosystem.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {9–12},
numpages = {4},
keywords = {malicious behavior, automotive, building trust, virtual evaluation, smart ecosystems},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/2541940.2541972,
author = {Andrus, Jeremy and Van't Hof, Alexander and AlDuaij, Naser and Dall, Christoffer and Viennot, Nicolas and Nieh, Jason},
title = {Cider: Native Execution of IOS Apps on Android},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541972},
doi = {10.1145/2541940.2541972},
abstract = {We present Cider, an operating system compatibility architecture that can run applications built for different mobile ecosystems, iOS or Android, together on the same smartphone or tablet. Cider enhances the domestic operating system, Android, of a device with kernel-managed, per-thread personas to mimic the application binary interface of a foreign operating system, iOS, enabling it to run unmodified foreign binaries. This is accomplished using a novel combination of binary compatibility techniques including two new mechanisms: compile-time code adaptation, and diplomatic functions. Compile-time code adaptation enables existing unmodified foreign source code to be reused in the domestic kernel, reducing implementation effort required to support multiple binary interfaces for executing domestic and foreign applications. Diplomatic functions leverage per-thread personas, and allow foreign applications to use domestic libraries to access proprietary software and hardware interfaces. We have built a Cider prototype, and demonstrate that it imposes modest performance overhead and runs unmodified iOS and Android applications together on a Google Nexus tablet running the latest version of Android.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {367–382},
numpages = {16},
keywords = {ios, binary compatibility, operating system compatibility, mobile computing, android},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@inproceedings{10.1145/3437378.3444367,
author = {Aslanpour, Mohammad S. and Toosi, Adel N. and Cicconetti, Claudio and Javadi, Bahman and Sbarski, Peter and Taibi, Davide and Assuncao, Marcos and Gill, Sukhpal Singh and Gaire, Raj and Dustdar, Schahram},
title = {Serverless Edge Computing: Vision and Challenges},
year = {2021},
isbn = {9781450389563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437378.3444367},
doi = {10.1145/3437378.3444367},
abstract = {Born from a need for a pure “pay-per-use” model and highly scalable platform, the “Serverless” paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things (IoT) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and IoT applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.},
booktitle = {Proceedings of the 2021 Australasian Computer Science Week Multiconference},
articleno = {10},
numpages = {10},
location = {Dunedin, New Zealand},
series = {ACSW '21}
}

@article{10.1145/2237796.2237799,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/2237796.2237799},
doi = {10.1145/2237796.2237799},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {11–19},
numpages = {9}
}

@inproceedings{10.1145/3549035.3561182,
author = {Oishwee, Sahrima Jannat and Codabux, Zadia and Stakhanova, Natalia},
title = {An Exploratory Study on the Relationship of Smells and Design Issues with Software Vulnerabilities},
year = {2022},
isbn = {9781450394574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549035.3561182},
doi = {10.1145/3549035.3561182},
abstract = {Software vulnerabilities are one of the leading causes of the  
loss of confidential data resulting in financial damages in the  
industry. As a result, software companies strive to discover potential  
vulnerabilities before the software is deployed. While traditionally,  
software metrics have been widely used to uncover vulnerabilities,  
more recent studies have been looking at code smells to detect  
vulnerabilities. This preliminary study explores the relationship  
between smells, design issues, and software vulnerabilities. As  
smells and design issues are indicators of potential problems in  
the software, establishing a relationship with vulnerabilities can  
be helpful for vulnerability prediction. In this study, we analyzed  
561 versions of nine open-source software by exploring the smells  
and design issues in the vulnerable and non-vulnerable classes.  
We found that some smells and design issues have a statistically  
significant relationship with the vulnerable classes. However, after a  
manual analysis of the code segments containing the vulnerabilities,  
we found no indication that smells or design issues induce the  
vulnerabilities. In fact, they were still present in those code  
segments even after the vulnerabilities were resolved.},
booktitle = {Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security},
pages = {16–20},
numpages = {5},
keywords = {Code Smells, Design Issues, Software Security, Mining Software Repositories, Software Vulnerabilities},
location = {Singapore, Singapore},
series = {MSR4P&amp;S 2022}
}

@inproceedings{10.1145/3463274.3463335,
author = {Aljedaani, Wajdi and Peruma, Anthony and Aljohani, Ahmed and Alotaibi, Mazen and Mkaouer, Mohamed Wiem and Ouni, Ali and Newman, Christian D. and Ghallab, Abdullatif and Ludi, Stephanie},
title = {Test Smell Detection Tools: A Systematic Mapping Study},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463335},
doi = {10.1145/3463274.3463335},
abstract = {Test smells are defined as sub-optimal design choices developers make when implementing test cases. Hence, similar to code smells, the research community has produced numerous test smell detection tools to investigate the impact of test smells on the quality and maintenance of test suites. However, little is known about the characteristics, type of smells, target language, and availability of these published tools. In this paper, we provide a detailed catalog of all known, peer-reviewed, test smell detection tools.We start with performing a comprehensive search of peer-reviewed scientific publications to construct a catalog of 22 tools. Then, we perform a comparative analysis to identify the smell types detected by each tool and other salient features that include programming language, testing framework support, detection strategy, and adoption, among others. From our findings, we discover tools that detect test smells in Java, Scala, Smalltalk, and C++ test suites, with Java support favored by most tools. These tools are available as command-line and IDE plugins, among others. Our analysis also shows that most tools overlap in detecting specific smell types, such as General Fixture. Further, we encounter four types of techniques these tools utilize to detect smells. We envision our study as a one-stop source for researchers and practitioners in determining the tool appropriate for their needs. Our findings also empower the community with information to guide future tool development.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {170–180},
numpages = {11},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.1145/3356773.3356803,
author = {Sutton, Stanley M. and Armbrust, Ove and Hebig, Regina and Clarke, Paul},
title = {Summary of the 2019 International Conference on Software and System Processes (ICSSP 2019)},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356803},
doi = {10.1145/3356773.3356803},
abstract = {The 2019 International Conference on Software and System Processes (ICSSP 2019) was held in conjunction with the 41st International Conference on Software Engineering (ICSE 2019) in Montreal, Canada, May 25-26, 2019. ICSSP is a leading international forum for research on software and systems processesthe 2019 conference extended a sequence of predecessor conferences and workshops stretching back more than a decade. Although ICSSP has often been associated with ICSE and "in cooperation" with ACM, 2019 was the rst year that ICSSP was operated under a full co-located event agreement with ICSE and was fully sponsored by ACM and IEEE. ICSSP 2019 was organized, as usual, by the International Software and Systems Process Association.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {34–37},
numpages = {4}
}

@proceedings{10.1145/3559712,
title = {SBCARS '22: Proceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse},
year = {2022},
isbn = {9781450397452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Uberlandia, Brazil}
}

@inproceedings{10.1145/2873587.2873591,
author = {Bhardwaj, Ketan and Gavrilovska, Ada and Schwan, Karsten},
title = {Ephemeral Apps},
year = {2016},
isbn = {9781450341455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2873587.2873591},
doi = {10.1145/2873587.2873591},
abstract = {Despite a tremendous increase in the number of mobile apps, coupled with their popularity with consumers, there exists a wide gap in app availability vs. their use. Recent trends suggest that this gap will further widen in the future. Ephemeral apps, proposed in this paper, lower the barrier for end-user app acceptance by removing the app installation step when `trying out' new apps, without requiring modifications to current apps or any additional programming efforts by app developers. We estimate the resulting reduction in time-to-use for apps to be a factor of 10x by leveraging the emerging `edge cloud' tier of the Internet.},
booktitle = {Proceedings of the 17th International Workshop on Mobile Computing Systems and Applications},
pages = {81–86},
numpages = {6},
keywords = {sanboxing, mobile apps, app stores, performance, web apps, ephemeral apps, permissions},
location = {St. Augustine, Florida, USA},
series = {HotMobile '16}
}

@inproceedings{10.1145/2134254.2134277,
author = {Sarkar, Santonu and Maltouf, Mageri Filali},
title = {Identifying Hotspots in a Program for Data Parallel Architecture: An Early Experience},
year = {2012},
isbn = {9781450311427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2134254.2134277},
doi = {10.1145/2134254.2134277},
abstract = {In applications that rely on data intensive computation, one can gain significant performance if the source code is suitably transformed for parallel hardware. A common approach is to identify the loops inside the program that consume a significant amount of time, that we call hotspots. One of the impending business need here is to quickly identify such loops for further transformation. However, the exact identification of such hotspots requires an elaborate runtime analysis. When we deal with a third party business application, only a partial version of the source code is available, with limited test inputs, which hinders a correct runtime analysis. Therefore, we resort to static analysis of source code to get a conservative loop iteration count. In this paper we describe our approach to analyze a source code to find hotspots. Our approach is based on estimating the iteration count of a loop using the polytope model for volume computation. This is then combined with the cyclomatic complexity measurement of the loop body. Both these metrics together provides an approximate idea of hotspots in a program and serves as a code transformation clue to programmers. We have run our tool on Rodinia benchmark applications and found encouraging results.},
booktitle = {Proceedings of the 5th India Software Engineering Conference},
pages = {131–137},
numpages = {7},
keywords = {polytope model, compilers, loop analysis, GPU, static analysis},
location = {Kanpur, India},
series = {ISEC '12}
}

@inproceedings{10.5555/2387880.2387901,
author = {Levis, Philip},
title = {Experiences from a Decade of TinyOS Development},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {When first written in 2000, TinyOS's users were a handful of academic computer science researchers. A decade later, TinyOS averages 25,000 downloads a year, is in many commercial products, and remains a platform used for a great deal of sensor network, low-power systems, and wireless research.We focus on how technical and social decisions influenced this success, sometimes in surprising ways. As TinyOS matured, it evolved language extensions to help experts write efficient, robust systems. These extensions revealed insights and novel programming abstractions for embedded software. Using these abstractions, experts could build increasingly complex systems more easily than with other operating systems, making TinyOS the dominant choice.This success, however, came at a long-term cost. System design decisions that seem good at first can have unforeseen and undesirable implications that play out over the span of years. Today, TinyOS is a stable, self-contained ecosystem that is discouraging to new users. Other systems, such as Arduino and Contiki, by remaining more accessible, have emerged as better solutions for simpler embedded sensing applications.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {207–220},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.1145/3133956.3134059,
author = {Derr, Erik and Bugiel, Sven and Fahl, Sascha and Acar, Yasemin and Backes, Michael},
title = {Keep Me Updated: An Empirical Study of Third-Party Library Updatability on Android},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134059},
doi = {10.1145/3133956.3134059},
abstract = {Third-party libraries in Android apps have repeatedly been shown to be hazards to the users' privacy and an amplification of their host apps' attack surface. A particularly aggravating factor to this situation is that the libraries' version included in apps are very often outdated.This paper makes the first contribution towards solving the problem of library outdatedness on Android. First, we conduct a survey with 203 app developers from Google Play to retrieve first-hand information about their usage of libraries and requirements for more effective library updates. With a subsequent study of library providers' semantic versioning practices, we uncover that those providers are likely a contributing factor to the app developers' abstinence from library updates in order to avoid ostensible re-integration efforts and version incompatibilities. Further, we conduct a large-scale library updatability analysis of 1,264,118 apps to show that, based on the library API usage, 85.6% of the libraries could be upgraded by at least one version without modifying the app code, 48.2% even to the latest version. Particularly alarming are our findings that 97.8% out of 16,837 actively used library versions with a known security vulnerability could be easily fixed through a drop-in replacement of the vulnerable library with the fixed version. Based on these results, we conclude with a thorough discussion of solutions and actionable items for different actors in the app ecosystem to effectively remedy this situation.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2187–2200},
numpages = {14},
keywords = {third-party library, updatability, app security, api, android},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.1109/ASE.2009.73,
author = {Ki, Yuhoon and Song, Meongchul},
title = {An Open Source-Based Approach to Software Development Infrastructures},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.73},
doi = {10.1109/ASE.2009.73},
abstract = {As software systems become larger and more complex, automated software engineering tools play a crucial role for effective software development management, which is a key factor to lead quality software systems. In this work, we present TRICA, an open source-based software development infrastructure. The name of TRICA represents its features such as Traceability, Relationship, Informativeness, Cost-effectiveness, and Automation. Essentially, in TRICA, a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool. We provisioned a mechanism to connect the open source tools in TRICA so that project members use the collaborated information to solve various issues and implementation problems efficiently, and easily share forthcoming issues during the course of the project. We show that TRICA can help to decentralize risks throughout the software development cycle and achieve successful software development.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {525–529},
numpages = {5},
keywords = {software engineering tools, issue tracking, open source, continuous integration, SCM},
series = {ASE '09}
}

@inproceedings{10.1145/2079360.2079369,
author = {Utani, Arifumi and Mizumoto, Teruhiro and Okumura, Takashi},
title = {How Geeks Responded to a Catastrophic Disaster of a High-Tech Country: Rapid Development of Counter-Disaster Systems for the Great East Japan Earthquake of March 2011},
year = {2011},
isbn = {9781450310444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2079360.2079369},
doi = {10.1145/2079360.2079369},
abstract = {A devastating earthquake hit Japan on March 11, 2011. A history of frequent and powerful earthquakes in the region, especially the great Hanshin-Awaji earthquake of 1995, led the country to develop disaster relief methods in preparation for such natural disasters. Nevertheless, the earthquake and following tsunami destroyed much of the coastland, and caused panic, due to the Fukushima-daiichi nuclear power plant accidents. During this situation, some of the crisis-management systems performed as expected. However, the poor performance of the others required system developers to implement new and improved counter-disaster systems on the fly. Such systems include the nationwide refugee locator, relief supply matching system, planning applications for scheduled power outages in the metropolitan area, twitter-mining systems for realtime monitoring of public transportation systems, etc. After the disaster, we conducted a comprehensive survey of such systems, in order to record how geeks in the high-tech country responded to such a national crisis. The analysis of the resulting list of counter-disaster applications gave us useful insight for future disasters: i) authorities are advised to disclose statistical information as quickly as possible, ii) coordination among developers must be provided, and iii) interconnection of databases is essential for efficiency.},
booktitle = {Proceedings of the Special Workshop on Internet and Disasters},
articleno = {9},
numpages = {8},
keywords = {software development, e-government, disaster response},
location = {Tokyo, Japan},
series = {SWID '11}
}

@inproceedings{10.1145/2800835.2801629,
author = {Lea, Rodger and Blackstock, Mike and Giang, Nam and Vogt, David},
title = {Smart Cities: Engaging Users and Developers to Foster Innovation Ecosystems},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2801629},
doi = {10.1145/2800835.2801629},
abstract = {Increasingly, city planners and government officials understand that cities are engines of innovation and wealth creation. Equally, there is a growing understanding that the application of technology in support of Smart Cities helps grow the urban economy and deliver better services to citizens. However, often Smart City projects are top-down projects focused on improving city infrastructure using technology. We argue, and our experience over the last decade has shown, that often, citizen driven, or grass-roots based Smart City projects deliver better value and sustainable success. In this paper we report on our work to engage citizens and the technology community in smart city projects and highlight some lessons learnt from our experiences. We show how a modest investment in a Smart City Data Hub (using our IoT platform -- WoTKit) plus development tools based on Node-RED helps bootstrap a Smart City innovation cluster.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1535–1542},
numpages = {8},
keywords = {middleware, internet of things (IoT), data hubs, WoTKit, Node-RED, smart cities, innovation},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/1409720.1409730,
author = {Cottam, Joseph A. and Hursey, Joshua and Lumsdaine, Andrew},
title = {Representing Unit Test Data for Large Scale Software Development},
year = {2008},
isbn = {9781605581125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1409720.1409730},
doi = {10.1145/1409720.1409730},
abstract = {Large scale software projects rely on routine, automated testing to gauge progress towards its goals. The diversity and quantity of these tests grow as time and project scope increase. This is as a consequence of both experience and expanding audience. It becomes increasingly difficult to interpret testing results as the testing suites multiply and diversify. If interpretation becomes too difficult, testings results could become ignored all together. Visualization has proven to be an effective tool to aid the interpretation of large amounts of data. We have adapted visualization techniques based on small multiples to communicate the health of the software project across several levels of abstraction. The collective set of techniques we refer to as the SeeTest visualization schema. We applied this visualization technique to the Open MPI test results in order to assist developers in the software release cycle. Through the visualizations, developers found a variety of surprising mismatches between their data and their intuitions. This exploration did not involve collecting any data not already being collected, merely presenting it in manner that better supported their needs. In this paper, we detail the development of the representation we used and give more particular analysis of the insights gained by the Open MPI community. The techniques presented in this paper can be applied to other software projects.},
booktitle = {Proceedings of the 4th ACM Symposium on Software Visualization},
pages = {57–66},
numpages = {10},
keywords = {MPI, testing, visualization, project management},
location = {Ammersee, Germany},
series = {SoftVis '08}
}

@inproceedings{10.1145/3524610.3527876,
author = {Venigalla, Akhila Sri Manasa and Boyalakuntla, Kowndinya and Chimalakonda, Sridhar},
title = {GitQ- towards Using Badges as Visual Cues for GitHub Projects},
year = {2022},
isbn = {9781450392983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524610.3527876},
doi = {10.1145/3524610.3527876},
abstract = {GitHub hosts millions of software repositories, facilitating developers to contribute to many projects in multiple ways. Most of the information about the repositories is text-based in the form of stars, forks, commits, and so on. However, developers willing to contribute to projects on GitHub often find it challenging to select appropriate projects to contribute to or reuse due to the large number of repositories present on GitHub. Further, obtaining this required information often becomes a tedious process, as one has to carefully mine information hidden inside the repository. To alleviate the effort intensive mining procedures, researchers have proposed npm-badges to outline information relating to build status of a project. However, these badges are static and limit their usage to package dependency and build details. Adding visual cues such as badges, (see PDF) to the repositories might reduce the search space for developers. Hence, we present GitQ, to automatically augment GitHub repositories with badges representing information about source code and project maintenance. Presenting GitQ as a browser plugin to GitHub could make it easily accessible to developers using GitHub. GitQ is evaluated with 15 developers based on the UTAUT model to understand developer perception towards its usefulness. We observed that 11 out of 15 developers perceived GitQ to be useful in identifying the right set of repositories using visual cues such as (see PDF) generated by GitQ. The source code and tool are available for download on GitHub at https://github.com/gitq-for-github/plugin, and the demo can be found at https://youtu.be/c0yohmIat3A.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
pages = {157–161},
numpages = {5},
keywords = {quality badges, GitHub repositories, software quality, ck metrics},
location = {Virtual Event},
series = {ICPC '22}
}

@inproceedings{10.1145/2597073.2597135,
author = {Farah, Gabriel and Tejada, Juan Sebastian and Correal, Dario},
title = {OpenHub: A Scalable Architecture for the Analysis of Software Quality Attributes},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597135},
doi = {10.1145/2597073.2597135},
abstract = {There is currently a vast array of open source projects available on the web, and although they are searchable by name or description in the search engines, there is no way to search for projects by how well they perform on a given set of quality attributes (e.g. usability or maintainability). With OpenHub, we present a scalable and extensible architecture for the static and runtime analysis of open source repositories written in Python, presenting the architecture and pinpointing future possibilities with it.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {420–423},
numpages = {4},
keywords = {GitHub, Python, Architecture, Quality Attributes},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/3548606.3563443,
author = {Torres-Arias, Santiago and Melara, Marcela and Simon, Laurent},
title = {SCORED '22: ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3563443},
doi = {10.1145/3548606.3563443},
abstract = {Recent attacks on the software supply chain have shed light on the fragility and importance of ensuring the security and integrity of this vital ecosystem. Addressing the technical and social challenges to building trustworthy software for deployment in sensitive and/or large-scale enterprise or governmental settings requires innovative solutions and an interdisciplinary approach. The Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses (SCORED) is a venue that brings together industry practitioners, academics, and policymakers to present and discuss security vulnerabilities, novel defenses against attacks, project demos, adoption requirements and best practices in the software supply chain. The complete SCORED'22 workshop proceedings are available at: https://dl.acm.org/doi/proceedings/10.1145/3560835},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3555–3556},
numpages = {2},
keywords = {defenses, software supply chain, attacks, security},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.5555/3433701.3433756,
author = {Torok, Gabor and Day, Mark R. and Hartman-Baker, Rebecca J. and Snavely, Cory},
title = {Iris: Allocation Banking and Identity and Access Management for the Exascale Era},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Without a reliable and scalable system for managing authorized users and ensuring they receive their allocated share of computational and storage resources, modern HPC centers would not be able to function. Exascale will amplify these demands with greater machine scale, more users, higher job throughput, and ever-increasing need for management insight and automation throughout the HPC environment. When our legacy system reached retirement age, NERSC took the opportunity to design and build Iris not only to meet our current needs, with 8,000 users and tens of thousands of jobs per day, but also to scale well into the exascale era. In this paper, we describe how we have designed Iris to meet these needs and discuss its key features as well as our implementation experience.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {42},
numpages = {11},
keywords = {allocation banking, identity and access management},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1145/3474827,
author = {Ebrahimi, Fahimeh and Tushev, Miroslav and Mahmoud, Anas},
title = {Classifying Mobile Applications Using Word Embeddings},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3474827},
doi = {10.1145/3474827},
abstract = {Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically static—new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&amp;Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {20},
numpages = {30},
keywords = {Word2Vec, fastText, word embeddings, GloVe, App classification, app store}
}

@inproceedings{10.1145/3196398.3196434,
author = {Mahmoudi, Mehran and Nadi, Sarah},
title = {The Android Update Problem: An Empirical Study},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196434},
doi = {10.1145/3196398.3196434},
abstract = {Many phone vendors use Android as their underlying OS, but often extend it to add new functionality and to make it compatible with their specific phones. When a new version of Android is released, phone vendors need to merge or re-apply their customizations and changes to the new release. This is a difficult and time-consuming process, which often leads to late adoption of new versions. In this paper, we perform an empirical study to understand the nature of changes that phone vendors make, versus changes made in the original development of Android. By investigating the overlap of different changes, we also determine the possibility of having automated support for merging them. We develop a publicly available tool chain, based on a combination of existing tools, to study such changes and their overlap. As a proxy case study, we analyze the changes in the popular community-based variant of Android, LineageOS, and its corresponding Android versions. We investigate and report the common types of changes that occur in practice. Our findings show that 83% of subsystems modified by LineageOS are also modified in the next release of Android. By taking the nature of overlapping changes into account, we assess the feasibility of having automated tool support to help phone vendors with the Android update problem. Our results show that 56% of the changes in LineageOS have the potential to be safely automated.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {220–230},
numpages = {11},
keywords = {software merging, software evolution, Android, merge conflicts},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2811681.2811698,
author = {Parsons, David and Susnjak, Teo and Mathrani, Anuradha},
title = {The Software Developer Cycle: Career Demographics and the Market Clock: Or, is SQL the New COBOL?},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811698},
doi = {10.1145/2811681.2811698},
abstract = {The software development community includes people from a wide spectrum of age and experience. While the industry itself is constantly evolving with new languages, tools and methods, developers themselves are advancing through their careers. A consequence of these two paths of change is that future demand for software-related skills is unpredictable, leading to challenges for educators, trainers and strategists. In order to gain some insights into the impact of such changes, we gathered a snapshot of a set of demographic and profile data from 443 developers who attended a global day of coderetreat. We analyzed the data to seek relevant patterns in the demographics, software development skills and activities of our respondents. Then, using the concept of the IT market clock as a framework for analysis, we identified trends from this dataset to formulate informed predictions about how the software development community may evolve, as a result of generational changes over time, and what impact that may have on the legacy systems of the future.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {86–90},
numpages = {5},
keywords = {legacy systems, demographics, survey, software development skills, IT market clock},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@article{10.1145/3434168,
author = {Chen, Yan and Lasecki, Walter S. and Dong, Tao},
title = {Towards Supporting Programming Education at Scale via Live Streaming},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434168},
doi = {10.1145/3434168},
abstract = {Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {259},
numpages = {19},
keywords = {live streaming, live coding, programming education, informal learning}
}

@article{10.1145/2483852.2483866,
author = {Al Bahra, Samy},
title = {Nonblocking Algorithms and Scalable Multicore Programming},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2483852.2483866},
doi = {10.1145/2483852.2483866},
abstract = {Exploring an alternative to lock-based synchronization.},
journal = {Commun. ACM},
month = {jul},
pages = {50–61},
numpages = {12}
}

@article{10.5555/1040151.1040184,
author = {Wilkins, Darren and Cole, Donald and Nelson, Michael},
title = {Windows Interoperability with Linux in the Enterprise (WINWILE): A Solution to the High Cost of Licensing, Downtime, and Security Problems},
year = {2004},
issue_date = {December 2004},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {20},
number = {2},
issn = {1937-4771},
abstract = {The cost of running an exclusive Microsoft platform can be prohibitive for many organizations. Never ending updates, upgrades, and patches, coupled with what seems to be a constant barrage of viruses and security concerns, can easily lead to an unacceptable amount of downtime. This paper reports on a successful effort to create a heterogeneous system utilizing both Microsoft and non-Microsoft products, including Linux. Initial user feedback has been quite positive, and cost savings can easily be 40% or more.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {260–266},
numpages = {7}
}

@inproceedings{10.1145/1321261.1321311,
author = {Marks, Stefan and Windsor, John and W\"{u}nsche, Burkhard},
title = {Evaluation of Game Engines for Simulated Surgical Training},
year = {2007},
isbn = {9781595939128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321261.1321311},
doi = {10.1145/1321261.1321311},
abstract = {The increasing complexity and costs of surgical training and the constant development of new surgical procedures has made virtual surgical training an essential tool in medical education. Unfortunately, commercial tools are very expensive and have a small support base. Game engines offer unique advantages for the creation of highly interactive and collaborative environments.This paper examines the suitability of currently available game engines for developing applications for medical education and simulated surgical training. We formally evaluate a list of available game engines for stability, availability, the possibility of custom content creation and the interaction of multiple users via a network. Based on these criteria, three of the highest ranked engines are used for further case studies.We found that in general it is possible to easily create scenarios with custom medical models that can be cooperatively viewed and interacted with. Limitations in physical simulation capabilities make some engines unsuitable for fully interactive applications, but they can be used in combination with predefined animations. We show that overall game engines represent a good foundation for low cost virtual surgery applications and we discuss technologies which can be used to further extend their physical simulation capabilities.},
booktitle = {Proceedings of the 5th International Conference on Computer Graphics and Interactive Techniques in Australia and Southeast Asia},
pages = {273–280},
numpages = {8},
keywords = {game engines, collaborative environments, simulation systems, physically-based animation},
location = {Perth, Australia},
series = {GRAPHITE '07}
}

@inproceedings{10.1145/2648511.2648523,
author = {Urli, Simon and Blay-Fornarino, Mireille and Collet, Philippe},
title = {Handling Complex Configurations in Software Product Lines: A Tooled Approach},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648523},
doi = {10.1145/2648511.2648523},
abstract = {As Software Product Lines (SPLs) are now more widely applied in new application fields such as IT or Web systems, complex and large-scale configurations have to be handled. In these fields, the strong domain orientation leads to the need to manage interrelated SPLs and multiple instances of configured sub-products, resulting in complex configurations that cannot be easily represented by simple sets of features. In this paper we propose a tooled approach to manage such SPLs through a domain model that interrelates several feature models in a consistent way. The approach thus shifts part of the domain knowledge to the problem space and supports the derivation of complex configurations with multiple instantiations and associations of sub-products. We also report on the application of our approach to an industrial-strength software development in the field of digital signage.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {112–121},
numpages = {10},
keywords = {configuration, software product line},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3150994.3150996,
author = {Fox, William and Ghoshal, Devarshi and Souza, Abel and Rodrigo, Gonzalo P. and Ramakrishnan, Lavanya},
title = {E-HPC: A Library for Elastic Resource Management in HPC Environments},
year = {2017},
isbn = {9781450351294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150994.3150996},
doi = {10.1145/3150994.3150996},
abstract = {Next-generation data-intensive scientific workflows need to support streaming and real-time applications with dynamic resource needs on high performance computing (HPC) platforms. The static resource allocation model on current HPC systems that was designed for monolithic MPI applications is insufficient to support the elastic resource needs of current and future workflows. In this paper, we discuss the design, implementation and evaluation of Elastic-HPC (E-HPC), an elastic framework for managing resources for scientific workflows on current HPC systems. E-HPC considers a resource slot for a workflow as an elastic window that might map to different physical resources over the duration of a workflow. Our framework uses checkpoint-restart as the underlying mechanism to migrate workflow execution across the dynamic window of resources. E-HPC provides the foundation necessary to enable dynamic resource allocation of HPC resources that are needed for streaming and real-time workflows. E-HPC has negligible overhead beyond the cost of checkpointing. Additionally, E-HPC results in decreased turnaround time of workflows compared to traditional model of resource allocation for workflows, where resources are allocated per stage of the workflow. Our evaluation shows that E-HPC improves core hour utilization for common workflow resource use patterns and provides an effective framework for elastic expansion of resources for applications with dynamic resource needs.},
booktitle = {Proceedings of the 12th Workshop on Workflows in Support of Large-Scale Science},
articleno = {1},
numpages = {11},
keywords = {elastic resource management, HPC systems, scientific workflows},
location = {Denver, Colorado},
series = {WORKS '17}
}

@inproceedings{10.5555/3291291.3291319,
author = {Beigi-Mohammadi, Nasim and Litoiu, Marin and Emami-Taba, Mahsa and Tahvildari, Ladan and Fokaefs, Marios and Merlo, Ettore and Onut, Iosif Viorel},
title = {A DevOps Framework for Quality-Driven Self-Protection in Web Software Systems},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Modern software is developed, deployed and operates continuously. At the same time, cyberattacks are on the rise. The continuity of development and operations and the constant threat of attacks requires novel approaches to identify, analyze and address potential security vulnerabilities. In this continuous and volatile execution environment, factors like security, performance, cost and functionality may not be able to be guaranteed in the same degree at the same time. In this work, we propose a DevOps framework for security adaptation that enables the development and operations teams to collaborate and address security vulnerabilities. The proposed framework spans across the different phases of software (development, operations, maintenance) and considers all other factors (performance, cost, functionality), when deciding for security adaptations. We demonstrate the approach on a prototype tool that shows how teams work together to tackle security concerns.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {270–274},
numpages = {5},
keywords = {software defined infrastructure, security, devops, self-adaptive systems, self-protection, web software},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3437359.3465576,
author = {Stubbs, Joe and Marru, Suresh and Mejia, Daniel and Navarro, John-Paul and Franz, Eric and Black, Steve and Wannipurage, Dimuthu and Pamidighantam, Sudhakar and Stirm, Claire and Dahan, Maytal and Pierce, Marlon and Zentner, Michael},
title = {Common Resource Descriptions for Interoperable Gateway Cyberinfrastructure},
year = {2021},
isbn = {9781450382922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437359.3465576},
doi = {10.1145/3437359.3465576},
abstract = {Science gateway projects face challenges utilizing the vast and heterogeneous landscape of powerful cyberinfrastructure available today, and interoperability across technologies remains poor. This interoperability issue leads to myriad problems: inability to bring multiple heterogeneous specialized resources together to solve problems where different resources are optimized for different facets of the problem; inability to choose from multiple resources on-the-fly as needed based on characteristics and available capacity; and ultimately a less than optimal application of nationally-funded resources toward advancing science. This paper presents version 1.0 of the Science Gateways Community Institute (SGCI) Resource Description Specification – a schema providing a common language for describing storage and computing resources utilized by science gateway technologies – as well as an Inventory API and software development kits for incorporating resource definitions into gateway projects. We discuss multiple gateway integration design options, with trade offs regarding robustness and availability. We detail the adoption to date of the SGCI Resource Specification by several prominent projects, including Apache Airavata, HUBzero®, Open OnDemand, Tapis, and XSEDE. The XSEDE adoption is worth highlighting explicitly as it has led to a new API within the XSEDE Information Services architecture which provides SGCI resource descriptions of all active XSEDE resources. Additionally, we show how the use of the SGCI Resource Specification provides interoperability across resource providers and projects that adopt it. Finally, as a proof of concept, we present a multi-step analysis that runs Quantum ESPRESSO and visualizes the energy band structures of a Gallium Arsenide (GaAs) crystal across multiple resource providers including the Halstead cluster at Purdue University and the Stampede2 supercomputer at TACC.},
booktitle = {Practice and Experience in Advanced Research Computing},
articleno = {20},
numpages = {9},
keywords = {interoperability, science gateways community institute, resource description, Cyberinfrastructure},
location = {Boston, MA, USA},
series = {PEARC '21}
}

@proceedings{10.1145/3624486,
title = {ESAAM '23: Proceedings of the 3rd Eclipse Security, AI, Architecture and Modelling Conference on Cloud to Edge Continuum},
year = {2023},
isbn = {9798400708350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ludwigsburg, Germany}
}

@article{10.1145/2089125.2089127,
author = {Crowston, Kevin and Wei, Kangning and Howison, James and Wiggins, Andrea},
title = {Free/Libre Open-Source Software Development: What We Know and What We Do Not Know},
year = {2008},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/2089125.2089127},
doi = {10.1145/2089125.2089127},
abstract = {We review the empirical research on Free/Libre and Open-Source Software (FLOSS) development and assess the state of the literature. We develop a framework for organizing the literature based on the input-mediator-output-input (IMOI) model from the small groups literature. We present a quantitative summary of articles selected for the review and then discuss findings of this literature categorized into issues pertaining to inputs (e.g., member characteristics, technology use, and project characteristics), processes (software development practices, social processes, and firm involvement practices), emergent states (e.g., social states and task-related states), and outputs (e.g. team performance, FLOSS implementation, and project evolution). Based on this review, we suggest topics for future research, as well as identify methodological and theoretical issues for future inquiry in this area, including issues relating to sampling and the need for more longitudinal studies.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {7},
numpages = {35},
keywords = {Free/Libre open-source software, computer-mediated communication, distributed work, development}
}

@inproceedings{10.1145/584955.584961,
author = {Faber, Brenton D.},
title = {Educational Models and Open Source: Resisting the Proprietary University},
year = {2002},
isbn = {1581135432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584955.584961},
doi = {10.1145/584955.584961},
abstract = {This paper presents an educational model derived from open source methods for computer programming. The article places this search for an alternative model within a framework of proprietary educational practices that are driven by a need for efficiency and rationalization. As an alternative model, the paper suggests that an open source derived educational process would emphasize collaborative problem based learning, working through drafts, risk taking, mentoring, user testing, releasing early and often, developing in collaboration with users, and rewarding and building from failure.At the same time, the paper notes that such a system would have much in common with existing theories of project-based or activity-based learning and with traditional methods of research and publication in scientific endeavors. However, the paper also argues that such a method is different from the open-course or open-curriculum projects recently publicized by several well-known universities as these practices appear to emphasize derived content rather than an open representation of process, or how the content was developed.Collaborative, problem-based learning provides constructive approaches for building corporate and community partnerships on university campuses. At the same time, the model teaches students about collaborative work practices, working as part of a larger community, and the nature of collaborative knowledge building. As such, the model reconnects knowledge creation to research communities and to communities of users and it complicates the belief that sustainable, useful innovation can occur within proprietary systems.},
booktitle = {Proceedings of the 20th Annual International Conference on Computer Documentation},
pages = {31–38},
numpages = {8},
keywords = {open source software, educational models, collaborative design},
location = {Toronto, Ontario, Canada},
series = {SIGDOC '02}
}

@inproceedings{10.1145/3382494.3410693,
author = {Haque, Mubin Ul and Iwaya, Leonardo Horn and Babar, M. Ali},
title = {Challenges in Docker Development: A Large-Scale Study Using Stack Overflow},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410693},
doi = {10.1145/3382494.3410693},
abstract = {Background: Docker technology has been increasingly used among software developers in a multitude of projects. This growing interest is due to the fact that Docker technology supports a convenient process for creating and building containers, promoting close cooperation between developer and operations teams, and enabling continuous software delivery. As a fast-growing technology, it is important to identify the Docker-related topics that are most popular as well as existing challenges and difficulties that developers face.Aims: This paper presents a large-scale empirical study identifying practitioners' perspectives on Docker technology by mining posts from the Stack Overflow (SoF) community. Method: A dataset of 113, 922 Docker-related posts was created based on a set of relevant tags and contents. The dataset was cleaned and prepared. Topic modelling was conducted using Latent Dirichlet Allocation (LDA), allowing the identification of dominant topics in the domain. Results: Our results show that most developers use SoF to ask about a broad spectrum of Docker topics including framework development, application deployment, continuous integration, web-server configuration and many more. We determined that 30 topics that developers discuss can be grouped into 13 main categories. Most of the posts belong to categories of application development, configuration, and networking. On the other hand, we find that the posts on monitoring status, transferring data, and authenticating users are more popular among developers compared to the other topics. Specifically, developers face challenges in web browser issues, networking error and memory management. Besides, there is a lack of experts in this domain. Conclusion: Our research findings will guide future work on the development of new tools and techniques, helping the community to focus efforts and understand existing trade-offs on Docker topics.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {7},
numpages = {11},
keywords = {software engineering, Docker, mining software repositories, natural language processing},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/1296951.1296970,
author = {Xiao, WenPeng and Chi, ChangYan and Yang, Min},
title = {On-Line Collaborative Software Development via Wiki},
year = {2007},
isbn = {9781595938619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1296951.1296970},
doi = {10.1145/1296951.1296970},
abstract = {Wiki is a collaborative authoring system for collective intelligence which is quickly gaining popularity in content publication. In software development communities, especially open source and global software development teams, wiki is already widely used for documentation and coordination purpose but not programming purpose. This paper presents a new programming approach based on wiki technology by which developers are able to experience "writing wiki page is wring source code". Moreover, developers are able to compile, execute and debug programs in wiki pages too. A prototype of such on-line collaborative software development environment, Galaxy Wiki, is developed in this environment iteratively in order to prove the concept.},
booktitle = {Proceedings of the 2007 International Symposium on Wikis},
pages = {177–183},
numpages = {7},
keywords = {software engineering, wiki, collaborative programming},
location = {Montreal, Quebec, Canada},
series = {WikiSym '07}
}

@inproceedings{10.1145/3551349.3560419,
author = {Reis, Sofia and Abreu, Rui and d'Amorim, Marcelo and Fortunato, Daniel},
title = {Leveraging Practitioners’ Feedback to Improve a Security Linter},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560419},
doi = {10.1145/3551349.3560419},
abstract = {Infrastructure-as-Code (IaC) is a technology that enables the management and distribution of infrastructure through code instead of manual processes. In 2020, Palo Alto Network’s Unit 42 announced the discovery of over 199K vulnerable IaC templates through their “Cloud Threat” Report. This report highlights the importance of tools to prevent vulnerabilities from reaching production. Unfortunately, we observed through a comprehensive study that a security linter for IaC scripts is not reliable yet—high false positive rates. Our approach to tackling this problem was to leverage community expertise to improve the precision of this tool. More precisely, we interviewed professional developers to collect their feedback on the root causes of imprecision of the state-of-the-art security linter for Puppet. From that feedback, we developed a linter adjusting 7 rules of an existing linter ruleset and adding 3 new rules. We conducted a new study with 131 practitioners, which helped us improve the tool’s precision significantly and achieve a final precision of . An important takeaway from this paper is that obtaining professional feedback is fundamental to improving the rules’ precision and extending the rulesets, which is critical for the usefulness and adoption of lightweight tools, such as IaC security linters.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {66},
numpages = {12},
keywords = {Security, Infrastructure, Linter},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/1842752.1842773,
author = {McGregor, John D.},
title = {A Method for Analyzing Software Product Line Ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842773},
doi = {10.1145/1842752.1842773},
abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {73–80},
numpages = {8},
keywords = {software product line, software ecosystem},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1145/3387904.3389293,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and He, Shan and L\"{a}mmel, Ralf and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {Ownership at Large: Open Problems and Challenges in Ownership Management},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389293},
doi = {10.1145/3387904.3389293},
abstract = {Software-intensive organizations rely on large numbers of software assets of different types, e.g., source-code files, tables in the data warehouse, and software configurations. Who is the most suitable owner of a given asset changes over time, e.g., due to reorganization and individual function changes. New forms of automation can help suggest more suitable owners for any given asset at a given point in time. By such efforts on ownership health, accountability of ownership is increased. The problem of finding the most suitable owners for an asset is essentially a program comprehension problem: how do we automatically determine who would be best placed to understand, maintain, evolve (and thereby assume ownership of) a given asset. This paper introduces the Facebook Ownesty system, which uses a combination of ultra large scale data mining and machine learning and has been deployed at Facebook as part of the company's ownership management approach. Ownesty processes many millions of software assets (e.g., source-code files) and it takes into account workflow and organizational aspects. The paper sets out open problems and challenges on ownership for the research community with advances expected from the fields of software engineering, programming languages, and machine learning.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {406–410},
numpages = {5},
keywords = {ownership, machine learning, global software engineering},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3511616.3513098,
author = {Shi, Lei and Krishnan, Shanti and Wen, Sheng},
title = {Study Cybersecurity of Cyber Physical System in the Virtual Environment: A Survey and New Direction},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513098},
doi = {10.1145/3511616.3513098},
abstract = {The Industry 4.0 is defined as connecting the physical system to the cyber world with the help of transformative technologies. As a result, the number of cyber-attacks have grown exponentially. Cybersecurity has been identified as one of main challenges in applying the transformative technologies of Industry 4.0. An industrial system within the context of Industry 4.0 generally, a cyber physical system (CPS), is incredibly complex and involves heterogeneous devices. How to detect the attacks and analyse the vulnerability of the system are actively explored and researched. However, because of the high cost and difficulty to access real industrial CPS, it is difficult for researchers to get opportunities to use real industrial systems in the research. For this reason, a variety of software tools and virtualization methods have been used to create virtual or hybrid systems for researchers. This paper study and review existing cybersecurity research on CPS in the virtual environment. Then authors point out four challenges in the state-of-the-art research based on the findings and discussion. From this, Digital Twin (DT), one of the main enabling technologies in Industry 4.0, has been identified to have the great potential in solving these challenges. DT sheds light on the real fusion of the cyber world and the physical world, and it has the potential to be used as a technology to help people build virtulized system with high-fidelity, hence greatly lower bar on the cybersecurity research of the cyber physical system and create other DT based cybersecurity applications.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {46–55},
numpages = {10},
keywords = {ICS, cybersecurity, Digital Twin, simulation, SCADA, emulation, virtualization, CPS},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.1145/3210459.3216093,
author = {Zhang, Rui and Xie, Genying},
title = {Toward Understanding IoT Developers in Chinese Startups},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3216093},
doi = {10.1145/3210459.3216093},
abstract = {The explosion of networked devices has driven a new computing environment called the Internet of Things (IoT) which enables physical objects and virtual world blending. IoT development processes and developers also vary from ordinary software development. Exploring IoT developer experience is meaningful. In this paper, we present a diary study and in-depth interview results to explore their development features and patterns, and distinguish different developer roles. We uncover 1) typical IoT developer roles' portraits. 2) the most common scenario for the whole IoT developers in this study is joint debugging/test. 3) the primary pain points are awkward tools and difficulty in deployment and maintenance. Finally, based on our research and current literature, we discuss the methods to make IoT developer better adapted to IoT era.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {181–186},
numpages = {6},
keywords = {Development Scenarios, IoT Developers, IoT Developers' Portraits, IoT developers' pain points},
location = {Christchurch, New Zealand},
series = {EASE '18}
}

@inproceedings{10.1145/3183519.3183529,
author = {Sharma, Tushar and Fragkoulis, Marios and Rizou, Stamatia and Bruntink, Magiel and Spinellis, Diomidis},
title = {Smelly Relations: Measuring and Understanding Database Schema Quality},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183529},
doi = {10.1145/3183519.3183529},
abstract = {Context: Databases are an integral element of enterprise applications. Similarly to code, database schemas are also prone to smells - best practice violations.Objective: We aim to explore database schema quality, associated characteristics and their relationships with other software artifacts.Method: We present a catalog of 13 database schema smells and elicit developers' perspective through a survey. We extract embedded sql statements and identify database schema smells by employing the DbDeo tool which we developed. We analyze 2925 production-quality systems (357 industrial and 2568 well-engineered open-source projects) and empirically study quality characteristics of their database schemas. In total, we analyze 629 million lines of code containing more than 393 thousand sql statements.Results: We find that the index abuse smell occurs most frequently in database code, that the use of an orm framework doesn't immune the application from database smells, and that some database smells, such as adjacency list, are more prone to occur in industrial projects compared to open-source projects. Our co-occurrence analysis shows that whenever the clone table smell in industrial projects and the values in attribute definition smell in open-source projects get spotted, it is very likely to find other database smells in the project.Conclusion: The awareness and knowledge of database smells are crucial for developing high-quality software systems and can be enhanced by the adoption of better tools helping developers to identify database smells early.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {55–64},
numpages = {10},
keywords = {code smells, software quality, database schema smells, technical debt, antipatterns, software maintenance},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1109/ICPC.2019.00014,
author = {Fakhoury, Sarah and Roy, Devjeet and Hassan, Sk. Adnan and Arnaoudova, Venera},
title = {Improving Source Code Readability: Theory and Practice},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00014},
doi = {10.1109/ICPC.2019.00014},
abstract = {There are several widely accepted metrics to measure code quality that are currently being used in both research and practice to detect code smells and to find opportunities for code improvement. Although these metrics have been proposed as a proxy of code quality, recent research suggests that more often than not, state-of-the-art code quality metrics do not successfully capture quality improvements in the source code as perceived by developers. More specifically, results show that there may be inconsistencies between, on the one hand, the results from metrics for cohesion, coupling, complexity, and readability, and, on the other hand, the interpretation of these metrics in practice. As code improvement tools rely on these metrics, there is a clear need to identify and resolve the aforementioned inconsistencies. This will allow for the creation of tools that are more aligned with developers' perception of quality, and can more effectively help source code improvement efforts.In this study, we investigate 548 instances of source code readability improvements, as explicitly stated by internal developers in practice, from 63 engineered software projects. We show that current readability models fail to capture readability improvements. We also show that tools to calculate additional metrics, to detect refactorings, and to detect style problems are able to capture characteristics that are specific to readability changes and thus should be considered by future readability models.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {2–12},
numpages = {11},
keywords = {readability, developers'perception, code quality metrics},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/2246056.2246062,
author = {Dayarathna, Miyuru and Houngkaew, Charuwat and Suzumura, Toyotaro},
title = {Introducing ScaleGraph: An X10 Library for Billion Scale Graph Analytics},
year = {2012},
isbn = {9781450314916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2246056.2246062},
doi = {10.1145/2246056.2246062},
abstract = {Highly Productive Computing Systems (HPCS) and PGAS languages are considered as important ways in achieving the exascale computational capabilities. Most of the current large graph processing applications are custom developed using non-HPCS/PGAS techniques such as MPI, MapReduce. This paper introduces Scale-Graph, an X10 library targeting billion scale graph analysis scenarios. Compared to non-PGAS alternatives, ScaleGraph defines concrete, simple abstractions for representing massive graphs. We have designed ScaleGraph from ground up considering graph structural property analysis, graph clustering and community detection. We describe the design of the library and provide some initial performance evaluation results of the library using a twitter graph with 1.47 billion edges.},
booktitle = {Proceedings of the 2012 ACM SIGPLAN X10 Workshop},
articleno = {6},
numpages = {9},
keywords = {X10, distributed computing, HPCS, PGAS, reusable libraries, large graph analytics, programming techniques},
location = {Beijing, China},
series = {X10 '12}
}

@inproceedings{10.1145/3196558.3196567,
author = {Ernst, Neil A. and Kazman, Rick and Bianco, Philip},
title = {Towards Rapid Composition with Confidence in Robotics Software},
year = {2018},
isbn = {9781450357609},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196558.3196567},
doi = {10.1145/3196558.3196567},
abstract = {Robotics software is booming thanks in part to a rich and productive ecosystem around the Robot Operating System. We introduce a military effort to leverage the ROS ecosystem and reduce the challenges in building military robots, called ROS-M. We outline some of the work we have done on the ROS-M initiative, and explain our future directions in analyzing ROS code to balance between rapid adoption and confidence in the component.},
booktitle = {Proceedings of the 1st International Workshop on Robotics Software Engineering},
pages = {44–47},
numpages = {4},
keywords = {quality attribute requirements, ROS, robotics software},
location = {Gothenburg, Sweden},
series = {RoSE '18}
}

@inproceedings{10.1109/ICSE43902.2021.00021,
author = {Jia, Zhouyang and Li, Shanshan and Yu, Tingting and Zeng, Chen and Xu, Erci and Liu, Xiaodong and Wang, Ji and Liao, Xiangke},
title = {DepOwl: Detecting Dependency Bugs to Prevent Compatibility Failures},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00021},
doi = {10.1109/ICSE43902.2021.00021},
abstract = {Applications depend on libraries to avoid reinventing the wheel. Libraries may have incompatible changes during evolving. As a result, applications will suffer from compatibility failures. There has been much research on addressing detecting incompatible changes in libraries, or helping applications co-evolve with the libraries. The existing solution helps the latest application version work well against the latest library version as an afterthought. However, end users have already been suffering from the failures and have to wait for new versions. In this paper, we propose DepOwl, a practical tool helping users prevent compatibility failures. The key idea is to avoid using incompatible versions from the very beginning. We evaluated DepOwl on 38 known compatibility failures from StackOverflow, and DepOwl can prevent 35 of them. We also evaluated DepOwl using the software repository shipped with Ubuntu-19.10. DepOwl detected 77 unknown dependency bugs, which may lead to compatibility failures.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {86–98},
numpages = {13},
keywords = {Software dependency, Library incompatibility, Compatibility failure},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3180155.3180209,
author = {Trockman, Asher and Zhou, Shurui and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {Adding Sparkle to Social Coding: An Empirical Study of Repository Badges in the Npm Ecosystem},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180209},
doi = {10.1145/3180155.3180209},
abstract = {In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {511–522},
numpages = {12},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3417990.3421402,
author = {Nikoo, Mahdi Saeedi and Babur, \"{O}nder and van den Brand, Mark},
title = {A Survey on Service Composition Languages},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421402},
doi = {10.1145/3417990.3421402},
abstract = {In recent years, service-oriented architecture (SOA) has been adopted by industry in developing enterprise systems. Web service composition has been one of the challenging topics in SOA. Numerous approaches have been proposed to tackle this problem. In industry big companies such as Amazon, Netflix, and Uber have developed their own web service composition languages and tools. In academia, on the other hand, there have also been attempts to resolve some of the complexities in web service composition. In this survey we identify and evaluate current prominent service composition languages, and discuss our key findings. After a scan of dozens of service composition systems, 14 systems that used a language-based approach were included in this study. We believe that our findings will help people from industry and academia to learn about some of the major active composition languages and get an overall idea about their commonalities and differences.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {44},
numpages = {5},
keywords = {service composition, orchestration, choreography, service oriented architecture, domain-specific language},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3014812.3014818,
author = {Botangen, Khavee Agustus and Yu, Jian and Sheng, Michael},
title = {Towards Measuring the Adaptability of an AO4BPEL Process},
year = {2017},
isbn = {9781450347686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014812.3014818},
doi = {10.1145/3014812.3014818},
abstract = {Adaptability is a significant property which enables software systems to continuously provide the required functionality and achieve optimal performance. The recognised importance of adaptability makes its evaluation an essential task. However, the various adaptability dimensions and implementation mechanisms make adaptive strategies difficult to evaluate. In service oriented computing, several frameworks that extend the WS-BPEL, the de facto standard in composing distributed business applications, focus on enabling the adaptability of processes. We aim to evaluate the adaptability of processes specified from the extended-BPEL frameworks. In this paper, we propose metrics to measure the adaptability of an AO4BPEL process. The metrics is grounded in the perspective that a process is capable of dynamically adapting to changes in business requirements. This opens potential future work on evaluating the adaptability of processes specified from various aspect-oriented WS-BPEL frameworks.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {6},
numpages = {7},
keywords = {WS-BPEL, adaptability measurement, AO4BPEL},
location = {Geelong, Australia},
series = {ACSW '17}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00046,
author = {Keshani, Mehdi},
title = {Scalable Call Graph Constructor for Maven},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00046},
doi = {10.1109/ICSE-Companion52605.2021.00046},
abstract = {As a rich source of data, Call Graphs are used for various applications including security vulnerability detection. Despite multiple studies showing that Call Graphs can drastically improve the accuracy of analysis, existing ecosystem-scale tools like Dependabot do not use Call Graphs and work at the package-level. Using Call Graphs in ecosystem use cases is not practical because of the scalability problems that Call Graph generators have. Call Graph generation is usually considered to be a "full program analysis" resulting in large Call Graphs and expensive computation. To make an analysis applicable to ecosystem scale, this pragmatic approach does not work, because the number of possible combinations of how a particular artifact can be combined in a full program explodes. Therefore, it is necessary to make the analysis incremental. There are existing studies on different types of incremental program analysis. However, none of them focuses on Call Graph generation for an entire ecosystem. In this paper, we propose an incremental implementation of the CHA algorithm that can generate Call Graphs on-demand, by stitching together partial Call Graphs that have been extracted for libraries before. Our preliminary evaluation results show that the proposed approach scales well and outperforms the most scalable existing framework called OPAL.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {99–101},
numpages = {3},
keywords = {logic and verification, program analysis, theory of computation},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@proceedings{10.1145/3571473,
title = {SBQS '22: Proceedings of the XXI Brazilian Symposium on Software Quality},
year = {2022},
isbn = {9781450399999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Curitiba, Brazil}
}

@article{10.1145/3559762,
author = {Shah, Parth and Shenoy, Ranjal Gautham and Srinivasan, Vaidyanathan and Bose, Pradip and Buyuktosunoglu, Alper},
title = {TokenSmart: Distributed, Scalable Power Management in the Many-Core Era},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3559762},
doi = {10.1145/3559762},
abstract = {Centralized power management control systems are hitting a scalability limit. In particular, enforcing a power cap in a many-core system in a performance-friendly manner is quite challenging. Today’s on-chip controller reduces the clock speed of compute domains in response to local or global power limit alerts. However, this is opaque to the operating system (OS), which continues to request higher clock frequency based on the workload characteristics acting against the centralized on-chip controller. To address these issues, we introduce TokenSmart, which implements a set of scalable distributed frequency control heuristics within the OS, using a novel token-based mechanism. The number of system-allocated power tokens represents the maximum allowable power consumption; and the OS governor orchestrates a token-passing (or sharing) algorithm between the compute engines. Token allocation count increase (decrease) corresponds to a increase (decrease) of clock frequency. The compute units are connected in a ring-topology allowing minimal meta-data to be passed along with the token value for regulating power budget. We explore different heuristics to assign tokens smartly across the units. This results in efficient power regulation and sustenance of turbo frequencies over a longer duration. Our proposed methodology can be implemented in hardware with multiple on-chip controllers, or in software where each set of cores acts as a compute unit. The methodology is currently implemented within the Linux kernel of a real IBM POWER9 many-core system and experimentally verified on different real world workloads such as Redis, Cassandra, PostgreSQL along with a micro-benchmark such as rt-app. Our experiments indicate the increase in throughput for all the workloads along with the benefit of power savings. For instance, results show a considerable boost of about 4% in throughput of both the PostgreSQL and Redis benchmark with a substantial savings in power consumption (18% and 37%, respectively). If the approach is implemented in hardware, then our experimental analysis speculates the throughput to increase up to 14% in PostgreSQL benchmark.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {4},
numpages = {26},
keywords = {scalable, decentralized, Power management}
}

@inproceedings{10.1145/2884781.2884840,
author = {Kononenko, Oleksii and Baysal, Olga and Godfrey, Michael W.},
title = {Code Review Quality: How Developers See It},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884840},
doi = {10.1145/2884781.2884840},
abstract = {In a large, long-lived project, an effective code review process is key to ensuring the long-term quality of the code base. In this work, we study code review practices of a large, open source project, and we investigate how the developers themselves perceive code review quality. We present a qualitative study that summarizes the results from a survey of 88 Mozilla core developers. The results provide developer insights into how they define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. We found that the review quality is primarily associated with the thoroughness of the feedback, the reviewer's familiarity with the code, and the perceived quality of the code itself. Also, we found that while different factors are perceived to contribute to the review quality, reviewers often find it difficult to keep their technical skills up-to-date, manage personal priorities, and mitigate context switching.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {1028–1038},
numpages = {11},
keywords = {developer perception, review quality, survey, code review},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3383583.3398539,
author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
title = {Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398539},
doi = {10.1145/3383583.3398539},
abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {235–242},
numpages = {8},
keywords = {space traffic management, data model, big data, knowledge system, data integration, data curation, graph database},
location = {Virtual Event, China},
series = {JCDL '20}
}

@article{10.1145/2488364.2492433,
author = {Al Bahra, Samy},
title = {Nonblocking Algorithms and Scalable Multicore Programming: Exploring Some Alternatives to Lock-Based Synchronization},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/2488364.2492433},
doi = {10.1145/2488364.2492433},
abstract = {Real-world systems with complicated quality-of-service guarantees may require a delicate balance between throughput and latency to meet operating requirements in a cost-efficient manner. The increasing availability and decreasing cost of commodity multicore and many-core systems make concurrency and parallelism increasingly necessary for meeting demanding performance requirements. Unfortunately, the design and implementation of correct, efficient, and scalable concurrent software is often a daunting task.},
journal = {Queue},
month = {may},
pages = {40–64},
numpages = {25}
}

@inproceedings{10.1145/3275219.3275230,
author = {Ren, Zhongshan and Wang, Wei and Wu, Guoquan and Gao, Chushu and Chen, Wei and Wei, Jun and Huang, Tao},
title = {Migrating Web Applications from Monolithic Structure to Microservices Architecture},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275230},
doi = {10.1145/3275219.3275230},
abstract = {In the traditional software development and deployment, the centralized monolithic is always adopted, as the modules are tightly coupled, which caused many inconvenience in software DevOps. The modules with bottlenecks in monolithic application cannot be extend separately as the application is an integral part, and different module cannot use different technology stack. To prolong the lifecycle of the monolithic applications, its need to migrated it to microservice architecture. Due to the complex logic and large number of third party framework libraries depended, get an accurate comprehensive of the application characteristics is challenging. The existing research mostly based on the static characteristics, lack of consideration of the runtime dynamic characteristics, and the completeness and accuracy of the static analysis is inadequate. To resolve above problems, we combined static and dynamic analysis to get static structure and runtime behavior characteristics of monolithic application. We employed the coupling among functions to evaluate the degree of dependence, and through function clustering to achieve the migration of legacy monolithic applications and its data to microservices architecture. Through the empirical study of migrate the typical legacy project to microservices, it is proved that we proposed method can offer precise guidance and assistance in the migration procedure. Experiments show that the method has high accuracy and low performance cost.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {7},
numpages = {10},
keywords = {monolithic application, application migration, microservices, function clustering},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/2912160.2912179,
author = {Hu, Yanhua and Bai, Xianyang and Sun, Shuyang},
title = {Readiness Assessment of Open Government Data Programs: A Case of Shenzhen},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912179},
doi = {10.1145/2912160.2912179},
abstract = {More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {97–103},
numpages = {7},
keywords = {Readiness assessment, Open government data, Open data},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/3474624.3476011,
author = {Pereira, Jherson Haryson A. and Souza, Alberto Luiz Oliveira Tavares de and Pinto, Victor Hugo Santiago C.},
title = {Cognitive Load Analyzer: A Support Tool for Cognitive-Driven Development},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476011},
doi = {10.1145/3474624.3476011},
abstract = {Software modularity refers to the decomposition of complex software to be manageable for the purpose of implementation and maintenance. Most methodologies and practices adopted in the industry follow this principle, often with the benefit of flexibility and variety in use. This is also a recognition that human work can be improved by focus on a limited set of data. However, code more complex than needed continues being produced resulting in cognitive overload for developers. Cognitive-Driven Development (CDD) is an inspiration from the Cognitive Load Theory for software development with the goal of reducing the split-attention effect and the problem space through a feasible cognitive complexity constraint. Implementation units can be kept under this limit even with the continuous expansion of software scale. Experimental studies were carried out and their results suggested that CDD is a promising method when guiding the development focusing on understanding and achieving high-quality code with respect to the quality metrics. In this paper, we present a tool called “Cognitive Load Analyzer” to support the CDD, a plugin for IntelliJ IDEA and Java language. This tool can be useful to support the adoption of the CDD aiming to overcome the to slice the code and help developers to reduce code complexity.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {468–473},
numpages = {6},
keywords = {Software complexity, Cognitive load analyzer, Cognitive-driven development},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2897022.2897028,
author = {McAllister, Andrew J. and O'Hara, Steven A.},
title = {Toward Effective Management of Large-Scale Software},
year = {2016},
isbn = {9781450341707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897022.2897028},
doi = {10.1145/2897022.2897028},
abstract = {This paper outlines challenges the authors have faced over decades of industrial experience with large-scale software analysis and maintenance projects (especially legacy modernization) for multiple organizations where millions of lines of source code are involved. Such projects require large teams cooperating on parsing, analyzing, and manipulating source code. In this context the use of traditional parsing techniques based on context-free grammars has proven problematic. We present the Programmar API, a recently developed parsing approach designed to overcome these problems. This paper describes the industrial experiences that led to our R&amp;D activities. The Programmar approach is designed to enable large teams to effectively extract complete, accurate, up-to-date information from application source code, and to provide this information as the basis for a wide variety of software management tools and activities. We present a framework that relates various types of such activities, and describe a vision for how the Programmar approach can provide significant benefits for the software industry in the future via an open-source distribution approach. This paper is intended to serve as an example of how challenges faced by industry can stimulate research, and as a catalyst for discussion of industry needs and potential future research directions.},
booktitle = {Proceedings of the 3rd International Workshop on Software Engineering Research and Industrial Practice},
pages = {16–22},
numpages = {7},
keywords = {legacy modernization, reverse engineering, source code parsing, post-production software management},
location = {Austin, Texas},
series = {SER&amp;IP '16}
}

@inproceedings{10.1145/2723372.2737784,
author = {Bailis, Peter and Fekete, Alan and Franklin, Michael J. and Ghodsi, Ali and Hellerstein, Joseph M. and Stoica, Ion},
title = {Feral Concurrency Control: An Empirical Investigation of Modern Application Integrity},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2737784},
doi = {10.1145/2723372.2737784},
abstract = {The rise of data-intensive "Web 2.0" Internet services has led to a range of popular new programming frameworks that collectively embody the latest incarnation of the vision of Object-Relational Mapping (ORM) systems, albeit at unprecedented scale. In this work, we empirically investigate modern ORM-backed applications' use and disuse of database concurrency control mechanisms. Specifically, we focus our study on the common use of feral, or application-level, mechanisms for maintaining database integrity, which, across a range of ORM systems, often take the form of declarative correctness criteria, or invariants. We quantitatively analyze the use of these mechanisms in a range of open source applications written using the Ruby on Rails ORM and find that feral invariants are the most popular means of ensuring integrity (and, by usage, are over 37 times more popular than transactions). We evaluate which of these feral invariants actually ensure integrity (by usage, up to 86.9%) and which---due to concurrency errors and lack of database support---may lead to data corruption (the remainder), which we experimentally quantify. In light of these findings, we present recommendations for database system designers for better supporting these modern ORM programming patterns, thus eliminating their adverse effects on application integrity.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1327–1342},
numpages = {16},
keywords = {invariants, concurrency control, application integrity, ruby on rails, orms, impedance mismatch},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/1083258.1083271,
author = {Shaikh, Maha and Cornford, Tony},
title = {Learning/Organizing in Linux: A Study of the 'Spaces in Between'},
year = {2005},
isbn = {1595931279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083258.1083271},
doi = {10.1145/1083258.1083271},
abstract = {We assume that open source communities or collectives are somewhat organized. we also assume that such collectives are capable of learning, and indeed do learn. However, it is far more difficult to say exactly where, when and how such learning occurs, or resulting (re-)organizing happens. Drawing on Clegg et al's [1] concept of learning and becoming this paper seeks to show, through a case study of the Linux discussion around version control software, how learning and organizing occur. The paper discusses the Linux community's engagement with BitKeeper and explains aspects of its adoption. In this we address version control software as not merely a collaborative, organizing vehicle but as a part of a generative duality.},
booktitle = {Proceedings of the Fifth Workshop on Open Source Software Engineering},
pages = {1–5},
numpages = {5},
keywords = {organizing, linux, version control software, becoming, learning},
location = {St. Louis, Missouri},
series = {5-WOSSE}
}

@inproceedings{10.1145/3605573.3605613,
author = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
title = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605613},
doi = {10.1145/3605573.3605613},
abstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {766–775},
numpages = {10},
keywords = {neural networks, gaze detection, text tagging, datasets},
location = {Salt Lake City, UT, USA},
series = {ICPP '23}
}

@inproceedings{10.5555/2820518.2820551,
author = {Camilo, Felivel and Meneely, Andrew and Nagappan, Meiyappan},
title = {Do Bugs Foreshadow Vulnerabilities? A Study of the Chromium Project},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. stability, compatibility, etc.) are associated with post-release vulnerabilities. While we found statistically significant correlations between pre-release bugs and post-release vulnerabilities, we also found the association to be weak. Number of features, SLOC, and number of pre-release security bugs are, in general, more closely associated with post-release vulnerabilities than any of our non-security bug categories. In a separate analysis, we found that the files with highest defect density did not intersect with the files of highest vulnerability density. These results indicate that bugs and vulnerabilities are empirically dissimilar groups, warranting the need for more research targeting vulnerabilities specifically.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {269–279},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/1520340.1520433,
author = {Green, Collin and Tollinger, Irene and Ratterman, Christian and Pyrzak, Guy and Eiser, Alex and Castro, Lanie and Vera, Alonso},
title = {Leveraging Open-Source Software in the Design and Development Process},
year = {2009},
isbn = {9781605582474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1520340.1520433},
doi = {10.1145/1520340.1520433},
abstract = {This paper presents a case study of the NASA Ames Research Center HCI Group's design and development of a problem reporting system for NASA's next generation vehicle (to replace the shuttle) based on the adaptation of an open source software application. We focus on the criteria used for selecting a specific system (Bugzilla) and discuss the outcomes of our project including eventual extensibility and maintainability. Finally, we address whether our experience may generalize considering where Bugzilla lies in the larger quantitative picture of current open source software projects.},
booktitle = {CHI '09 Extended Abstracts on Human Factors in Computing Systems},
pages = {3061–3074},
numpages = {14},
keywords = {collaboration, open-source software, software development, benefits analysis},
location = {Boston, MA, USA},
series = {CHI EA '09}
}

@inproceedings{10.1145/2287016.2287025,
author = {Deligiannis, Pantazis and Loidl, Hans-Wolfgang and Kouidi, Evangelia},
title = {Improving the Diagnosis of Mild Hypertrophic Cardiomyopathy with MapReduce},
year = {2012},
isbn = {9781450313438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2287016.2287025},
doi = {10.1145/2287016.2287025},
abstract = {Hypertrophic Cardiomyopathy (HCM), an inherited heart disease, is the most common cause of sudden cardiac death in young athletes. Successful diagnosis of mild HCM presents a major medical challenge, especially in athletes with exercise-induced hypertrophy that overlaps with HCM. This is due to a wide spectrum of non-specific clinical parameters and their complex dependencies. Recently, medical researchers proposed multidisciplinary strategies, defining differential diagnostic scoring algorithms, with the goal of identifying which parameters correlate with HCM in order to achieve faster and more accurate diagnosis. These algorithms require extensive testing against large medical datasets in order to identify potential correlations, and assess the overall algorithmic quality and diagnostic accuracy.We present a prototype data-parallel algorithm for improving the diagnosis of mild HCM, by refining the set of parameters contributing to the main diagnostic function. To this end, we employ a rule-based, machine-learning approach and develop an iterative MapReduce application for applying the diagnostic function on large data-sets. The core component of the algorithm, including the diagnostic function, has been implemented in Java, Pig and Hive in order to identify potential productivity gains by using a high-level MapReduce language specifically for medical applications. Finally, we assess the algorithmic performance on up to 64 cores of our Hadoop (version 0.20.1) enabled Beowulf cluster, managing to achieve near-linear speedups while reducing the overall runtime from over 9 hours to a couple of minutes for a realistic dataset of 10,000 medical records.},
booktitle = {Proceedings of Third International Workshop on MapReduce and Its Applications Date},
pages = {41–48},
numpages = {8},
keywords = {Hadoop, hypertrophic cardiomyopathy, MapReduce, machine learning, medical diagnosis},
location = {Delft, The Netherlands},
series = {MapReduce '12}
}

@article{10.1145/317665.317670,
author = {Press, Larry},
title = {Personal Computing: The Post-PC Era},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/317665.317670},
doi = {10.1145/317665.317670},
journal = {Commun. ACM},
month = {oct},
pages = {21–24},
numpages = {4}
}

@inproceedings{10.1109/MSR.2019.00061,
author = {Dietrich, Jens and Pearce, David J. and Stringer, Jacob and Tahir, Amjed and Blincoe, Kelly},
title = {Dependency Versioning in the Wild},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00061},
doi = {10.1109/MSR.2019.00061},
abstract = {Many modern software systems are built on top of existing packages (modules, components, libraries). The increasing number and complexity of dependencies has given rise to automated dependency management where package managers resolve symbolic dependencies against a central repository. When declaring dependencies, developers face various choices, such as whether or not to declare a fixed version or a range of versions. The former results in runtime behaviour that is easier to predict, whilst the latter enables flexibility in resolution that can, for example, prevent different versions of the same package being included and facilitates the automated deployment of bug fixes.We study the choices developers make across 17 different package managers, investigating over 70 million dependencies. This is complemented by a survey of 170 developers. We find that many package managers support --- and the respective community adapts --- flexible versioning practices. This does not always work: developers struggle to find the sweet spot between the predictability of fixed version dependencies, and the agility of flexible ones, and depending on their experience, adjust practices. We see some uptake of semantic versioning in some package managers, supported by tools. However, there is no evidence that projects switch to semantic versioning on a large scale.The results of this study can guide further research into better practices for automated dependency management, and aid the adaptation of semantic versioning.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {349–359},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3546932.3547002,
author = {Schulze, Sandro and Kr\"{u}ger, Jacob and W\"{u}nsche, Johannes},
title = {Towards Developer Support for Merging Forked Test Cases},
year = {2022},
isbn = {9781450394437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546932.3547002},
doi = {10.1145/3546932.3547002},
abstract = {Developers rely on branching and forking mechanisms of modern versioning systems to evolve and maintain their software systems. As a result, systems often exist in the form of various short-living or even long-living (i.e., clone &amp; own development) variants. Such variants may have to be merged with the main system or other variants, for instance, to propagate features or bug fixes. Within such merging processes, test cases are highly interesting, since they allow to improve the test coverage and hopefully the reliability of the system (e.g., by merging missing tests and bug fixes in test code). However, as all source code, test cases may evolve independently between two or more variants, which makes it non-trivial to decide what changes of the test cases are relevant for the merging. For instance, some test cases in one variant may be irrelevant in another variant (e.g., because the feature shall not be propagated) or may subsume existing test cases. In this paper, we propose a technique that allows for a fine-grained comparison of test cases to support developers in deciding whether and how to merge these. Precisely, inspired by code-clone detection, we use abstract syntax trees to decide on the relations between test cases of different variants. We evaluate the applicability of our technique qualitatively on five open-source systems written in Java (e.g., JUnit 5, Guava). Our insights into the merge potential of 50 pull requests with test cases from these systems indicate that our technique can support the comprehension of differences in variants' test cases, and also highlight future research opportunities.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume A},
pages = {131–141},
numpages = {11},
keywords = {merging, feature forks, variant-rich systems, test cases},
location = {Graz, Austria},
series = {SPLC '22}
}

@article{10.1145/2557963.2566628,
author = {Madhavapeddy, Anil and Scott, David J.},
title = {Unikernels: Rise of the Virtual Library Operating System: What If All the Software Layers in a Virtual Appliance Were Compiled within the Same Safe, High-Level Language Framework?},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {11},
issn = {1542-7730},
url = {https://doi.org/10.1145/2557963.2566628},
doi = {10.1145/2557963.2566628},
abstract = {Cloud computing has been pioneering the business of renting computing resources in large data centers to multiple (and possibly competing) tenants. The basic enabling technology for the cloud is operating-system virtualization such as Xen1 or VMWare, which allows customers to multiplex VMs (virtual machines) on a shared cluster of physical machines. Each VM presents as a self-contained computer, booting a standard operating-system kernel and running unmodified applications just as if it were executing on a physical machine.},
journal = {Queue},
month = {dec},
pages = {30–44},
numpages = {15}
}

@inproceedings{10.1145/3442167.3442178,
author = {Ashenden, Debi and Ollis, Gail},
title = {Putting the Sec in DevSecOps: Using Social Practice Theory to Improve Secure Software Development},
year = {2021},
isbn = {9781450389952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442167.3442178},
doi = {10.1145/3442167.3442178},
abstract = {Practices such as open source development, agile, DevOps and DevSecOps mean that cyber security professionals need to find ways to blend cyber security with software development practices. One way of approaching this is as an awareness, education and training problem and many organisations are focusing on training software developers in cyber security. In this paper, however, we make the case for looking more broadly at group rather than individual behaviours, by examining the social practices of software developers. Changing software development practices are shaping the lived experience of software developers and we argue that understanding these practices will enable us to improve secure software development. We use social practice theory as a framework to develop recommendations for aligning and blending cyber security and software development. To achieve this, we carried out a rapid review of research on software development practices and supplemented this with data from ten key informant interviews to ascertain what we need to consider when developing an intervention for secure software development. Finally, we outline how our research could be used to develop a workshop that would facilitate the co-creation of security practices for software development. We conclude with suggestions for future research.},
booktitle = {Proceedings of the New Security Paradigms Workshop 2020},
pages = {34–44},
numpages = {11},
keywords = {DevSecOps, Secure Software Development, Cyber Security, Social Practice Theory},
location = {Online, USA},
series = {NSPW '20}
}

@inproceedings{10.1145/2945292.2945303,
author = {Lemme, Stefan and Sutter, Jan and Schlinkmann, Christian and Slusallek, Philipp},
title = {The Basic Building Blocks of Declarative 3D on the Web},
year = {2016},
isbn = {9781450344289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945292.2945303},
doi = {10.1145/2945292.2945303},
abstract = {WebGL enabled real-time 3D graphics on the Web. With the objective to integrate 3D graphics into the rest of the Web technology stack, and to make it easier for Web developers to develop interactive 3D graphics, Declarative 3D approaches were developed: X3DOM and XML3D. While the former focuses on backward-compatibility to X3D and a large set of convenience elements, the latter attempts to define a minimal set of flexible elements as an extension to HTML5.It has now been more than 6 years since Declarative 3D was first proposed for the Web. However, despite their different philosophies neither X3DOM nor XML3D has yet been able to achieve the same momentum and adoption rate as imperative frameworks like three.js. In the meantime, the underlying Web technology stack has made significant advances.In this paper we revisit both approaches in light of new Web technologies, such as Web Components, to define a small set of core elements that can provide the convenience of X3DOM while remaining as flexible and customizable as XML3D. Further, we present a strategy for building upon these core elements to enable user-defined elements, with the ability to cover domain-specific needs in Declarative 3D. Lastly, we show how these concepts can be used to simplify existing approaches (i.e. X3DOM and XML3D) and provide the basic building blocks of Declarative 3D on the Web.},
booktitle = {Proceedings of the 21st International Conference on Web3D Technology},
pages = {17–25},
numpages = {9},
keywords = {web components, X3DOM, XML3D, HTML, Dec3D},
location = {Anaheim, California},
series = {Web3D '16}
}

@inproceedings{10.1145/3389189.3397986,
author = {Plattner, Johanna and Oberrauner, Elena and Str\"{o}ckl, Daniela Elisabeth and Oberzaucher, Johannes},
title = {Using IoT Middleware Solutions in Interdisciplinary Research Projects in the Context of AAL},
year = {2020},
isbn = {9781450377737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3389189.3397986},
doi = {10.1145/3389189.3397986},
abstract = {Research projects in the field of Active and Assisted Living (AAL) are dedicated to enhance quality of life and independent living of the elderly by providing assistive technical and non-technical solutions. With the progress of the Age of Internet of Things (IoT) and digitalization, the implemented applications are also often focused on the integration of IoT technologies. As the field of AAL is characterized by a high amount of interdisciplinarity, there arises the need of integrating various proprietary solutions to one comprehensive application. This can be achieved by using dedicated IoT middleware solutions which provide the possibility to integrate information from different systems, users and devices into one comprehensive application. The aim of this paper is to show an overview on the process of interdisciplinary research in the field of AAL and to demonstrate the arising requirements for IoT middleware solutions in the given context. Furthermore, selected middleware frameworks and platforms are presented and the implementation of one selected solution is described for a given use case from the Smart VitAALity project.},
booktitle = {Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {51},
numpages = {6},
keywords = {AAL, IoT requirements, FIWARE, interdisciplinary research, IoT middleware},
location = {Corfu, Greece},
series = {PETRA '20}
}

@inproceedings{10.1145/3379597.3387454,
author = {Borrelli, Antonio and Nardone, Vittoria and Di Lucca, Giuseppe A. and Canfora, Gerardo and Di Penta, Massimiliano},
title = {Detecting Video Game-Specific Bad Smells in Unity Projects},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387454},
doi = {10.1145/3379597.3387454},
abstract = {The growth of the video game market, the large proportion of games targeting mobile devices or streaming services, and the increasing complexity of video games trigger the availability of video game-specific tools to assess performance and maintainability problems. This paper proposes UnityLinter, a static analysis tool that supports Unity video game developers to detect seven types of bad smells we have identified as relevant in video game development. Such smell types pertain to performance, maintainability and incorrect behavior problems. After having defined the smells by analyzing the existing literature and discussion forums, we have assessed their relevance with a survey involving 68 participants. Then, we have analyzed the occurrence of the studied smells in 100 open-source Unity projects, and also assessed UnityLinter's accuracy. Results of our empirical investigation indicate that developers well-received performance- and behavior-related issues, while some maintainability issues are more controversial. UnityLinter is, in general, accurate enough in detecting smells (86%-100% precision and 50%-100% recall), and our study shows that the studied smell types occur in 39%-97% of the analyzed projects.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {198–208},
numpages = {11},
keywords = {Linters, Bad Smells, Static Analysis, Video Game Development},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/1636642.1636652,
author = {Abadi, Aharon and Ettinger, Ran and Feldman, Yishai A.},
title = {Re-Approaching the Refactoring Rubicon},
year = {2008},
isbn = {9781605583396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1636642.1636652},
doi = {10.1145/1636642.1636652},
abstract = {Fowler saw the availability of automated support for the Extract Method refactoring in modern IDEs as an indication for the crossing of the refactoring Rubicon. In spite of the advances in refactoring technology, it seems that this Rubicon has not yet been crossed, and refactoring support in modern IDEs leaves a lot to be desired.We performed a case study in which we converted a Java servlet to use the model-view-controller pattern, using as much automated support as available. We found that while the whole conversion could be described as a series of refactorings, most of these were inadequately supported by the IDE, and some were not supported at all.Based on these findings, we outline the requirements from a refactoring framework that will support much more of the conversion process, and will also enable the composition of small refactorings into larger ones.},
booktitle = {Proceedings of the 2nd Workshop on Refactoring Tools},
articleno = {10},
numpages = {4},
location = {Nashville, Tennessee},
series = {WRT '08}
}

@article{10.1145/1578012.1578013,
author = {Baker, Ken A.},
title = {Learning Theory and the Re-Education of Older Software Engineers},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/1578012.1578013},
doi = {10.1145/1578012.1578013},
abstract = {This paper provides an overview of behaviorist, cognitive, and social learning theories, as well as memory and motivation when applied to a paradigm of older software engineers and re-education. While not comprehensive, it does give some insight into the applicability of the different learning theories when applied to the problem of re-educating older software engineers. We find that cognitivist theories, memory and motivation predominantly hold as pedagogical approaches to effectively address this audience.},
journal = {SIGITERes. IT},
month = {jul},
pages = {2–10},
numpages = {9},
keywords = {cognitive load theory, cognitive, memory and motivation, social learning, behaviorist, learning theory, paradigm shift}
}

@inproceedings{10.1145/1595800.1595806,
author = {Treinen, Ralf and Zacchiroli, Stefano},
title = {Expressing Advanced User Preferences in Component Installation},
year = {2009},
isbn = {9781605586779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595800.1595806},
doi = {10.1145/1595800.1595806},
abstract = {State of the art component-based software collections - such as FOSS distributions - are made of up to dozens of thousands components, with complex inter-dependencies and conflicts. Given a particular installation of such a system, each request to alter the set of installed components has potentially (too) many satisfying answers.We present an architecture that allows to express advanced user preferences about package selection in FOSS distributions. The architecture is composed by a distribution-independent format for describing available and installed packages called CUDF (Common Upgradeability Description Format), and a foundational language called MooML to specify optimization criteria. We present the syntax and semantics of CUDF and MooML, and discuss the partial evaluation mechanism of MooML which allows to gain efficiency in package dependency solvers.},
booktitle = {Proceedings of the 1st International Workshop on Open Component Ecosystems},
pages = {31–40},
numpages = {10},
keywords = {selection, upgrade, foss, packages, preferences},
location = {Amsterdam, The Netherlands},
series = {IWOCE '09}
}

@inproceedings{10.1145/1518701.1518852,
author = {Bach, Paula M. and DeLine, Robert and Carroll, John M.},
title = {Designers Wanted: Participation and the User Experience in Open Source Software Development},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518852},
doi = {10.1145/1518701.1518852},
abstract = {We present design concepts and related mockups that support the user experience for projects hosted on CodePlex, an open source project hosting website. Rationale for the design concepts is grounded in the open source literature and a thirteen-week study with the CodePlex team. We propose that fostering ways to build trust, providing opportunities for merit, supporting crossover of work activities, and supporting user experience (UX) best practices in CodePlex will help dismantle the social and technological barriers for UX and encourage UX designer participation. We address UX designer motivation as a challenge for participation and conclude that the mockups presented are a first step in furthering the user experience in open source software development.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {985–994},
numpages = {10},
keywords = {user experience, codeplex, communities of practice, design, open source software, software development},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{10.1145/3544548.3581060,
author = {Tahaei, Mohammad and Abu-Salma, Ruba and Rashid, Awais},
title = {Stuck in the Permissions With You: Developer &amp; End-User Perspectives on App Permissions &amp; Their Privacy Ramifications},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581060},
doi = {10.1145/3544548.3581060},
abstract = {While the literature on permissions from the end-user perspective is rich, there is a lack of empirical research on why developers request permissions, their conceptualization of permissions, and how their perspectives compare with end-users’ perspectives. Our study aims to address these gaps using a mixed-methods approach. Through interviews with 19 app developers and a survey of 309 Android and iOS end-users, we found that both groups shared similar concerns about unnecessary permissions breaking trust, damaging the app’s reputation, and potentially allowing access to sensitive data. We also found that developer participants sometimes requested multiple permissions due to confusion about the scope of certain permissions or third-party library requirements. Additionally, most end-user participants believed they were responsible for granting a permission request, and it was their choice to do so, a belief shared by many developer participants. Our findings have implications for improving the permission ecosystem for both developers and end-users.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {168},
numpages = {24},
keywords = {usable security, programming, mixed-methods research, smartphone permissions, developers, privacy, empirical software engineering, app users, usable privacy},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3183399.3183417,
author = {Hejderup, Joseph and van Deursen, Arie and Gousios, Georgios},
title = {Software Ecosystem Call Graph for Dependency Management},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183417},
doi = {10.1145/3183399.3183417},
abstract = {A popular form of software reuse is the use of open source software libraries hosted on centralized code repositories, such as Maven or npm. Developers only need to declare dependencies to external libraries, and automated tools make them available to the workspace of the project. Recent incidents, such as the Equifax data breach and the leftpad package removal, demonstrate the difficulty in assessing the severity, impact and spread of bugs in dependency networks. While dependency checkers are being adapted as a counter measure, they only provide indicative information. To remedy this situation, we propose a fine-grained dependency network that goes beyond packages and into call graphs. The result is a versioned ecosystem-level call graph. In this paper, we outline the process to construct the proposed graph and present a preliminary evaluation of a security issue from a core package to an affected client application.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {101–104},
numpages = {4},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1145/2591062.2591171,
author = {Snipes, Will and Nair, Anil R. and Murphy-Hill, Emerson},
title = {Experiences Gamifying Developer Adoption of Practices and Tools},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591171},
doi = {10.1145/2591062.2591171},
abstract = {As software development practices evolve, toolsmiths face the continuous challenge of getting developers to adopt new practices and tools. We tested an idea with industrial software developers that adding game-like feedback to the development environment would improve adoption of tools and practices for code navigation. We present results from a pre-study survey of 130 developers' opinions on gamification and motivation, usage data from a study with an intact team of six developers of a game on code navigation practices, and feedback collected in post-study interviews. Our pre-study survey showed that most developers were interested in gamification, though some have strong negative opinions. Study results show that two of the six study developers adjusted their practices when presented with competitive game elements.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {105–114},
numpages = {10},
keywords = {Productivity, Code Navigation, Development},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2304696.2304720,
author = {Eklund, Ulrik and Bosch, Jan},
title = {Using Architecture for Multiple Levels of Access to an Ecosystem Platform},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304720},
doi = {10.1145/2304696.2304720},
abstract = {The paper presents a novel ecosystem for software-intensive embedded systems as an alternative to integration-centric software development. A set of necessary qualities is identified for the embedded platform enabling the ecosystem, i.e. deployability of new functions, maintainability over time, and configurability according to hardware within the product family. The embedded platform must support composability of software from different development stakeholders, such as the original equipment manufacturer, strategic and 3rd-party developers.The paper presents some key mechanisms for implementing the necessary certification in the platform that facilitates composability while still preserving dependability. A theoretical example of how this would look like in the automotive domain is presented, and an actual case of an in-vehicle platform implementing some of the mechanisms is shown.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {143–148},
numpages = {6},
keywords = {embedded system, software ecosystem},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.1145/3319008.3319028,
author = {Roy, Sohon and Deursen, Arie van and Hermans, Felienne},
title = {Perceived Relevance of Automatic Code Inspection in End-User Development: A Study on VBA},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319028},
doi = {10.1145/3319008.3319028},
abstract = {Microsoft VBA (Visual Basic for Applications) is a programming language widely used by end-user programmers, often alongside the popular spreadsheet software Excel. Together they form the popular Excel-VBA application ecosystem. Despite being popular, spreadsheets are known to be fault-prone, and to minimize risk of faults in the overall Excel-VBA ecosystem, it is important to support end-user programmers in improving the code quality of their VBA programs also, in addition to improving spreadsheet technology and practices. In traditional software development, automatic code inspection using static analysis tools has been found effective in improving code quality, but the practical relevance of this technique in an end-user development context remains unexplored. With the aim of popularizing it in the end-user community, in this paper we examine the relevance of automatic code inspection in terms of how inspection rules are perceived by VBA programmers. We conduct a qualitative study consisting of interviews with 14 VBA programmers, who share their perceptions about 20 inspection rules that most frequently detected code quality issues in an industrial dataset of 25 VBA applications, obtained from a financial services company. Results show that the 20 studied inspection rules can be grouped into three categories of user perceptions based on the type of issues they warn about: i) 11 rules that warn about serious problems which need fixing, ii) 7 rules that warn about bad practices which do not mandate fixing, and iii) 2 rules that warn about purposeful code elements rather than issues. Based on these perceptions, we conclude that automatic code inspection is considerably relevant in an end-user development context such as VBA. The perceptions also indicate which inspection rules deserve the most attention from interested researchers and tool developers. Lastly, our results also reveal 3 additional issue types that are not covered by the existing inspection rules, and are therefore impetus for creating new rules.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {167–176},
numpages = {10},
keywords = {static analysis, end-user development, developer perceptions, code quality, VBA},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1145/1364742.1364753,
author = {Wright, Richard},
title = {Digital Audiovisual Repositories: An Introduction},
year = {2006},
isbn = {1595936084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1364742.1364753},
doi = {10.1145/1364742.1364753},
abstract = {This paper briefly describes the essential aspects of the digital world that audiovisual archives are entering - or being swallowed-up in. The crucial issue is whether archives will sink or swim in this all-digital environment. The core issue is defining - and meeting - the requirements for a secure, sustainable digital repository.},
booktitle = {Proceedings of the 2006 International Workshop on Research Issues in Digital Libraries},
articleno = {8},
numpages = {7},
keywords = {library, archive, digital, repository, audiovisual},
location = {Kolkata, India},
series = {IWRIDL '06}
}

@inproceedings{10.1145/1710035.1710037,
author = {Raivio, Yrjo and Luukkainen, Sakari and Juntunen, Antero},
title = {Open Telco: A New Business Potential},
year = {2009},
isbn = {9781605585369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1710035.1710037},
doi = {10.1145/1710035.1710037},
abstract = {Until today, most mobile operators have still been very profitable, although Average Revenue per User (ARPU) has been steadily declining. The core mobile services, voice and text messaging, bring the majority of the income. In addition to those, new innovations have actively been searched for the last decade with insignificant success. One reason for the modest results can be found from the so-called walled garden approach. The situation is finally changing: openness and Open Innovation are the new paradigms. For a long time, open APIs have been utilized by Internet Service Providers (ISPs), such as Google, Flickr and Yahoo, and now the same approach has been proposed for mobile operators, as well. However, mobile operators have difficulties in justifying the opening of the walled garden: the major challenge is the lack of business case. This paper proposes an Open Telco approach to accelerate the innovativeness in the mobile space. The proposal is analyzed utilizing the so-called STOF (Service, Technology, Organization, and Finance) Model. Finally, recommendations for the next steps are given.},
booktitle = {Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems},
articleno = {2},
numpages = {6},
keywords = {STOF, web 2.0, mashup, Open Telco, open API, business model},
location = {Nice, France},
series = {Mobility '09}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@inproceedings{10.1145/3219104.3219159,
author = {Marini, Luigi and Gutierrez-Polo, Indira and Kooper, Rob and Satheesan, Sandeep Puthanveetil and Burnette, Maxwell and Lee, Jong and Nicholson, Todd and Zhao, Yan and McHenry, Kenton},
title = {Clowder: Open Source Data Management for Long Tail Data},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3219159},
doi = {10.1145/3219104.3219159},
abstract = {Clowder is an open source data management system to support data curation of long tail data and metadata across multiple research domains and diverse data types. Institutions and labs can install and customize their own instance of the framework on local hardware or on remote cloud computing resources to provide a shared service to distributed communities of researchers. Data can be ingested directly from instruments or manually uploaded by users and then shared with remote collaborators using a web front end. We discuss some of the challenges encountered in designing and developing a system that can be easily adapted to different scientific areas including digital preservation, geoscience, material science, medicine, social science, cultural heritage and the arts. Some of these challenges include support for large amounts of data, horizontal scaling of domain specific preprocessing algorithms, ability to provide new data visualizations in the web browser, a comprehensive Web service API for automatic data ingestion and curation, a suite of social annotation and metadata management features to support data annotation by communities of users and algorithms, and a web based front-end to interact with code running on heterogeneous clusters, including HPC resources.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {40},
numpages = {8},
keywords = {linked data, data management, data curation, metadata management, scientific gateways},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@article{10.1145/3038926,
author = {Soetens, Quinten David and Robbes, Romain and Demeyer, Serge},
title = {Changes as First-Class Citizens: A Research Perspective on Modern Software Tooling},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3038926},
doi = {10.1145/3038926},
abstract = {Software must evolve to keep up with an ever-changing context, the real world. We discuss an emergent trend in software evolution research revolving around the central notion that drives evolution: Change. By reifying change, and by modelling it as a first-class entity, researchers can now analyse the complex phenomenon known as software evolution with an unprecedented degree of accuracy. We present a Systematic Mapping Study of 86 articles to give an overview on the state of the art in this area of research and present a roadmap with open issues and future directions.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {18},
numpages = {38},
keywords = {fine-grained changes, atomic change operations, Systematic mapping study, change recording, change distilling, fine-grained edit operations}
}

@article{10.1145/3290837,
author = {Gasson, Susan and Purcelle, Michelle},
title = {A Participation Architecture to Support User Peripheral Participation in a Hybrid FOSS Community},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3290837},
doi = {10.1145/3290837},
abstract = {Participation by product users is critical to success in free, open-source software (FOSS) software communities as they originate and develop valuable ideas for product innovation that are unlikely to originate from the core software development community. Users tend to be involved at the periphery of FOSS communities, suggesting new product ideas, highlighting problems with user documentation, or explaining when the product design fails to fit with the needs of their local user application domain. As an increasing number of FOSS projects employ a hybrid participation model that combines volunteer effort with paid software development effort or product support, it can be difficult for non-developer users to participate in product innovation. In colocated organizations, it is theorized that peripheral participants learn how to engage with the practices and cultural identity of a community through a sociocultural apprenticeship known as legitimate peripheral participation. But we have little literature that explores how legitimate peripheral participation is enabled in online communities.The research study presented in this article explores how participation by peripheral users in a hybrid FOSS project is afforded by participation architecture channels and community mechanisms that mediate two forms of engagement: a “cognitive apprenticeship” that introduces participants to situated domain activity, such as the community processes involved in product innovation, and a “social apprenticeship” by which participants become enculturated in the system of meanings, values, norms, and behaviors that govern community/participant identity. We identified five stages of community innovation, analyzing sociotechnical affordances of the online participation architecture that enable peripheral participants to internalize the meanings of community practice and to develop a social identity within the FOSS community. Our contribution to theory is provided by the substantive explanation of the cognitive and social translations that enable legitimate peripheral participation in online communities, mediated by sociotechnical access channels and mechanisms that afford two contrasting forms of opportunities for action: those resulting from interactions between a goal-oriented actor and the technology platform features or channels of participation, and those associated with the social structures, roles, and relationships underpinning community interactions. Neither of these is sufficient without the other. Our contribution to practice is provided by an explanation of how four distinct categories of affordance provide these cognitive and social apprenticeship benefits, allowing participation architecture designers to cater to all forms of peripheral user participation. We conclude that the technical affordances of a typical FOSS community participation architecture are insufficient to mediate peripheral participation by nontechnical users. Meaningful participation is mediated by interactions between boundary spanners who play knowledge-brokering and organizational bridging roles. The combination of technical and social affordances enables peripheral participants to acquire an interior view of community practices and social culture and in turn to introduce new ideas, new values, and new rationales to produce a generative dance of innovation that percolates through the community.},
journal = {Trans. Soc. Comput.},
month = {dec},
articleno = {14},
numpages = {46},
keywords = {user participation, Legitimate peripheral participation, hybrid-FOSS community, participation architecture, affordances, innovation}
}

@inproceedings{10.1145/2818000.2818042,
author = {Falsina, Luca and Fratantonio, Yanick and Zanero, Stefano and Kruegel, Christopher and Vigna, Giovanni and Maggi, Federico},
title = {Grab 'n Run: Secure and Practical Dynamic Code Loading for Android Applications},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818042},
doi = {10.1145/2818000.2818042},
abstract = {Android introduced the dynamic code loading (DCL) mechanism to allow for code reuse, to achieve extensibility, to enable updating functionalities, or to boost application start-up performance. In spite of its wide adoption by developers, previous research has shown that the secure implementation of DCL-based functionality is challenging, often leading to remote code injection vulnerabilities. Unfortunately, previous attempts to address this problem by both the academic and Android developers communities are affected by either practicality or completeness issues, and, in some cases, are affected by severe vulnerabilities.In this paper, we propose, design, implement, and test Grab 'n Run, a novel code verification protocol and a series of supporting libraries, APIs, and tools, that address the problem by abstracting away from the developer many of the challenging implementation details. Grab 'n Run is designed to be practical: Among its tools, it provides a drop-in library, which requires no modifications to the Android framework or the underlying Dalvik/ART runtime, is very similar to the native API, and most code can be automatically rewritten to use it. Grab 'n Run also contains an application-rewriting tool, which allows to easily port legacy or third-party applications to use the secure APIs developed in this work.We evaluate the Grab 'n Run library with a user study, obtaining very encouraging results in vulnerability reduction, ease of use, and speed of development. We also show that the performance overhead introduced by our library is negligible. For the benefit of the security of the Android ecosystem, we released Grab 'n Run as open source.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {201–210},
numpages = {10},
keywords = {Dynamic Code Loading, Android, DexClassLoader},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@inproceedings{10.1145/3037697.3037743,
author = {Huang, Jian and Allen-Bond, Michael and Zhang, Xuechen},
title = {Pallas: Semantic-Aware Checking for Finding Deep Bugs in Fast Path},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037743},
doi = {10.1145/3037697.3037743},
abstract = {Software optimization is constantly a serious concern for developing high-performance systems. To accelerate the workflow execution of a specific functionality, software developers usually define and implement a fast path to speed up the critical and commonly executed functions in the workflow. However, producing a bug-free fast path is nontrivial. Our study on the Linux kernel discloses that a committed fast path can have up to 19 follow-up patches for bug fixing, and most of them are deep semantic bugs, which are difficult to be pinpointed by existing bug-finding tools.In this paper, we present such a new category of software bugs based on our fast-path bug study across various system software including virtual memory manager, file systems, network, and device drivers. We investigate their root causes and identify five error-prone aspects in a fast path: path state, trigger condition, path output, fault handling, and assistant data structure. We find that many of the deep bugs can be prevented by applying static analysis incorporating simple semantic information. We extract a set of rules based on our findings and build a toolkit PALLAS to check fast-path bugs. The evaluation results show that PALLAS can effectively reveal fast-path bugs in a variety of systems including Linux kernel, mobile operating system, software-defined networking system, and web browser.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {709–722},
numpages = {14},
keywords = {static analysis, fast path, semantic bugs, software optimization},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@inproceedings{10.1145/3135974.3135981,
author = {Andrus, Jeremy and AlDuaij, Naser and Nieh, Jason},
title = {Binary Compatible Graphics Support in Android for Running IOS Apps},
year = {2017},
isbn = {9781450347204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3135974.3135981},
doi = {10.1145/3135974.3135981},
abstract = {Mobile apps make extensive use of GPUs on smartphones and tablets to access Web content. To support pervasive Web content, we introduce three key OS techniques for binary graphics compatibility necessary to build a real-world system to run iOS and Android apps together on the same smartphone or tablet. First diplomat usage patterns manage resources to bridge proprietary iOS and Android graphics implementations. Second, thread impersonation allows a single thread-specific context to be shared amongst multiple threads using multiple iOS and Android personas. Third, dynamic library replication allows multiple, independent instances of the same library to be loaded in a single process to support iOS apps on Android while using multiple graphics API versions at the same time. We use these techniques to build a system prototype, and demonstrate that it runs widely-used iOS apps, including apps such as Safari that use the popular GPU-accelerated WebKit framework, using a Google Nexus tablet running Android.},
booktitle = {Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference},
pages = {55–67},
numpages = {13},
keywords = {iOS, GPUs, operating system compatibility, mobile computing, OpenGL, web browsers, computer graphics, Android},
location = {Las Vegas, Nevada},
series = {Middleware '17}
}

@inbook{10.1145/3591366.3591380,
author = {Jacobs, Ian},
title = {The World Wide Web Consortium},
year = {2023},
isbn = {9798400707940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3591366.3591380},
booktitle = {Linking the World’s Information: Essays on Tim Berners-Lee’s Invention of the World Wide Web},
pages = {155–165},
numpages = {11}
}

@inproceedings{10.5555/2666036.2666041,
author = {McGregor, John D. and Monteith, J. Yates and Zhang, Jie},
title = {Technical Debt Aggregation in Ecosystems},
year = {2012},
isbn = {9781467317498},
publisher = {IEEE Press},
abstract = {The members of the ecosystem encompassing our organization are affected by our decisions just as we are affected by their decisions. If an organization takes on technical debt with respect to a specific asset, that decision will affect users of the asset either directly or indirectly. In this position paper we distinguish between incurring technical debt directly and experiencing the effects of technical debt indirectly. We illustrate why two separate concepts are needed for a complete theory and provide examples from ecosystem models we have created for several organizations. The result is a model that produces good explanations for posited scenarios.},
booktitle = {Proceedings of the Third International Workshop on Managing Technical Debt},
pages = {27–30},
numpages = {4},
keywords = {supply chain, technical debt, ecosystem, value chain},
location = {Zurich, Switzerland},
series = {MTD '12}
}

@inproceedings{10.1145/3369457.3369462,
author = {Farr-Wharton, Geremy},
title = {Seed to Feed: Leveraging HCl and Capitalising on Office Environments to Grow Food},
year = {2020},
isbn = {9781450376969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369457.3369462},
doi = {10.1145/3369457.3369462},
abstract = {Food security is a global concern, but achieving it faces no shortage of challenges. With the world continuing to urbanise, traditional forms of agriculture are diminishing, driving exploration into alternatives methods of food production. Urban farming is becoming a growing phenomenon with individuals investing in methods, such as WindowFarms, Aquafarms and vertical farms as a means of supplementing their food supply. Applications of alternative growing methods have centred on domestic settings or localised communities, and limited research has explored urban farming methods used in workplaces. This paper presents the findings of four alternative food growing applications within a workplace, to explore the viability of future implementations within similar workplace contexts. The findings from this study can be used to better understand the sociotechnical challenges of growing food in unique urban environments, presenting opportunities for urban farming to occur in a variety of settings that are not limited to domestic applications.},
booktitle = {Proceedings of the 31st Australian Conference on Human-Computer-Interaction},
pages = {53–63},
numpages = {11},
keywords = {HCI, Urban Agriculture, Food, Permaculture, Window Farming},
location = {Fremantle, WA, Australia},
series = {OzCHI '19}
}

@inproceedings{10.1145/3433210.3453115,
author = {Meng, Dongyu and Guerriero, Michele and Machiry, Aravind and Aghakhani, Hojjat and Bose, Priyanka and Continella, Andrea and Kruegel, Christopher and Vigna, Giovanni},
title = {Bran: Reduce Vulnerability Search Space in Large Open Source Repositories by Learning Bug Symptoms},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3453115},
doi = {10.1145/3433210.3453115},
abstract = {Software is continually increasing in size and complexity, and therefore, vulnerability discovery would benefit from techniques that identify potentially vulnerable regions within large code bases, as this allows for easing vulnerability detection by reducing the search space. Previous work has explored the use of conventional code-quality and complexity metrics in highlighting suspicious sections of (source) code. Recently, researchers also proposed to reduce the vulnerability search space by studying code properties with neural networks. However, previous work generally failed in leveraging the rich metadata that is available for long-running, large code repositories.In this paper, we present an approach, named Bran, to reduce the vulnerability search space by combining conventional code metrics with fine-grained repository metadata. Bran locates code sections that are more likely to contain vulnerabilities in large code bases, potentially improving the efficiency of both manual and automatic code audits. In our experiments on four large code bases, Bran successfully highlights potentially vulnerable functions, outperforming several baselines, including state-of-art vulnerability prediction tools. We also assess Bran's effectiveness in assisting automated testing tools. We use Bran to guide syzkaller, a known kernel fuzzer, in fuzzing a recent version of the Linux kernel. The guided fuzzer identifies 26 bugs (10 are zero-day flaws), including arbitrary writes and reads.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {731–743},
numpages = {13},
keywords = {static analysis, vulnerabilities, machine learning},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3359061.3361076,
author = {Yang, Junwen},
title = {Improving Performance and Quality of Database-Backed Software},
year = {2019},
isbn = {9781450369923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359061.3361076},
doi = {10.1145/3359061.3361076},
abstract = {Modern web applications have stringent latency requirements while processing an ever-increasing amount of user data. To address these challenges and improve programmer productivity, Object Relational Mapping (ORM) frameworks have been developed to allow developers writing database processing code in an object-oriented manner. Despite such frameworks, prior work found that developers still struggle in developing ORM-based web applications. This paper presents a series of study and developed tools for optimizing web applications developed using the Ruby on Rails ORM. Using automated static analysis, we detect ORM related inefficiency problems and suggests fixes to developers. Our evaluation on 12 real-world applications shows that more than 1000 performance issues can be detected and fixed.},
booktitle = {Proceedings Companion of the 2019 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {23–25},
numpages = {3},
keywords = {Object-Relational Mapping frameworks, RubyMine Plugin, database-backed applications, performance anti-patterns},
location = {Athens, Greece},
series = {SPLASH Companion 2019}
}

@inproceedings{10.1145/1274940.1274955,
author = {Pearce, Celia and Ashmore, Calvin},
title = {Principles of Emergent Design in Online Games: Mermaids Phase 1 Prototype},
year = {2007},
isbn = {9781595937490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1274940.1274955},
doi = {10.1145/1274940.1274955},
abstract = {This paper outlines the first phase prototype of Mermaids, a massively multiplayer online game (MMOG) being developed by Georgia Tech's Emergent Game Group {EGG}. We describe Mermaids in the context of the group's research mission, to develop specific games, techniques and design features that promote large-scale emergent social behavior in multiplayer games. We also discuss some of the innovative design features of the Mermaids game, and describe the rapid prototyping and iterative development process that enabled us to create a working prototype in a relatively short period of time on a zero budget project using a student-based development team. We also discuss the special challenges encountered when trying to develop a nontraditional game, one of whose stated research goals is to interrogate MMOG conventions, using a relatively conventional game engine.},
booktitle = {Proceedings of the 2007 ACM SIGGRAPH Symposium on Video Games},
pages = {65–71},
numpages = {7},
keywords = {real-time 3D, massively multiplayer online games, online games, emergence in games, networked games, MMOGs},
location = {San Diego, California},
series = {Sandbox '07}
}

@article{10.1145/3474688,
author = {Rogerson, Melissa J. and Sparrow, Lucy A. and Gibbs, Martin R.},
title = {More Than a Gimmick - Digital Tools for Boardgame Play},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3474688},
doi = {10.1145/3474688},
abstract = {Despite increasing interest in the use of digital tools in boardgames for both commercial and research purposes, little research has to date explored how and why these tools are used. We interviewed 18 professionals working in the boardgame industry to explore the combination of digital tools and tabletop play, which affords new experiences and opportunities for both players and designers. We generated five key themes from the interview data. Participants engaged with ontological questions about the fundamental nature of games; they showed strong opinions about the use of digital tools; they discussed the impacts of digital tools for game design as well as in their design practice; they raised concerns about the costs to develop and maintain such tools; and they considered how they affect the in-game player experience. From these themes, we generate five design principles for digital tools for boardgame play: traceability, completeness, integration, privacy, and materiality. Grounded in empirical data, these design principles guide game designers and researchers seeking to explore this novel design space. Our research focuses attention on the role of digital components in play and on the need for thoughtful implementation that considers the entire lifecycle of the game, from development through publication and, ultimately, archival access.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {261},
numpages = {23},
keywords = {user studies, boardgames, game design, hybrid digital boardgames, hybrid play}
}

@inproceedings{10.1109/ICSE43902.2021.00123,
author = {Wang, Pei and Bangert, Julian and Kern, Christoph},
title = {If It's Not Secure, It Should Not Compile: Preventing DOM-Based XSS in Large-Scale Web Development with API Hardening},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00123},
doi = {10.1109/ICSE43902.2021.00123},
abstract = {With tons of efforts spent on its mitigation, Cross-site scripting (XSS) remains one of the most prevalent security threats on the internet. Decades of exploitation and remediation demonstrated that code inspection and testing alone does not eliminate XSS vulnerabilities in complex web applications with a high degree of confidence.This paper introduces Google's secure-by-design engineering paradigm that effectively prevents DOM-based XSS vulnerabilities in large-scale web development. Our approach, named API hardening, enforces a series of company-wide secure coding practices. We provide a set of secure APIs to replace native DOM APIs that are prone to XSS vulnerabilities. Through a combination of type contracts and appropriate validation and escaping, the secure APIs ensure that applications based thereon are free of XSS vulnerabilities. We deploy a simple yet capable compile-time checker to guarantee that developers exclusively use our hardened APIs to interact with the DOM. We make various of efforts to scale this approach to tens of thousands of engineers without significant productivity impact. By offering rigorous tooling and consultant support, we help developers adopt the secure coding practices as seamlessly as possible. We present empirical results showing how API hardening has helped reduce the occurrences of XSS vulnerabilities in Google's enormous code base over the course of two-year deployment.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1360–1372},
numpages = {13},
keywords = {language-based security, cross-site scripting, empirical software engineering, Web security},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2909609.2909658,
author = {Houston, Lara and Jackson, Steven J.},
title = {Caring for the "next Billion" Mobile Handsets: Opening Proprietary Closures through the Work of Repair},
year = {2016},
isbn = {9781450343060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909609.2909658},
doi = {10.1145/2909609.2909658},
abstract = {ICTD is profoundly interested in the "next billion" users, and how to leverage technology to improve their everyday lives. In this paper we ask how the concept of care might be generatively extended to the 'lives' of the "next billion" mobile handsets. Drawing on a growing literature on repair in ICTD and HCI, and theories of care from the social sciences, this paper makes two central contributions. First, our ethnographic study of mobile phone repair in downtown Kampala, Uganda provides new insights into how technologies are sustained in developing contexts, with a special focus on how independent technicians in informal repair shops circumvent the proprietary closures that limit their work. Second, we show how attending to care in ICTD contexts can help us locate immediate forms of technical work (here, repair) within wider moral and political orderings. Thinking about repair and care together opens up new possibilities for ICTD to engage with the materiality of technologies over longer temporal horizons, beyond privileged moments of design and adoption.},
booktitle = {Proceedings of the Eighth International Conference on Information and Communication Technologies and Development},
articleno = {10},
numpages = {11},
keywords = {care, Mobile telephony, ethnography, Uganda, proprietary technology, repair, ICT4D},
location = {Ann Arbor, MI, USA},
series = {ICTD '16}
}

@inproceedings{10.5555/3320516.3320673,
author = {Marcosig, Ezequiel Pecker and Giribet, Juan I. and Castro, Rodrigo},
title = {DEVS-over-ROS (DoveR): A Framework for Simulation-Driven Embedded Control of Robotic Systems Based on Model Continuity},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Designing hybrid controllers for cyber-physical systems raises the need to interact with embedded platforms, robotic applications being a paradigmatic example. This can become a difficult, time consuming and error-prone task for non-specialists as it demands for background on low-level software/hardware interfaces often falling beyond the scope of control designers. We propose a simulation-driven methodology and tool for designing hybrid controllers based on a model continuity approach. The simulation model of a controller should evolve transparently from a desktop-based mocking up environment until its final embedded target without the need of intermediate adaptations. DEVS-over-ROS relies on the DEVS framework for robust modeling and real-time simulation of hybrid controllers, and on the ROS middleware for flexible abstraction of software/hardware interfaces for sensors and actuators. We successfully tested DoveR in a case study where a custom-made crafted robotic system is built concurrently with the design of its controller.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {1250–1261},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3311790.3400848,
author = {Stubbs, Joe and Marru, Suresh and Mejia, Daniel and Katz, Daniel S. and Chard, Kyle and Dahan, Maytal and Pierce, Marlon and Zentner, Michael},
title = {Toward Interoperable Cyberinfrastructure: Common Descriptions for Computational Resources and Applications},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3400848},
doi = {10.1145/3311790.3400848},
abstract = {The user-facing components of the Cyberinfrastructure (CI) ecosystem, science gateways and scientific workflow systems, share a common need of interfacing with physical resources (storage systems and execution environments) to manage data and execute codes (applications). However, there is no uniform, platform-independent way to describe either the resources or the applications. To address this, we propose uniform semantics for describing resources and applications that will be relevant to a diverse set of stakeholders. We sketch a solution to the problem of a common description and catalog of resources: we describe an approach to implementing a resource registry for use by the community and discuss potential approaches to some long-term challenges. We conclude by looking ahead to the application description language.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {522–525},
numpages = {4},
keywords = {science gateways community institute, resource description, interoperability, Cyberinfrastructure, application description, science gateways},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.1145/2568225.2568322,
author = {Brindescu, Caius and Codoban, Mihai and Shmarkatiuk, Sergii and Dig, Danny},
title = {How Do Centralized and Distributed Version Control Systems Impact Software Changes?},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568322},
doi = {10.1145/2568225.2568322},
abstract = {Distributed Version Control Systems (DVCS) have seen an increase in popularity relative to traditional Centralized Version Control Systems (CVCS). Yet we know little on whether developers are benefitting from the extra power of DVCS. Without such knowledge, researchers, developers, tool builders, and team managers are in the danger of making wrong assumptions.  In this paper we present the first in-depth, large scale empirical study that looks at the influence of DVCS on the practice of splitting, grouping, and committing changes. We recruited 820 participants for a survey that sheds light into the practice of using DVCS. We also analyzed 409M lines of code changed by 358300 commits, made by 5890 developers, in 132 repositories containing a total of 73M LOC. Using this data, we uncovered some interesting facts. For example, (i) commits made in distributed repositories were 32% smaller than the centralized ones, (ii) developers split commits more often in DVCS, and (iii) DVCS commits are more likely to have references to issue tracking labels.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {322–333},
numpages = {12},
keywords = {Centralized Version Control, Distributed Version Control, Software Change, Version Control},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1145/3386326,
author = {King, Paul},
title = {A History of the Groovy Programming Language},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386326},
doi = {10.1145/3386326},
abstract = {This paper describes the history of the Groovy programming language. At the time of Groovy’s inception, Java was a dominant programming language with a wealth of useful libraries. Despite this, it was perceived by some to be evolving slowing and to have shortcomings for scripting, rapid prototyping and when trying to write minimalistic code. Other languages seemed to be innovating faster than Java and, while overcoming some of Java’s shortcomings, used syntax that was less familiar to Java developers. Integration with Java libraries was also non-optimal. Groovy was created as a complementary language to Java—its dynamic counterpart. It would look and feel like Java but focus on extensibility and rapid innovation. Groovy would borrow ideas from dynamic languages like Ruby, Python and Smalltalk where needed to provide compelling JVM solutions for some of Java’s shortcomings. Groovy supported innovation through its runtime and compile-time metaprogramming capabilities. It supported simple operator overloading, had a flexible grammar and was extensible. These characteristics made it suitable for growing the language to have new commands (verbs) and properties (nouns) specific to a particular domain, a so called Domain Specific Language (DSL). While still intrinsically linked with Java, over time Groovy has evolved from a niche dynamic scripting language into a compelling mainstream language. After many years as a principally dynamically-typed language, a static nature was added to Groovy. Code could be statically type checked or when dynamic features weren’t needed, they could be turned off entirely for Java-like performance. A number of nuances to the static nature came about to support the style of coding used by Groovy developers. Many choices made by Groovy in its design, later appeared in other languages (Swift, C#, Kotlin, Ceylon, PHP, Ruby, Coffeescript, Scala, Frege, TypeScript and Java itself). This includes Groovy’s dangling closure, Groovy builders, null-safe navigation, the Elvis operator, ranges, the spaceship operator, and flow typing. For most languages, we don’t know to what extent Groovy played a part in their choices. We do know that Kotlin took inspiration from Groovy’s dangling closures, builder concept, default it parameter for closures, templates and interpolated strings, null-safe navigation and the Elvis operator. The leadership, governance and sponsorship arrangements of Groovy have evolved over time, but Groovy has always been a successful highly collaborative open source project driven more by the needs of the community than by a vision of a particular company or person.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {76},
numpages = {53},
keywords = {Domain Specific Languages, Static typing, Closure, Scripting, Extensibility, Dynamic typing, Object-oriented, Functional programming, Metaprogramming}
}

@inproceedings{10.1145/1134285.1134352,
author = {Gurbani, Vijay K. and Garvert, Anita and Herbsleb, James D.},
title = {A Case Study of a Corporate Open Source Development Model},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134352},
doi = {10.1145/1134285.1134352},
abstract = {Open source practices and tools have proven to be highly effective for overcoming the many problems of geographically distributed software development. We know relatively little, however, about the range of settings in which they work. In particular, can corporations use the open source development model effectively for software projects inside the corporate domain? Or are these tools and practices incompatible with development environments, management practices, and market-driven schedule and feature decisions typical of a commercial software house? We present a case study of open source software development methodology adopted by a significant commercial software project in the telecommunications domain. We extract a number of lessons learned from the experience, and identify open research questions.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {472–481},
numpages = {10},
keywords = {open source, architecture, software development, session initiation protocol},
location = {Shanghai, China},
series = {ICSE '06}
}

@inproceedings{10.1145/3210459.3210464,
author = {Williams, Ashley},
title = {Using Reasoning Markers to Select the More Rigorous Software Practitioners' Online Content When Searching for Grey Literature},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210464},
doi = {10.1145/3210459.3210464},
abstract = {Background: Blog articles have potential value as a source of practitioner generated evidence in grey literature reviews: they could complement already accepted sources (e.g. interviews and focus groups). To be valuable to research, blog articles need to be relevant, rigorous and evidence-based.Objective: This paper focuses on the rigour of blog articles. We develop, evaluate and partially validate a set of reasoning markers that can be used to search for rigorous blog articles. We then demonstrate how these markers can be used in the online search of grey literature for software testing.Method: We identify discourse markers from literature and then select those that are explicit indicators of reasoning. We evaluate the set against a corpus of persuasive essays, and validate false negatives to refine our set further. We then demonstrate the use of the set in a search of grey literature on software testing.Results: The set of markers is reasonably successful at detecting reasoning within the corpus, achieving a precision of 89.6% in the first pass and 91.1% after refining our set. However, recall is low due to specifically focusing only on explicit indicators of reasoning (31.3% in the first pass and 38.7% in the second). Our overall F1-Score is 46.4% in the first pass and 54.3% in the second. This is acceptable for the time being as the current focus is on the quality of results retrieved from a keyword-based search. Improving the recall of results is left for future research.Conclusion: Our work provides a set of discourse markers that can be used to indicate the presence of reasoning in a blog article, and therefore provides an indication of the more rigorous blog article content. We intend to extend the work through considering the presence of evidence, and improving the relevance of blog articles found.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {46–56},
numpages = {11},
keywords = {grey literature reviews, Evidence based software engineering, systematic literature reviews, evidence, argumentation, discourse markers, blogs},
location = {Christchurch, New Zealand},
series = {EASE '18}
}

@inproceedings{10.1145/2532352.2532354,
author = {Nanthaamornphong, Aziz and Morris, Karla and Filippone, Salvatore},
title = {Extracting UML Class Diagrams from Object-Oriented Fortran: ForUML},
year = {2013},
isbn = {9781450324991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532352.2532354},
doi = {10.1145/2532352.2532354},
abstract = {Many scientists and engineers who implement high performance computing (HPC) software have adopted the object-oriented (OO) Fortran paradigm. One of the challenges faced by OO Fortran developers is the inability to obtain high level software design descriptions of existing applications. Knowledge of the overall software design is not only valuable in the absence of documentation, it can also serve to assist developers with accomplishing different tasks during the software development process, especially maintenance and refactoring. The software engineering community commonly uses reverse engineering techniques to deal with this challenge. A number of reverse engineering-based tools have been proposed, but few of them can be applied to object-oriented Fortran applications.In this paper, we propose a software tool to extract unified modeling language (UML) class diagrams from Fortran code. The UML class diagram facilitates the developers' ability to examine the entities and their relationships in the software system. The extracted diagrams enhance software maintenance and refactoring. The experiments carried out with the aim to evaluate the proposed tool show its accuracy and a few limitations.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering},
pages = {9–16},
numpages = {8},
keywords = {Fortran programming, reverse engineering, computational science and engineering, high performance computing, tools},
location = {Denver, Colorado},
series = {SE-HPCCSE '13}
}

@inproceedings{10.1145/2351676.2351679,
author = {Jacobson, Ivar and Spence, Ian and Johnson, Pontus and Kajko-Mattsson, Mira},
title = {Re-Founding Software Engineering – SEMAT at the Age of Three (Keynote Abstract)},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351679},
doi = {10.1145/2351676.2351679},
abstract = {Software engineering is gravely hampered by immature practices. Specific problems include: The prevalence of fads more typical of the fashion industry than an engineering discipline; a huge number of methods and method variants, with differences little understood and artificially magnified; the lack of credible experimental evaluation and validation; and the split between industry practice and academic research.  At the root of the problems we lack a sound, widely accepted theoretical basis. A prime example of such a basis is Maxwell’s equations in electrical engineering. It is difficult to fathom what electrical engineering would be today without those four concise equations. They are a great example to the statement “There is nothing so practical as a good theory”. In software engineering we have nothing similar, and there is widespread doubt whether it is needed. This talk will argue for the need of a basic theory in software engineering, a theory identifying its pure essence, its common ground or its kernel.  The Semat (Software Engineering Methods and Theory) community addresses this huge challenge. It supports a process to refound software engineering based on a kernel of widely-agreed elements, extensible for specific uses, addressing both technology and people issues. This kernel represents the essence of software engineering. This talk promises to make you see the light in the tunnel.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {15–19},
numpages = {5},
keywords = {activity space, education, method, status of SEMAT, software engineering theory, Kernel, practice, alpha},
location = {Essen, Germany},
series = {ASE '12}
}

@inproceedings{10.1145/2393132.2393173,
author = {Juntunen, Tomi and Kostakos, Vassilis and Perttunen, Mikko and Ferreira, Denzil},
title = {Web Tool for Traffic Engineers: Direct Manipulation and Visualization of Vehicular Traffic Using Google Maps},
year = {2012},
isbn = {9781450316378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393132.2393173},
doi = {10.1145/2393132.2393173},
abstract = {We present a lightweight web-based tool aimed for traffic engineers that allows an engineer-friendly way to interact and explore traffic volume statistics. This system has been created in participatory design with experienced traffic engineers from a local traffic center. The tool has been built using common web technologies and utilizing an existing traffic detection loop network, and public online GIS and graph tool APIs.},
booktitle = {Proceeding of the 16th International Academic MindTrek Conference},
pages = {209–210},
numpages = {2},
keywords = {statistics, traffic volume, web-based services, tools, vehicular traffic},
location = {Tampere, Finland},
series = {MindTrek '12}
}

@inproceedings{10.1145/2756594.2756598,
author = {Peterson, Larry and Baker, Scott and De Leenheer, Marc and Bavier, Andy and Bhatia, Sapan and Wawrzoniak, Mike and Nelson, Jude and Hartman, John},
title = {XoS: An Extensible Cloud Operating System},
year = {2015},
isbn = {9781450335683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2756594.2756598},
doi = {10.1145/2756594.2756598},
abstract = {This paper describes XOS, a cloud operating system designed to manage hardware and software resources across a multi-tiered cloud. XOS raises the level of abstraction in an IaaS cloud architecture by elevating scalable software services to first-class objects. This involves adopting three design principles: (1) Everything-as-a-Service (XaaS) (services are building blocks, and combinations of those building blocks are also services); (2) Multi-tenancy (a tenant relationship links one service to another, and facilitates reasoning about safety, privacy and efficiency); and (3) Control/Data-plane separation (services are configured through a logically centralized service controller interface, but the controller is not on the data path between services). XOS applies these principles through the lens of an operating system - it defines a set of abstractions that support constructing multi-tenant services that can be folded back into XOS as available building blocks, while also extending the capabilities of conventional IaaS. The paper shows how these abstractions can be used to build a functional, evolvable, service-oriented cloud.},
booktitle = {Proceedings of the 2nd International Workshop on Software-Defined Ecosystems},
pages = {23–30},
numpages = {8},
location = {Portland, Oregon, USA},
series = {BigSystem '15}
}

@inproceedings{10.1145/3317549.3319721,
author = {Guan, Le and Cao, Chen and Zhu, Sencun and Lin, Jingqiang and Liu, Peng and Xia, Yubin and Luo, Bo},
title = {Protecting Mobile Devices from Physical Memory Attacks with Targeted Encryption},
year = {2019},
isbn = {9781450367264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317549.3319721},
doi = {10.1145/3317549.3319721},
abstract = {Sensitive data in a process could be scattered over the memory of a computer system for a prolonged period of time. Unfortunately, DRAM chips were proven insecure in previous studies. The problem becomes worse in the mobile environment, in which users' smartphones are easily lost or stolen. The powered-on phones may contain sensitive data in the vulnerable DRAM chips. In this paper, we propose MemVault, a mechanism to protect sensitive data in Android devices against physical memory attacks. MemVault keeps track of the propagation of well-marked sensitive data sources, and selectively encrypts tainted sensitive memory contents in the DRAM chip. When a tainted object is accessed, MemVault redirects the access to the internal RAM (iRAM), where the cipher-text object is decrypted transparently. iRAM is a system-on-chip (SoC) component which is by nature immune to physical memory exploits. We have implemented a MemVault prototype system, and have evaluated it with extensive experiments. Our results validate that MemVault effectively eliminates the occurrences of clear-text sensitive objects in DRAM chips, and imposes acceptable overheads.},
booktitle = {Proceedings of the 12th Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {34–44},
numpages = {11},
keywords = {taint analysis, memory encryption, physical attack},
location = {Miami, Florida},
series = {WiSec '19}
}

@article{10.1145/2533685,
author = {Stol, Klaas-Jan and Avgeriou, Paris and Babar, Muhammad Ali and Lucas, Yan and Fitzgerald, Brian},
title = {Key Factors for Adopting Inner Source},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2533685},
doi = {10.1145/2533685},
abstract = {A number of organizations have adopted Open Source Software (OSS) development practices to support or augment their software development processes, a phenomenon frequently referred to as Inner Source. However the adoption of Inner Source is not a straightforward issue. Many organizations are struggling with the question of whether Inner Source is an appropriate approach to software development for them in the first place. This article presents a framework derived from the literature on Inner Source, which identifies nine important factors that need to be considered when implementing Inner Source. The framework can be used as a probing instrument to assess an organization on these nine factors so as to gain an understanding of whether or not Inner Source is suitable. We applied the framework in three case studies at Philips Healthcare, Neopost Technologies, and Rolls-Royce, which are all large organizations that have either adopted Inner Source or were planning to do so. Based on the results presented in this article, we outline directions for future research.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {18},
numpages = {35},
keywords = {inner source, framework, Case study, open-source development practices}
}

@inproceedings{10.1145/3364138.3364170,
author = {Montoya, Maria F and Mu\~{n}oz, John and Henao, Oscar},
title = {Design of an Upper Limbs Rehabilitation Videogame with SEMG and Biocybernetic Adaptation},
year = {2020},
isbn = {9781450371513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364138.3364170},
doi = {10.1145/3364138.3364170},
abstract = {Motor rehabilitation is known to have several difficulties regarding patient's engagement and therapy adherence. Since advances in physiological computing technologies have had an exponential grown in the last decade, its use in novel therapies for motor rehabilitation is being popularized. Serious games, for instance, have been applied as a complementary therapy for neuromuscular disorders, being the game design process a key factor to influence both the attractiveness and effectiveness of the game. In this paper, we expose a design methodology used for the creation of a serious videogame for motor rehabilitation of upper limbs using surface electromyography (sEMG) as the human-computer interface to control the game and monitor the players' fatigue levels. By using an adaptation mechanism from the physiological computing field, called biocybernetic adaptation; the videogame can adapt the game difficulty based on measured fatigue levels. The game design was also informed with therapeutic recommendations and followed an iterative design process. We hope this paper can reveal important insights for both engineers and game designers to create more physiologically intelligent solutions for motor rehabilitation.},
booktitle = {Proceedings of the 5th Workshop on ICTs for Improving Patients Rehabilitation Research Techniques},
pages = {152–155},
numpages = {4},
keywords = {motor rehabilitation, biocybernetic adaptation, electromyography, physiological computing, serious games, fatigue},
location = {Popayan, Columbia},
series = {REHAB '19}
}

@inproceedings{10.1145/3338906.3340456,
author = {Babi\'{c}, Domagoj and Bucur, Stefan and Chen, Yaohui and Ivan\v{c}i\'{c}, Franjo and King, Tim and Kusano, Markus and Lemieux, Caroline and Szekeres, L\'{a}szl\'{o} and Wang, Wei},
title = {FUDGE: Fuzz Driver Generation at Scale},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340456},
doi = {10.1145/3338906.3340456},
abstract = {At Google we have found tens of thousands of security and robustness bugs by fuzzing C and C++ libraries. To fuzz a library, a fuzzer requires a fuzz driver—which exercises some library code—to which it can pass inputs. Unfortunately, writing fuzz drivers remains a primarily manual exercise, a major hindrance to the widespread adoption of fuzzing. In this paper, we address this major hindrance by introducing the Fudge system for automated fuzz driver generation. Fudge automatically generates fuzz driver candidates for libraries based on existing client code. We have used Fudge to generate thousands of new drivers for a wide variety of libraries. Each generated driver includes a synthesized C/C++ program and a corresponding build script, and is automatically analyzed for quality. Developers have integrated over 200 of these generated drivers into continuous fuzzing services and have committed to address reported security bugs. Further, several of these fuzz drivers have been upstreamed to open source projects and integrated into the OSS-Fuzz fuzzing infrastructure. Running these fuzz drivers has resulted in over 150 bug fixes, including the elimination of numerous exploitable security vulnerabilities.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {975–985},
numpages = {11},
keywords = {code synthesis, fuzz testing, testing, program slicing, fuzzing, automated test generation, software security},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3239235.3239237,
author = {Behnamghader, Pooyan and Meemeng, Patavee and Fostiropoulos, Iordanis and Huang, Di and Srisopha, Kamonphop and Boehm, Barry},
title = {A Scalable and Efficient Approach for Compiling and Analyzing Commit History},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239237},
doi = {10.1145/3239235.3239237},
abstract = {Background: Researchers oftentimes measure quality metrics only in the changed files when analyzing software evolution over commit-history. This approach is not suitable for compilation and using program analysis techniques that require byte-code. At the same time, compiling the whole software not only is costly but may also leave us with many uncompilable and unanalyzed revisions. Aims: We intend to demonstrate if analyzing changes in a module results in achieving a high compilation ratio and a better understanding of software quality evolution. Method: We conduct a large-scale multi-perspective empirical study on 37838 distinct revisions of the core module of 68 systems across Apache, Google, and Netflix to assess their compilability and identify when the software is uncompilable as a result of a developer's fault. We study the characteristics of uncompilable revisions and analyze compilable ones to understand the impact of developers on software quality. Results: We achieve high compilation ratios: 98.4% for Apache, 99.0% for Google, and 94.3% for Netflix. We identify 303 sequences of uncompile commits and create a model to predict uncompilability based on commit metadata with an F1-score of 0.89 and an AUC of 0.96. We identify statistical differences between the impact of affiliated and external developers of organizations. Conclusions: Focusing on a module results in a more complete and accurate software evolution analysis, reduces the cost and complexity, and facilitates manual inspection.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {27},
numpages = {10},
keywords = {software technical debt, software maintainbaility evolution, software compilability, mining software repositories},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/2972206.2972224,
author = {\"{O}qvist, Jesper and Hedin, G\"{o}rel and Magnusson, Boris},
title = {Extraction-Based Regression Test Selection},
year = {2016},
isbn = {9781450341356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972206.2972224},
doi = {10.1145/2972206.2972224},
abstract = {Frequent regression testing is a core activity in agile software development, but large test suites can lead to long test running times, hampering agility. By safe RTS (Regression Test Selection) techniques, a subset of the tests can be identified that cover all tests that can change result since the last run. To pay off in practice, the RTS overhead must be low. Most existing RTS techniques are based on dynamic coverage analysis, making the overhead related to the tests run. We present Extraction-Based RTS, a new safe RTS technique which uses a fast static analysis with very low overhead, related to the size of the modification rather than to the tests run. The method is suitable for program-driven testing, commonly used in agile development, where each test is a piece of code that uses parts of the system under test. We have implemented the method for Java, and bench-marked it on a number of open source projects, showing that it pays off substantially in practice.},
booktitle = {Proceedings of the 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
articleno = {5},
numpages = {10},
keywords = {Regression Test Selection, Program Extraction, Static Analysis},
location = {Lugano, Switzerland},
series = {PPPJ '16}
}

@article{10.1145/365181.365234,
author = {Booch, Grady},
title = {Developing the Future},
year = {2001},
issue_date = {March 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/365181.365234},
doi = {10.1145/365181.365234},
journal = {Commun. ACM},
month = {mar},
pages = {118–121},
numpages = {4}
}

@inproceedings{10.1145/3284179.3284292,
author = {Marcos-Pablos, Samuel and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e}},
title = {Decision Support Tools for SLR Search String Construction},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284292},
doi = {10.1145/3284179.3284292},
abstract = {Systematic literature reviews (SLRs) have gained popularity during the last years as a form of providing state of the art about previous research. As part of the SLR tasks, devising the search strategy and particularly finding the right keywords to be included in the search string is a difficult and critical step, as it will determine what evidence will be identified in the different searched sources and thus condition the rest of the review. In order to support the search process, this paper presents an iterative methodology for search string construction along with a set of decision support tools that help in building the search string by finding appropriate key terms related to the topic of interest in order to assist the researcher in the SLR conduction.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {660–667},
numpages = {8},
keywords = {decision support tool, Systematic literature review, text mining},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/1806799.1806871,
author = {Guo, Philip J. and Zimmermann, Thomas and Nagappan, Nachiappan and Murphy, Brendan},
title = {Characterizing and Predicting Which Bugs Get Fixed: An Empirical Study of Microsoft Windows},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806871},
doi = {10.1145/1806799.1806871},
abstract = {We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter.Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {495–504},
numpages = {10},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/1880071.1880073,
author = {Luther, Kurt and Caine, Kelly and Ziegler, Kevin and Bruckman, Amy},
title = {Why It Works (When It Works): Success Factors in Online Creative Collaboration},
year = {2010},
isbn = {9781450303873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1880071.1880073},
doi = {10.1145/1880071.1880073},
abstract = {Online creative collaboration (peer production) has enabled the creation of Wikipedia and open source software (OSS), and is rapidly expanding to encompass new domains, such as video, music, and animation. But what are the underlying principles allowing online creative collaboration to succeed, and how well do they transfer from one domain to another? In this paper, we address these questions by comparing and contrasting online, collaborative animated movies, called collabs, with OSS projects. First, we use qualitative methods to solicit potential success factors from collab participants. Then, we test these predictions by quantitatively analyzing a data set of nearly 900 collabs. Finally, we compare and contrast our results with the literature on OSS development and propose broader theoretical implications. Our findings offer a starting point for a systematic research agenda seeking to unlock the potential of online creative collaboration.},
booktitle = {Proceedings of the 2010 ACM International Conference on Supporting Group Work},
pages = {1–10},
numpages = {10},
keywords = {peer production, success, online community, open source, leadership, online creative collaboration, social computing},
location = {Sanibel Island, Florida, USA},
series = {GROUP '10}
}

@inproceedings{10.1145/2457276.2457297,
author = {Li, Wenbin and Badr, Youakim and Biennier, Fr\'{e}d\'{e}rique},
title = {Digital Ecosystems: Challenges and Prospects},
year = {2012},
isbn = {9781450317559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457276.2457297},
doi = {10.1145/2457276.2457297},
abstract = {Mimicking biological ecosystems, digital ecosystems refer to complex and interdependent systems and their underlying infrastructures by which all constituents interact and exhibit as a whole self-organizing, scalable and sustainable behaviors. This emerging topic is of great interest to scholars. Nevertheless, we observe a divergence of opinions and a disagreement about what Digital Ecosystems are, how they are designed, and what their applications should be. In this paper, we present an overall view of the concept of Digital Ecosystem and provide background for a common understanding. Firstly, we identify four factors that have lead to the emergence of Digital Ecosystems and present their definitions, characteristics and categories. Secondly, we discuss models and simulation tools to show current achievements and limitations. Finally, we illustrate the main challenges and shed light on further work in the field of Digital Ecosystems.},
booktitle = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems},
pages = {117–122},
numpages = {6},
keywords = {digital ecosystem, simulation, case study, survey},
location = {Addis Ababa, Ethiopia},
series = {MEDES '12}
}

@article{10.1145/3552309,
author = {Reed, Daniel and Gannon, Dennis and Dongarra, Jack},
title = {HPC Forecast: Cloudy and Uncertain},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3552309},
doi = {10.1145/3552309},
abstract = {An examination of how the technology landscape has changed and possible future directions for HPC operations and innovation.},
journal = {Commun. ACM},
month = {jan},
pages = {82–90},
numpages = {9}
}

@article{10.1145/3593802,
author = {Assi, Maram and Hassan, Safwat and Georgiou, Stefanos and Zou, Ying},
title = {Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3593802},
doi = {10.1145/3593802},
abstract = {Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database, to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {141},
numpages = {34},
keywords = {topics of issue reports, defect fixing, code churn, change impact analysis, Issue reports, amount of change, fixing effort}
}

@inproceedings{10.1145/3238147.3241985,
author = {Di Cosmo, Roberto},
title = {Software Heritage: Collecting, Preserving, and Sharing All Our Source Code (Keynote)},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3241985},
doi = {10.1145/3238147.3241985},
abstract = {Software Heritage is a non profit initiative whose ambitious goal is to collect, preserve and share the source code of all software ever written, with its full development history, building a universal source code software knowledge base. Software Heritage addresses a variety of needs: preserving our scientific and technological knowledge, enabling better software development and reuse for society and industry, fostering better science, and building an essential infrastructure for large scale, reproducible software studies. We have already collected over 4 billions unique source files from over 80 millions repositories, and organised them into a giant Merkle graph, with full deduplication across all repositories. This allows us to cope with the growth of collaborative software development, and provides a unique vantage point for observing its evolution. In this talk, we will highlight the new challenges and opportunities that Software Heritage brings up.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {1–2},
numpages = {2},
keywords = {Software archive},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3097983.3098075,
author = {McNamara, Quinten and De La Vega, Alejandro and Yarkoni, Tal},
title = {Developing a Comprehensive Framework for Multimodal Feature Extraction},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098075},
doi = {10.1145/3097983.3098075},
abstract = {Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions--ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (videos, images, audio, and text), and is expressly implemented with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct simple feature extraction workflows while increasing code clarity and maintainability.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1567–1574},
numpages = {8},
keywords = {feature extraction, python, multimodal retrieval, standardization, wrappers},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2593882.2593883,
author = {Fuggetta, Alfonso and Di Nitto, Elisabetta},
title = {Software Process},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593883},
doi = {10.1145/2593882.2593883},
abstract = {This paper is a travelogue of Software Process research and practice in the past 15 years. It is based on the paper written by one of the authors for the FOSE Track at ICSE 2000. Since then, the landscape of Software Process research has significantly evolved: technological breakthroughs and market disruptions have defined new and complex challenges for Software Engineering researchers and practitioners.  In this paper we provide an overview of the current status of research and practice, highlight new challenges, and provide a non-exhaustive list of research issues that, in our view, need to be tackled by future research work.},
booktitle = {Future of Software Engineering Proceedings},
pages = {1–12},
numpages = {12},
keywords = {Software Process, Software Development, Software Development Environments, Social Fac- tors in Software Development, Empirical Studies, Agile Software De- velopment},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.5555/957289.957326,
author = {Ridjanovic, Dzenan},
title = {Pedagogical Tools for Database Design, Development and Use},
year = {2003},
isbn = {0954414519},
publisher = {Computer Science Press, Inc.},
address = {USA},
abstract = {I teach undergraduate and graduate (MBA) courses in information systems. My primary teaching objective is to help students design and develop dynamic web applications. My students are primarily business oriented; therefore I cannot spend too much time on technical details.A database is a very important part of any dynamic web application. It is also its most technical part. To hide the complexity of database technologies, I developed two tools: Magic Models to design a data model and generate a relational database schema, and Database Framework to allow an easy database access. Both tools are developed in Java. Magic Models is Swing based graphical tool and Database Framework uses JDBC for databases access and updates.},
booktitle = {Proceedings of the 2nd International Conference on Principles and Practice of Programming in Java},
pages = {121–124},
numpages = {4},
keywords = {code generation, data model, database framework},
location = {Kilkenny City, Ireland},
series = {PPPJ '03}
}

@inproceedings{10.1145/1363686.1364237,
author = {Baumgartner, Norbert and Retschitzegger, Werner and Schwinger, Wieland},
title = {A Software Architecture for Ontology-Driven Situation Awareness},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1364237},
doi = {10.1145/1363686.1364237},
abstract = {Human operators of large-scale control systems face the problem of information overload induced by the large amount of information provided by multiple heterogeneous and highly-dynamic information sources. Situation-aware information systems support operators by the aggregation of the available information to meaningful situations. Ontologies are a promising technology for realizing such systems, because of their semantically-rich kind of knowledge representation. The cross-cutting role of ontologies and the streaming character of situation awareness, however, challenge the design of an appropriate software architecture. In this paper, we propose a domain-independent software architecture based on a core ontology for situation awareness which leverages the reusability and the scalability of involved software components. This is achieved by the application of the well-known software architecture pattern pipes-and-filters. The proposed architecture is demonstrated by examples from the field of road traffic management. In addition, we contribute several lessons learned which should be helpful for developing ontology-driven information systems in general.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {2326–2330},
numpages = {5},
keywords = {ontologies, software architecture, situation awareness},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/2372251.2372274,
author = {Anh, Nguyen-Duc and Cruzes, Daniela S. and Conradi, Reidar},
title = {Dispersion, Coordination and Performance in Global Software Teams: A Systematic Review},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372274},
doi = {10.1145/2372251.2372274},
abstract = {Effective team coordination is crucial for successful global software projects. Although considerable research effort has been made in this area, no agreement has been reached on the influence of dispersion on team coordination and performance. The objective of this paper is to summarize the evidence on the relationship among context dispersion, team coordination and performance in global software projects. We have performed a Systematic literature review (SLR) to collect relevant studies and a thematic analysis to synthesize the extracted data. We found 28 primary studies reporting the impact of five dispersion dimensions on team performance. Previously, only two primary studies considered and distinguished all of these dispersion dimensions in studying dispersed team performance. The dispersion dimensions affect team outcomes indirectly through influencing organic and mechanistic coordination processes. Empirical evidence show that geographical dispersion impacts negatively and temporal dispersion has a mixed effect on team performance. While studies with teams working across different time zones shows a tendency that the team performance is pessimistically perceived, studies that use direct measure on task performance shows a positive association to temporal dispersion. The paper provides implications for future research and practitioners in establishing effective distributed team coordination.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {129–138},
numpages = {10},
keywords = {communication, performance, systematic literature review, global software development, team coordination, distribution},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/2038558.2038569,
author = {Zhu, Li and Vaghi, Ivan and Barricelli, Barbara Rita},
title = {A Meta-Reflective Wiki for Collaborative Design},
year = {2011},
isbn = {9781450309097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038558.2038569},
doi = {10.1145/2038558.2038569},
abstract = {This paper presents MikiWiki, a meta-reflective wiki developed to prototype key aspects of the Hive-Mind Space model. MikiWiki is aimed at supporting End-User Development activities and exploring the opportunities to enable software tailoring at use time. Such an open-ended collaborative design process is realized by providing basic boundary object prototypes, allowing end users to remix, modify, and create their own boundary objects. Moreover, MikiWiki minimizes essential services at the server-side, while putting the main functionalities on the client-side, opening the whole system to its users for further tailoring. In addition to traditional wikis, MikiWiki allows different Communities of Practice to collaboratively design and to continuously evolve the whole system. This approach illustrates the meta-design concept, where some software collaboration between professional developers and end users is made possible through communication channels properly associated with the environment. As such, the MikiWiki environment is presented as a 'concept demonstrator' for meta-design and end-user tailoring.},
booktitle = {Proceedings of the 7th International Symposium on Wikis and Open Collaboration},
pages = {53–62},
numpages = {10},
keywords = {meta-design, co-evolution, wiki, habitable environment, Hive-Mind Space model, mikinugget, boundary objects, MikiWiki, end-user development},
location = {Mountain View, California},
series = {WikiSym '11}
}

@inproceedings{10.1145/581339.581381,
author = {Michail, Amir},
title = {Browsing and Searching Source Code of Applications Written Using a GUI Framework},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581381},
doi = {10.1145/581339.581381},
abstract = {Nowadays, applications are typically written using an object-oriented GUI framework. In this paper we explore the possibility of using the GUI of such applications to guide browsing and search of their source code. Such a tool would be helpful for software maintenance and reuse, particularly when the application source is unfamiliar. Intuitively, we would expect the task of browsing and searching source code of an application written using a GUI framework to be easier than one that doesn't because the GUI framework imposes a structure on the application. Generally, the GUI framework is in control and makes calls into the application code to handle various events --- thus providing fundamental entry points into the application code, namely the callbacks. Of course, this is a property of frameworks in general but GUI frameworks have one additional advantage: the GUI is visible to the end-user and contains text messages describing what the application can do. Thus we have an explicit connection between an informal specification fragment visible in the GUI and its precise entry point to the implementation in the source. We demonstrate our approach, which takes advantage of this connection, on KDE applications written using the KDE GUI framework.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {327–337},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@proceedings{10.1145/3549035,
title = {MSR4P&amp;S 2022: Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security},
year = {2022},
isbn = {9781450394574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 1st International Workshop on Mining Software Repositories for Privacy and Security (MSR4P&amp;S 2022). MSR4P&amp;S is co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). This year, because of the Covid-19 pandemic, MSR4P&amp;S (as part of ESEC/FSE) is held virtually with an adapted program that will bring together international researchers to exchange ideas, share experiences, investigate problems, and propose promising solutions concerning the application of Mining Software Repositories (MSR) to investigate the different stages of privacy and security. The workshop topics cover a wide range of MSR applications for cybersecurity research, including empirical and mixed-method approaches, as well as datasets and tools.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2382196.2382215,
author = {Muthukumaran, Divya and Jaeger, Trent and Ganapathy, Vinod},
title = {Leveraging "Choice" to Automate Authorization Hook Placement},
year = {2012},
isbn = {9781450316514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382196.2382215},
doi = {10.1145/2382196.2382215},
abstract = {When servers manage resources on behalf of multiple, mutually-distrusting clients, they must mediate access to those resources to ensure that each client request complies with an authorization policy. This goal is typically achieved by placing authorization hooks at appropriate locations in server code. The goal of authorization hook placement is to completely mediate all security-sensitive operations on shared resources.To date, authorization hook placement in code bases, such as the X server and postgresql, has largely been a manual procedure, driven by informal analysis of server code and discussions on developer forums. Often, there is a lack of consensus about basic concepts, such as whatconstitutes a security-sensitive operation.In this paper, we propose an automated hook placement approach that is motivated by a novel observation --- that the deliberate choices made by clients for objects from server collections and for processing those objects must all be authorized. We have built a tool that uses this observation to statically analyze the server source. Using real-world examples (the X server and postgresql), we show that the hooks placed by our method are just as effective as hooks that were manually placed over the course of years while greatly reducing the burden on programmers.},
booktitle = {Proceedings of the 2012 ACM Conference on Computer and Communications Security},
pages = {145–156},
numpages = {12},
keywords = {authorization hooks, static taint analysis},
location = {Raleigh, North Carolina, USA},
series = {CCS '12}
}

@inproceedings{10.1145/3196398.3196439,
author = {Moslehi, Parisa and Adams, Bram and Rilling, Juergen},
title = {Feature Location Using Crowd-Based Screencasts},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196439},
doi = {10.1145/3196398.3196439},
abstract = {Crowd-based multi-media documents such as screencasts have emerged as a source for documenting requirements of agile software projects. For example, screencasts can describe buggy scenarios of a software product, or present new features in an upcoming release. Unfortunately, the binary format of videos makes traceability between the video content and other related software artifacts (e.g., source code, bug reports) difficult. In this paper, we propose an LDA-based feature location approach that takes as input a set of screencasts (i.e., the GUI text and/or spoken words) to establish traceability link between the features described in the screencasts and source code fragments implementing them. We report on a case study conducted on 10 WordPress screencasts, to evaluate the applicability of our approach in linking these screencasts to their relevant source code artifacts. We find that the approach is able to successfully pinpoint relevant source code files at the top 10 hits using speech and GUI text. We also found that term frequency rebalancing can reduce noise and yield more precise results.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {192–202},
numpages = {11},
keywords = {information extraction, crowd-based documentation, mining video content, software traceability, feature location},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00039,
author = {Han, Junxiao and Deng, Shuiguang and Lo, David and Zhi, Chen and Yin, Jianwei and Xia, Xin},
title = {An Empirical Study of the Landscape of Open Source Projects in Baidu, Alibaba, and Tencent},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00039},
doi = {10.1109/ICSE-SEIP52600.2021.00039},
abstract = {Open source software has drawn more and more attention from researchers, developers and companies nowadays. Meanwhile, many Chinese technology companies are embracing open source and choosing to open source their projects. Nevertheless, most previous studies are concentrated on international companies such as Microsoft or Google, while the practical values of open source projects of Chinese technology companies remain unclear. To address this issue, we conduct a mixed-method study to investigate the landscape of projects open sourced by three large Chinese technology companies, namely Baidu, Alibaba, and Tencent (BAT). We study the categories and characteristics of open source projects, the developer's perceptions towards open sourcing effort for these companies, and the internationalization effort of their open source projects. We collected 1,000 open source projects that were open sourced by BAT in GitHub and performed an online survey that received 101 responses from developers of these projects. Some key findings include: 1) BAT prefer to open source frontend development projects, 2) 88% of the respondents are positive towards open sourcing software projects in their respective companies, 3) 64% of the respondents reveal that the most common motivations for BAT to open source their projects are the desire to gain fame, expand their influence and gain recruitment advantage, 4) respondents believe that the most common internationalization effort is "providing an English version of readme files", 5) projects with more internationalization effort (i.e., include an English readme file) are more popular. Our findings provide directions for software engineering researchers and provide practical suggestions to software developers and Chinese technology companies.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {298–307},
numpages = {10},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/1985441.1985479,
author = {Wang, Xinlei (Oscar) and Baik, Eilwoo and Devanbu, Premkumar T.},
title = {Operating System Compatibility Analysis of Eclipse and Netbeans Based on Bug Data},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985479},
doi = {10.1145/1985441.1985479},
abstract = {Eclipse and Netbeans are two top of the line Integrated Development Environments (IDEs) for Java development. Both of them provide support for a wide variety of development tasks and have a large user base. This paper provides an analysis and comparison for the compatibility and stability of Eclipse and Netbeans on the three most commonly used operating systems, Windows, Linux and Mac OS. Both IDEs are programmed in Java and use a Bugzilla issue tracker to track reported bugs and feature requests. We looked into the Bugzilla repository databases of these two IDEs, which contains the bug records and histories of these two IDEs. We used some basic data mining techniques to analyze some historical statistics of the bug data. Based on the analysis, we try to answer certain stability-comparison oriented questions in the paper, so that users can have a better idea which of these two IDEs is designed better to work on different platforms.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {230–233},
numpages = {4},
keywords = {operating systems, bugs, eclipse, stability, netbeans},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/1183316.1183322,
author = {Allen, Thomas J.},
title = {The Passion of the Developer: Ea_spouse in the H_ouse! A Panel on Labor Relations and Quality of Life in the Industry},
year = {2006},
isbn = {1595933867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183316.1183322},
doi = {10.1145/1183316.1183322},
abstract = {Not much has changed in two years. A large portion of the video game industry honestly believes that poor quality of life is a requirement for the business. They have publicly argued that game developers are "passionate" people; that "passion" can only manifest itself in relentless schedules and minimal compensation, like a deranged high school football coach screaming at a player to make a touchdown with a broken leg. They insist that if scheduling or compensation models changed, the passion would evaporate - that suffering is necessary for the industry's existence.},
booktitle = {Proceedings of the 2006 ACM SIGGRAPH Symposium on Videogames},
pages = {29–40},
numpages = {12},
location = {Boston, Massachusetts},
series = {Sandbox '06}
}

@inproceedings{10.1145/3579856.3595799,
author = {Abbadini, Marco and Facchinetti, Dario and Oldani, Gianluca and Rossi, Matthew and Paraboschi, Stefano},
title = {Cage4Deno: A Fine-Grained Sandbox for Deno Subprocesses},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3595799},
doi = {10.1145/3579856.3595799},
abstract = {Deno is a runtime for JavaScript and TypeScript that is receiving great interest by developers, and is increasingly used for the construction of back-ends of web applications. A primary goal of Deno is to provide a secure and isolated environment for the execution of JavaScript programs. It also supports the execution of subprocesses, unfortunately without providing security guarantees. In this work we propose Cage4Deno, a set of modifications to Deno enabling the creation of fine-grained sandboxes for the execution of subprocesses. The design of Cage4Deno satisfies the compatibility, transparency, flexibility, usability, security, and performance needs of a modern sandbox. The realization of these requirements partially stems from the use of Landlock and eBPF, two robust and efficient security technologies. Significant attention has been paid to the design of a flexible and compact policy model consisting of RWX permissions, which can be automatically created, and deny rules to declare exceptions. The sandbox effectiveness is demonstrated by successfully blocking a number of exploits for recent CVEs, while runtime experiments prove its efficiency. The proposal is associated with an open-source implementation.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {149–162},
numpages = {14},
keywords = {Sandboxing, Deno, Subprocess, JavaScript runtime, Access control},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@inproceedings{10.1145/3364641.3364650,
author = {Favato, Danilo and Ishitani, Daniel and Oliveira, Johnatan and Figueiredo, Eduardo},
title = {Linus's Law: More Eyes Fewer Flaws in Open Source Projects},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364650},
doi = {10.1145/3364641.3364650},
abstract = {Linus's Law states that "given enough eyeballs, all bugs are shallow". In other words, given a large enough number of developers, almost every programming flaw is characterized and fixed quickly. Although there is much debate about this subject, we still lack empirical evidence to support this law. Given that this theme has, and still is, motivating business decisions in software development, we investigate the implications of Linus's Law in two empirical studies on open source projects mined from GitHub. In the first pilot study, we mined seven popular Java projects from GitHub and investigated the correlation between committers and programming flaws in source code files. Results of this pilot study suggest a positive correlation between the number of developers and programming flaws. We cross-validate these results in a second study with almost one hundred Python projects from GitHub. In this second study, we analyzed the correlation between the number of forks - i.e., a proxy for number of developers - and programming flaws identified in projects. In both studies, programming flaws were detected by using static code analysis tools. As a result of the second study, we could not observe a correlation between the number of developers and the number of programming flaws in Python projects. From both studies we conclude that we were unable to find evidence to support the Linus's Law.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {69–78},
numpages = {10},
keywords = {python, software quality, Linus's law, java},
location = {Fortaleza, Brazil},
series = {SBQS '19}
}

@inproceedings{10.1145/3494193.3494298,
author = {Severo Leite da Silva, Alandey and Teixeira, Antonio Alberto and Carvalho Ramos Cavalcanti, Tatiana},
title = {The Digital Divide and the Smart Digital Governance in Brazil: Tensions between the Skills of Multiple Policy Areas and the Needs of Society},
year = {2022},
isbn = {9781450390118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494193.3494298},
doi = {10.1145/3494193.3494298},
abstract = {In addition to the technical discourse of reports and studies from the most important nations and world organizations that highlighted that greater adoption and use of Information and Communication Technologies (ICT) can contribute to growth, development and overcoming the various difficulties, researches have presented the digital divide as a reality and responsibility that goes beyond being "connected" or "disconnected" to resources and innovations in information technology and communication. This study aims to provide an overview of the tensions between the competencies of the multiple political domains, and some of the needs of society that contribute to the digital divide and demand efficient practices of Smart Digital Governance in Brazil. To support this analysis, a literature review and a Framework [1] for the study of Tensions based on Information Infrastructure Theory will be adopted. As for future studies, it is proposed to investigate the approaches presented in smart digital governance practices and the state of the digital divide in developed or developing countries.},
booktitle = {Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance},
pages = {129–135},
numpages = {7},
keywords = {Tensions, Framework, Smart Digital Governance, Digital Divide, Information Infrastructures},
location = {Athens, Greece},
series = {ICEGOV '21}
}

@inproceedings{10.5555/2820518.2820559,
author = {White, Martin and Vendome, Christopher and Linares-V\'{a}squez, Mario and Poshyvanyk, Denys},
title = {Toward Deep Learning Software Repositories},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyperparameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {334–345},
numpages = {12},
keywords = {machine learning, software language models, deep learning, neural networks, n-grams, software repositories},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1145/3386327,
author = {Wirfs-Brock, Allen and Eich, Brendan},
title = {JavaScript: The First 20 Years},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386327},
doi = {10.1145/3386327},
abstract = {How a sidekick scripting language for Java, created at Netscape in a ten-day hack, ships first as a de facto Web standard and eventually becomes the world's most widely used programming language. This paper tells the story of the creation, design, evolution, and standardization of the JavaScript language over the period of 1995--2015. But the story is not only about the technical details of the language. It is also the story of how people and organizations competed and collaborated to shape the JavaScript language which dominates the Web of 2020.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {77},
numpages = {189},
keywords = {JavaScript, Web browsers, History of programming languages, Standards, ECMAScript, Browser game theory}
}

@inproceedings{10.1145/2076674.2076678,
author = {Nystrom, Nathaniel},
title = {Harmless Compiler Plugins},
year = {2011},
isbn = {9781450308939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2076674.2076678},
doi = {10.1145/2076674.2076678},
abstract = {Languages such as Java and Scala allow programmers to write compiler extensions, or plugins, that extend the host programming language with new functionality to enable additional static checking and code transformations.However, by permitting arbitrary code transformations, compiler plugins can change the host language semantics in unexpected ways. Moreover, plugins do not compose. Plugins can interfere with each other such that one plugin can undo the effects of another, or worse, cause another plugin to generate incorrect code.In this paper, we develop a theoretical framework for harmless compiler plugins. Host language programs are annotated to limit the scope of plugins. Plugins may change the termination behavior of code outside these scopes, but they are prohibited from changing the values computed by the original computation. The framework is based on an extension of Welterweight Java and uses an information-flow type system to limit plugin effects.},
booktitle = {Proceedings of the 13th Workshop on Formal Techniques for Java-Like Programs},
articleno = {4},
numpages = {6},
location = {Lancaster, United Kingdom},
series = {FTfJP '11}
}

@inproceedings{10.1145/3544902.3546634,
author = {Rahman, Md. Masudur and Satter, Abdus and Joarder, Md. Mahbubul Alam and Sakib, Kazi},
title = {An Empirical Study on the Occurrences of Code Smells in Open Source and Industrial Projects},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546634},
doi = {10.1145/3544902.3546634},
abstract = {Background: Reusing source code containing code smells can induce significant amount of maintenance time and cost. A list of code smells has been identified in the literature and developers are encouraged to avoid the smells from the very beginning while writing new code or reusing existing code, and it increases time and cost to identify and refactor the code after the development of a system. Again, remembering a long list of smells is difficult specially for the new developers. Besides, two different types of software development environment - open source and industry, might have an effect on the occurrences of code smells. Aims: A study on the occurrences of code smells in open source and industrial systems can provide insights about the most frequently occurring smells in each type of software system. The insights can make developers aware of the most frequent occurring smells, and researchers to focus on the improvement and innovation of automatic refactoring tools or techniques for the smells on priority basis. Method: We have conducted a study on 40 large scale Java systems, where 25 are open source and 15 are industrial systems, for 18 code smells. Results: The results show that 6 smells have not occurred in any system, and 12 smells have occurred 21,182 times in total where 60.66% in the open source systems and 39.34% in the industrial systems. Long Method, Complex Class and Long Parameter List have been seen as frequently occurring code smells. The one tailed t-test with 5% level of significant analysis has shown that there is no difference between the occurrences of 10 code smells in industrial and open source systems, and 2 smells are occurred more frequently in open source systems than industrial systems. Conclusions: Our findings conclude that all smells do not occur at the same frequency and some smells are very frequent. The short list of most frequently occurred smells can help developers to write or reuse source code carefully without inducing the smells from the beginning during software development. Our study also concludes that industry and open source environments do not have significant impact on the occurrences of code smells.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {289–294},
numpages = {6},
keywords = {code smell, open source system, empirical study, industrial system},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@article{10.1145/3349589,
author = {Monperrus, Martin and Urli, Simon and Durieux, Thomas and Martinez, Matias and Baudry, Benoit and Seinturier, Lionel},
title = {Repairnator Patches Programs Automatically},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2019},
number = {July},
url = {https://doi.org/10.1145/3349589},
doi = {10.1145/3349589},
abstract = {Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds in synthesizing a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to produce patches that were accepted by the human developers and permanently merged into the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.},
journal = {Ubiquity},
month = {jul},
articleno = {2},
numpages = {12}
}

@inbook{10.1145/2886107.2886114,
title = {Conclusion},
year = {2018},
isbn = {9781970001570},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2886107.2886114},
abstract = {Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines.At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too.This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing.We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective.This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.},
booktitle = {An Architecture for Fast and General Data Processing on Large Clusters}
}

@inproceedings{10.1145/2647868.2655064,
author = {Wyse, Lonce},
title = {Interactive Audio Web Development Workflow},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2655064},
doi = {10.1145/2647868.2655064},
abstract = {New low-level sound synthesis capabilities have recently become available in Web browsers. However, there is a considerable gap between the enabling technology for interactive audio and its wide-spread adoption in Web media content. We identify several areas where technologies are necessary to support the various stages of development and deployment, describe systems we have developed to address those needs, and show how they work together within a specific Web content development scenario.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {1065–1068},
numpages = {4},
keywords = {w3c web audio api, interactive audio, web development workflow, development tools},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.5555/1234341.1234350,
author = {Yue, Billy and de-Byl, Penny},
title = {The State of the Art in Game AI Standardisation},
year = {2006},
isbn = {869059017},
publisher = {Murdoch University},
address = {Murdoch, AUS},
abstract = {The Artificial Intelligence Interface Standards Committee (AIISC) was formed in 2002 to develop interface standards for AI middleware in the computer game industry. In this paper, we provide an overview of the work completed to date by the working groups making up the AIISC. We describe the issues each group is addressing and examine their relevance and importance to the domain of computer games development, and discuss their benefits and shortcomings. We also outline the current progress of each group, and the direction they will be taking in the future in order to implement a useable and widely accepted game AI API.},
booktitle = {Proceedings of the 2006 International Conference on Game Research and Development},
pages = {41–46},
numpages = {6},
keywords = {AIISC, API},
location = {Perth, Australia},
series = {CyberGames '06}
}

@inproceedings{10.1145/3183713.3190664,
author = {Armbrust, Michael and Das, Tathagata and Torres, Joseph and Yavuz, Burak and Zhu, Shixiong and Xin, Reynold and Ghodsi, Ali and Stoica, Ion and Zaharia, Matei},
title = {Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3190664},
doi = {10.1145/3183713.3190664},
abstract = {With the ubiquity of real-time data, organizations need streaming systems that are scalable, easy to use, and easy to integrate into business applications. Structured Streaming is a new high-level streaming API in Apache Spark based on our experience with Spark Streaming. Structured Streaming differs from other recent streaming APIs, such as Google Dataflow, in two main ways. First, it is a purely declarative API based on automatically incrementalizing a static relational query (expressed using SQL or DataFrames), in contrast to APIs that ask the user to build a DAG of physical operators. Second, Structured Streaming aims to support end-to-end real-time applications that integrate streaming with batch and interactive analysis. We found that this integration was often a key challenge in practice. Structured Streaming achieves high performance via Spark SQL's code generation engine and can outperform Apache Flink by up to 2x and Apache Kafka Streams by 90x. It also offers rich operational features such as rollbacks, code updates, and mixed streaming/batch execution. We describe the system's design and use cases from several hundred production deployments on Databricks, the largest of which process over 1 PB of data per month.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {601–613},
numpages = {13},
keywords = {stream processing, apache spark, programming models},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/1772690.1772700,
author = {Barton, John J. and Odvarko, Jan},
title = {Dynamic and Graphical Web Page Breakpoints},
year = {2010},
isbn = {9781605587998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1772690.1772700},
doi = {10.1145/1772690.1772700},
abstract = {Breakpoints are perhaps the quintessential feature of a de-bugger: they allow a developer to stop time and study the program state. Breakpoints are typically specified by selecting a line of source code. For large, complex, web pages with multiple developers, the relevant source line for a given user interface problem may not be known to the developer. In this paper we describe the implementation of breakpoints in dynamically created source, and on error messages, network events, DOMmutation, DOMobject property changes, and CSS style rule updates. Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views. The breakpoints are implemented in the open source Fire-bug project, version 1.5, for the Firefox Web browser.},
booktitle = {Proceedings of the 19th International Conference on World Wide Web},
pages = {81–90},
numpages = {10},
keywords = {web, dynamic, javascript, debugging, html, breakpoints, firebug, css},
location = {Raleigh, North Carolina, USA},
series = {WWW '10}
}

@inproceedings{10.1145/2155620.2155651,
author = {Li, Sheng and Lim, Kevin and Faraboschi, Paolo and Chang, Jichuan and Ranganathan, Parthasarathy and Jouppi, Norman P.},
title = {System-Level Integrated Server Architectures for Scale-out Datacenters},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155651},
doi = {10.1145/2155620.2155651},
abstract = {A System-on-Chip (SoC) integrates multiple discrete components into a single chip, for example by placing CPU cores, network interfaces and I/O controllers on the same die. While SoCs have dominated high-end embedded products for over a decade, system-level integration is a relatively new trend in servers, and is driven by the opportunity to lower cost (by reducing the number of discrete parts) and power (by reducing the pin crossings from the cores to the I/O). Today, the mounting cost pressures in scale-out dat-acenters demand technologies that can decrease the Total Cost of Ownership (TCO). At the same time, the diminshing return of dedicating the increasing number of available transistors to more cores and caches is creating a stronger case for SoC-based servers.This paper examines system-level integration design options for the scale-out server market, specifically targeting datacenter-scale throughput computing workloads. We develop tools to model the area and power of a variety of discrete and integrated server configurations. We evaluate the benefits, trade-offs, and trends of system-level integration for warehouse-scale datacenter servers, and identify the key "uncore" components that reduce cost and power. We perform a comprehensive design space exploration at both SoC and datacenter level, identify the sweet spots, and highlight important scaling trends of performance, power, area, and cost from 45nm to 16nm. Our results show that system integration yields substantial benefits, enables novel aggregated configurations with a much higher compute density, and significantly reduces total chip area and dynamic power versus a discrete-component server.Finally, we use utilization traces and architectural profiles of real machines to evaluate the dynamic power consumption of typical scale-out cloud applications, and combine them in an overall TCO model. Our results show that, for example at 16nm, SoC-based servers can achieve more than a 26% TCO reduction at datacenter scale.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {260–271},
numpages = {12},
keywords = {server, TCO, datacenter, system-on-chip, cost},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}

@inproceedings{10.1145/3472749.3474819,
author = {Guo, Philip},
title = {Ten Million Users and Ten Years Later: Python Tutor’s Design Guidelines for Building Scalable and Sustainable Research Software in Academia},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474819},
doi = {10.1145/3472749.3474819},
abstract = {Research software is often built as prototypes that never get widespread usage and are left unmaintained after a few papers get published. To counteract this trend, we propose a method for building research software with scale and sustainability in mind so that it can organically grow a large userbase and enable longer-term research. To illustrate this method, we present the design and implementation of Python Tutor (pythontutor.com), a code visualization tool that is, to our knowledge, one of the most widely-used pieces of research software developed within a university lab. Over the past decade, it has been used by over ten million people in over 180 countries. It has also contributed to 55 publications from 35 research groups in 13 countries. We distilled lessons from working on Python Tutor into three sets of design guidelines: 1) user experience design for scale and sustainability, 2) software architecture design for long-term sustainability, and 3) designing a sustainable software development workflow within academia. These guidelines can enable a student to create long-lasting software that reaches many users and facilitates research from many independent groups.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {1235–1251},
numpages = {17},
keywords = {sustainability, research software, Python Tutor, code visualization},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.1145/3441643,
author = {Rottleuthner, Michel and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {Sense Your Power: The ECO Approach to Energy Awareness for IoT Devices},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/3441643},
doi = {10.1145/3441643},
abstract = {Energy-constrained sensor nodes can adaptively optimize their energy consumption if a continuous measurement is provided. This is of particular importance in scenarios of high dynamics such as with energy harvesting. Still, self-measuring of power consumption at reasonable cost and complexity is unavailable as a generic system service.In this article, we present ECO, a hardware-software co-design that adds autonomous energy management capabilities to a large class of low-end IoT devices. ECO consists of a highly portable hardware shield built from inexpensive commodity components and software integrated into the RIOT operating system. RIOT supports more than 200 popular microcontrollers. Leveraging this flexibility, we assembled a variety of sensor nodes to evaluate key performance properties for different device classes. An overview and comparison with related work shows how ECO fills the gap of in situ power attribution transparently for consumers and how it improves over existing solutions. We also report about two different real-world field trials, which validate our solution for long-term production use.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {mar},
articleno = {24},
numpages = {25},
keywords = {power measurement, Energy harvesting, energy management, IoT operating system}
}

@article{10.14778/3352063.3352139,
author = {Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Schuster, Martin and Selmer, Petra and Voigt, Hannes},
title = {Updating Graph Databases with Cypher},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352139},
doi = {10.14778/3352063.3352139},
abstract = {The paper describes the present and the future of graph updates in Cypher, the language of the Neo4j property graph database and several other products. Update features include those with clear analogs in relational databases, as well as those that do not correspond to any relational operators. Moreover, unlike SQL, Cypher updates can be arbitrarily intertwined with querying clauses. After presenting the current state of update features, we point out their shortcomings, most notably violations of atomicity and non-deterministic behavior of updates. These have not been previously known in the Cypher community. We then describe the industry-academia collaboration on designing a revised set of Cypher update operations. Based on discovered shortcomings of update features, a number of possible solutions were devised. They were presented to key Cypher users, who were given the opportunity to comment on how update features are used in real life, and on their preferences for proposed fixes. As the result of the consultation, a new set of update operations for Cypher were designed. Those led to a streamlined syntax, and eliminated the unexpected and problematic behavior that original Cypher updates exhibited.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2242–2254},
numpages = {13}
}

@inproceedings{10.1145/3448016.3457552,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457552},
doi = {10.1145/3448016.3457552},
abstract = {Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14},
keywords = {streaming processing, real-time infrastructure},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1109/SESoS/WDES.2019.00009,
author = {Park, Sumin and Mihret, B. Zelalem and Bae, Doo-Hwan},
title = {A Simulation-Based Behavior Analysis for MCI Response System of Systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SESoS/WDES.2019.00009},
doi = {10.1109/SESoS/WDES.2019.00009},
abstract = {A System of systems (SoS) vastly differs from conventional systems, both in structure and means of goal achievement. Structurally, an SoS contains autonomous systems which are managerially and operationally independent. The autonomous systems composing an SoS, commonly referred as constituent systems (CSs), interact each other to achieve common goals. With regard to means to goal achievement, SoS relies on each CSs' roles and assigned responsibilities. Due to the complex and characteristics of SoS, there still remains additional efforts to deal and address the challenges posed in the modeling and analysis of SoS behavior. In this paper, we presented an SoS behavior analysis approach via simulation. Our behavior analysis approach is similar to that of chaos engineering in that we inject stimuli into the system and then analyze the system behavior. Our simulation engine is based on discrete time multi-agent simulation. In our experiment, to mimic the real-world phenomenon into the simulation, we identified the real-world events that occurred in the real-world Mass Casualty Incident (MCI) response SoS. With the identified real-world events, we defined stimuli which can represent the real-world events and issues in real-world MCI. The defined stimuli are injected into the developed simulation to mimic the real-world MCI response case in practice.},
booktitle = {Proceedings of the 7th International Workshop on Software Engineering for Systems-of-Systems and 13th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {2–9},
numpages = {8},
keywords = {stimulus, mass casualty incident, constituent system, system of systems, simulation, inject},
location = {Montreal, Quebec, Canada},
series = {SESoS-WDES '19}
}

@inproceedings{10.1145/2818567.2818583,
author = {Panigrahy, Sandeep and Karpate, Sarang and Wahile, Saurabh},
title = {HYFRAMAL: An Innovative Automation Framework for Internet of Things},
year = {2015},
isbn = {9781450335522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818567.2818583},
doi = {10.1145/2818567.2818583},
abstract = {With the surge of cheaper electronics and newer computing technologies, the dream of an ideal smart automation system is slowly being realized. However, despite many proposals for a smart automation system, a system that balances both complexity and flexibility is not yet presented. In this paper, such a solution is proposed that enables the smart automation system to take complex decisions like machine learning routines despite being based on the lightweight HTTP system. The system also has unique features such as authentication, to ensure security and a flexible structure using RESTful API, so that new functionalities can be easily added. Finally, the system caters to a wide variety of users and developers.},
booktitle = {Proceedings of the Sixth International Conference on Computer and Communication Technology 2015},
pages = {86–90},
numpages = {5},
keywords = {machine learning, smart homes, internet of things, automation, framework, K-Means algorithm, REST},
location = {Allahabad, India},
series = {ICCCT '15}
}

@inproceedings{10.1145/2810103.2813690,
author = {Liu, Yutao and Zhou, Tianyu and Chen, Kexin and Chen, Haibo and Xia, Yubin},
title = {Thwarting Memory Disclosure with Efficient Hypervisor-Enforced Intra-Domain Isolation},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813690},
doi = {10.1145/2810103.2813690},
abstract = {Exploiting memory disclosure vulnerabilities like the HeartBleed bug may cause arbitrary reading of a victim's memory, leading to leakage of critical secrets such as crypto keys, personal identity and financial information. While isolating code that manipulates critical secrets into an isolated execution environment is a promising countermeasure, existing approaches are either too coarse-grained to prevent intra-domain attacks, or require excessive intervention from low-level software (e.g., hypervisor or OS), or both. Further, few of them are applicable to large-scale software with millions of lines of code. This paper describes a new approach, namely SeCage, which retrofits commodity hardware virtualization extensions to support efficient isolation of sensitive code manipulating critical secrets from the remaining code. SeCage is designed to work under a strong adversary model where a victim application or even the OS may be controlled by the adversary, while supporting large-scale software with small deployment cost. SeCage combines static and dynamic analysis to decompose monolithic software into several compart- ments, each of which may contain different secrets and their corresponding code. Following the idea of separating control and data plane, SeCage retrofits the VMFUNC mechanism and nested paging in Intel processors to transparently provide different memory views for different compartments, while allowing low-cost and transparent invocation across domains without hypervisor intervention.We have implemented SeCage in KVM on a commodity Intel machine. To demonstrate the effectiveness of SeCage, we deploy it to the Nginx and OpenSSH server with the OpenSSL library as well as CryptoLoop with small efforts. Security evaluation shows that SeCage can prevent the disclosure of private keys from HeartBleed attacks and memory scanning from rootkits. The evaluation shows that SeCage only incurs small performance and space overhead.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1607–1619},
numpages = {13},
keywords = {virtualization, privacy protection, memory disclosure},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@article{10.14778/2367502.2367523,
author = {Ports, Dan R. K. and Grittner, Kevin},
title = {Serializable Snapshot Isolation in PostgreSQL},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367523},
doi = {10.14778/2367502.2367523},
abstract = {This paper describes our experience implementing PostgreSQL's new serializable isolation level. It is based on the recently-developed Serializable Snapshot Isolation (SSI) technique. This is the first implementation of SSI in a production database release as well as the first in a database that did not previously have a lock-based serializable isolation level. We reflect on our experience and describe how we overcame some of the resulting challenges, including the implementation of a new lock manager, a technique for ensuring memory usage is bounded, and integration with other PostgreSQL features. We also introduce an extension to SSI that improves performance for read-only transactions. We evaluate PostgreSQL's serializable isolation level using several benchmarks and show that it achieves performance only slightly below that of snapshot isolation, and significantly outperforms the traditional two-phase locking approach on read-intensive workloads.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1850–1861},
numpages = {12}
}

@article{10.1145/3154502,
author = {Schardl, Tao B. and Denniston, Tyler and Doucet, Damon and Kuszmaul, Bradley C. and Lee, I-Ting Angelina and Leiserson, Charles E.},
title = {The CSI Framework for Compiler-Inserted Program Instrumentation},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3154502},
doi = {10.1145/3154502},
abstract = {The CSI framework provides comprehensive static instrumentation that a compiler can insert into a program-under-test so that dynamic-analysis tools - memory checkers, race detectors, cache simulators, performance profilers, code-coverage analyzers, etc. - can observe and investigate runtime behavior. Heretofore, tools based on compiler instrumentation would each separately modify the compiler to insert their own instrumentation. In contrast, CSI inserts a standard collection of instrumentation hooks into the program-under-test. Each CSI-tool is implemented as a library that defines relevant hooks, and the remaining hooks are "nulled" out and elided during either compile-time or link-time optimization, resulting in instrumented runtimes on par with custom instrumentation. CSI allows many compiler-based tools to be written as simple libraries without modifying the compiler, lowering the bar for the development of dynamic-analysis tools.We have defined a standard API for CSI and modified LLVM to insert CSI hooks into the compiler's internal representation (IR) of the program. The API organizes IR objects - such as functions, basic blocks, and memory accesses - into flat and compact ID spaces, which not only simplifies the building of tools, but surprisingly enables faster maintenance of IR-object data than do traditional hash tables. CSI hooks contain a "property" parameter that allows tools to customize behavior based on static information without introducing overhead. CSI provides "forensic" tables that tools can use to associate IR objects with source-code locations and to relate IR objects to each other.To evaluate the efficacy of CSI, we implemented six demonstration CSI-tools. One of our studies shows that compiling with CSI and linking with the "null" CSI-tool produces a tool-instrumented executable that is as fast as the original uninstrumented code. Another study, using a CSI port of Google's ThreadSanitizer, shows that the CSI-tool rivals the performance of Google's custom compiler-based implementation. All other demonstration CSI tools slow down the execution of the program-under-test by less than 70%.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {43},
numpages = {25},
keywords = {compiler-inserted instrumentation, dynamic program analysis, program instrumentation}
}

@inproceedings{10.1145/940880.940883,
author = {Penas, Juan Jos\'{e} S\'{a}nchez and Ramiro, Carlos Abalde},
title = {Extending the VoDKA Architecture to Improve Resource Modelling},
year = {2003},
isbn = {1581137729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/940880.940883},
doi = {10.1145/940880.940883},
abstract = {VoDKA is a Video-on-Demand server developed using Erlang/OTP. In this paper, the evolution of the core architecture of the system, designed for improving resource modelling, is described. After explaining the main goals of the project, the steps taken towards an optimal architecture are explained. Finally, a new architecture is proposed, solving all the problems and limitations in the previous ones. Special attention is paid to the use of design patterns, implementation behaviours, and reusable software components.},
booktitle = {Proceedings of the 2003 ACM SIGPLAN Workshop on Erlang},
pages = {15–22},
numpages = {8},
keywords = {functional programming, design patterns, multimedia servers, distributed computing, cluster computing},
location = {Uppsala, Sweden},
series = {ERLANG '03}
}

@inproceedings{10.1145/3579027.3608984,
author = {Krieter, Sebastian and Kr\"{u}ger, Jacob and Leich, Thomas and Saake, Gunter},
title = {VariantInc: Automatically Pruning and Integrating Versioned Software Variants},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608984},
doi = {10.1145/3579027.3608984},
abstract = {Developers use version-control systems and software-hosting platforms to manage their software systems. They rely on the provided branching and forking mechanisms to implement new features, fix bugs, and develop customized system variants. A particular problem arises when forked variants are not re-integrated (i.e., merged), but kept and co-evolved as individual systems. This can cause maintenance overheads, due to change propagation and limitations in simultaneously managing variations in space (variants) and time (revisions). Thus, most organizations decide to integrate their set of variants into a single platform at some point, and several techniques have been proposed to semi-automate such an integration. However, existing techniques usually consider only a single revision of each variant and do not merge the revision histories, disregarding that not only variants (i.e., configuring the features of the system) but also revisions (i.e., checking out specific versions of the features) are important. We propose an automated technique, VariantInc, for analyzing, pruning, and integrating variants of a system that also merges the revision history of each variant into the resulting platform (i.e., using presence conditions). To validate VariantInc, we employed it on 160 open-source C systems of various sizes (i.e., number of forks, revisions, source code). The results show that VariantInc works as intended, and allows developers or researchers to automatically integrate variants into a platform as well as to perform software analyses.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {129–140},
numpages = {12},
keywords = {Variant-rich systems, Version control, Forks, Variant integration},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@proceedings{10.1145/3524614,
title = {IWSiB '22: Proceedings of the 5th International Workshop on Software-Intensive Business: Towards Sustainable Software Business},
year = {2022},
isbn = {9781450393027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {There are many researchers and practitioners whose work is related to the field of software-intensive business. However, they are often not fully aware of each other's work as the research is scattered. For example, individual research contributions have emerged related to, for example, software engineering economics, digital ecosystems and software startups. The goal of the workshop on Software-intensive Business is to bring these different sub-fields together and strengthen their ties.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3442391.3442410,
author = {Pett, Tobias and Krieter, Sebastian and Runge, Tobias and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {Stability of Product-Line Samplingin Continuous Integration},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442410},
doi = {10.1145/3442391.3442410},
abstract = {Companies strive to implement continuous integration into their development process to ensure the quality of their systems. Regression testing within the CI process considers the efficient re-test of systems after changes. However, even with regression testing, it is not feasible to test all configurations from a highly-configurable software system due to the combinatorial-explosion problem. Numerous sampling algorithms have been proposed that aim at computing a considerably smaller yet sufficiently representative set of configurations to be tested. Those algorithms are typically evaluated with regard to efficiency (i.e., number of configurations in a sample and computational effort for generating a sample) and effectiveness (i.e., feature-interaction coverage or number of faults detected). In this paper, we argue that a further crucial characteristic of sampling algorithms is their tendency to produce similar configurations when applied consecutively to an evolving configurable system. We propose sampling stability as a new evaluation criterion for sampling algorithms. We present a procedure to compute the sampling stability of sampling algorithms based on the similarity between consecutive samples. In our evaluation, we compare the sampling stability of multiple established t-wise sampling algorithms on large real-world systems.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {18},
numpages = {9},
keywords = {sampling, product-line evolution, product lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/2384592.2384597,
author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
title = {Socio-PLT: Principles for Programming Language Adoption},
year = {2012},
isbn = {9781450315623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384592.2384597},
doi = {10.1145/2384592.2384597},
abstract = {Why do some programming languages fail and others succeed? What does the answer tell us about programming language design, implementation, and principles? To help answer these and other questions, we argue for examining the sociological groundings of programming language theory: socio-PLT.Researchers in the social sciences have studied adoption in many contexts. We show how their findings are applicable to programming language design. For example, many programming language features provide benefits that programmers cannot directly or immediately observe and therefore may not find compelling. From clean water to safe sex, the health community has repeatedly identified and surmounted similar observability barriers. We use such results from outside of programming language theory to frame a research agenda that should help us understand the social foundations of languages. Finally, we examine implications of our approach, such as for the design space of language features and the assessment of scientific research into programming languages.},
booktitle = {Proceedings of the ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {39–54},
numpages = {16},
keywords = {sociology, programming language adoption, history},
location = {Tucson, Arizona, USA},
series = {Onward! 2012}
}

@inproceedings{10.1145/2950290.2983955,
author = {Zhang, Hongyu and Jain, Anuj and Khandelwal, Gaurav and Kaushik, Chandrashekhar and Ge, Scott and Hu, Wenxiang},
title = {Bing Developer Assistant: Improving Developer Productivity by Recommending Sample Code},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983955},
doi = {10.1145/2950290.2983955},
abstract = {In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface (API) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant (BDA), which improves developer productivity by recommending sample code mined from public software repositories (such as GitHub) and web pages (such as Stack Overflow). BDA can automatically mine code snippets that implement an API or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial release in December 2014. BDA is publicly available at: http://aka.ms/devassistant.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {956–961},
numpages = {6},
keywords = {API Usage Extraction, Software Reuse, Code Search, API, GitHub},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/2811681.2817757,
author = {Yusop, Nor Shahida Mohamad},
title = {Understanding Usability Defect Reporting in Software Defect Repositories},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2817757},
doi = {10.1145/2811681.2817757},
abstract = {Software defect management is a critical component of good software engineering practice. The information reported about a defect is a key element to ensure defects are rectified effectively. However, based on research, reporting usability defects using an existing defect tracking system (DTS) is impractical. This is due to text-centric design and lack of features to support usability attributes. In addition, not all defects can be explained textually; especially defects that involve interface redesign. Another aspect to consider is that the reporters describe usability defects based on their usability knowledge and the information available at the time the defects are found. Defects stored in a DTS in a universal format. Therefore, when reporting usability defects there are some possibilities: the data may not be relevant or irrelevant, useful or not useful, or may even be beyond the reporter's knowledge. This makes it impossible to submit a high quality defect report. To address these issues, I propose a custom defect template that can adjust defect form according to whom, when and how the defect is found. In this way, it will provide flexibility to the reporters to record data based on their expertise and knowledge.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {134–137},
numpages = {4},
keywords = {software testing, defect reporting tools, Usability defect reporting},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/1809018.1809026,
author = {Lass, Robert N. and Macker, Joe and Millar, David and Taylor, Ian J.},
title = {GUMP: Adapting Client/Server Messaging Protocols into Peer-to-Peer Serverless Environments},
year = {2010},
isbn = {9781450300865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1809018.1809026},
doi = {10.1145/1809018.1809026},
abstract = {In this paper we present a generic environment for creating message-oriented server-side proxies to support adaptation from TCP transport-oriented client-server sessions to many-to-many peer-to-peer networking environments more suitable for deployment in dynamic wireless networks, capable of multicast forwarding. At its input, GUMP provides an interface for exposing network server implementations in order to allow existing GUI applications to connect to GUMP. At the back-end, GUMP's generic service discovery and multicast interfaces allow access to multiple implementations, enabling the discovery of necessary services on the network, maintenance of the network state, and transport of messages amongst peers, for tuning to a specific network environment. At the heart of GUMP, there is a mechanism for selecting a server-side proxy implementation for a given messaging protocol, allowing multiple proxies to co-exist and run time adaption of the system. As a primary example and use case, we show how GUMP has been used to implement an XMPP proxy allowing existing off-the-shelf XMPP client software to dynamically create and operate multi-user chat sessions in a serverless network environment. This resulting proxy integration demonstrates the power of GUMP in its ability to adapt between different methods of input using either HTTP or TCP oriented server systems, the use of its different discovery subsystem bindings (SLPv2 and JmDNS), and its support for multicast architectures. GUMP therefore allows a single messaging protocol server-side implementation to be dynamically adapted to suit a particular distributed wireless deployment environment at run time.},
booktitle = {Proceedings of the 2nd Workshop on Bio-Inspired Algorithms for Distributed Systems},
pages = {39–46},
numpages = {8},
keywords = {SLP, NORM, peer- to-peer, serverless chat, multicast, XMPP, ws-notification, JmDNS},
location = {Washington, DC, USA},
series = {BADS '10}
}

@inproceedings{10.1109/ASE.2019.00058,
author = {Liu, Jiaxiang and Shi, Xiaomu and Tsai, Ming-Hsien and Wang, Bow-Yaw and Yang, Bo-Yin},
title = {Verifying Arithmetic in Cryptographic C Programs},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00058},
doi = {10.1109/ASE.2019.00058},
abstract = {Cryptographic primitives are ubiquitous for modern security. The correctness of their implementations is crucial to resist malicious attacks. Typical arithmetic computation of these C programs contains large numbers of non-linear operations, hence is challenging existing automatic C verification tools. We present an automated approach to verify cryptographic C programs. Our approach successfully verifies C implementations of various arithmetic operations used in NIST P-224, P-256, P-521 and Curve25519 in OpenSSL. During verification, we expose a bug and a few anomalies that have been existing for a long time. They have been reported to and confirmed by the OpenSSL community. Our results establish the functional correctness of these C implementations for the first time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {552–564},
numpages = {13},
keywords = {cryptographic programs, OpenSSL, functional correctness, program verification},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/1028014.1028018,
author = {Bertelsen, Olav W. and Pold, S\o{}ren},
title = {Criticism as an Approach to Interface Aesthetics},
year = {2004},
isbn = {1581138571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1028014.1028018},
doi = {10.1145/1028014.1028018},
abstract = {In this paper we discuss the re-orientation of human-computer interaction as an aesthetic field. We argue that mainstream approaches lack of general openness and ability to assess experience aspects of interaction, but that this can indeed be remedied. We introduce the concept of interface criticism as a way to turn the conceptual re-orientation into handles for practical design, and we present and discuss an interface criticism guide.},
booktitle = {Proceedings of the Third Nordic Conference on Human-Computer Interaction},
pages = {23–32},
numpages = {10},
keywords = {formative assessment, interface aesthetics, interface criticism},
location = {Tampere, Finland},
series = {NordiCHI '04}
}

@inproceedings{10.1145/3426020.3426025,
author = {Ahmed Nazib, Rezoan and Moh, Sangman},
title = {Energy-Efficient Data Gathering Schemes in UAV-Based Wireless Sensor Networks},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426025},
doi = {10.1145/3426020.3426025},
abstract = {Wireless sensors networks (WSNs) comprise small sensing and computing units with limited power, and often run in non-replaceable energy sources. A large number of researches have been conducted for energy-efficient data gathering in unmanned aerial vehicle (UAV)-aided WSNs (UWSNs) for prolonging the lifetime of WSNs. UAVs are equipped with rechargeable batteries and can fly greater distances within a shorter period of time. However, the data gathering in UWSNs is still an under-investigated topic and more structured researches are required. To measure the effectiveness of the state-of-the-art models, performance analysis and comparison must be done by varying key parameters. As a new emerging field, there is no proper performance analysis guideline established in this topic yet. This study investigates major researches in this field and elaborately discusses the performance analysis techniques and tools used in the investigated research works. The qualitative comparisons of the performance analysis techniques will be able to provide a proper guideline to future researchers.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {22–27},
numpages = {6},
keywords = {energy efficiency, performance evaluation, simulation, Unmanned aerial vehicle, wireless sensor network},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@article{10.1145/1897852.1897871,
author = {B\"{o}rner, Katy},
title = {Plug-and-Play Macroscopes},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/1897852.1897871},
doi = {10.1145/1897852.1897871},
abstract = {Compose "dream tools" from continuously evolving bundles of software to make sense of complex scientific data sets.},
journal = {Commun. ACM},
month = {mar},
pages = {60–69},
numpages = {10}
}

@inproceedings{10.1145/2578903.2579149,
author = {Barbosa, Fernando S\'{e}rgio and Aguiar, Ademar},
title = {Reusable Roles, a Test with Patterns},
year = {2011},
isbn = {9781450312837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578903.2579149},
doi = {10.1145/2578903.2579149},
abstract = {Although roles have been around for a long time they have not yet reached mainstream programming languages. The variety of existing role models may be a limiting factor. We believe that for roles to be widely accepted they must enhance code reuse. An outcome would be a library of roles. We present and discuss what we feel are the characteristics that a role model must have to enable reusable and player-independent roles. In this paper we present our role model and JavaStage, a role language that extends Java, with examples of reusable roles. Finally, we present our steps towards the building of a role library, by presenting the roles developed from the analysis of the GoF Design Patterns. The results obtained, we developed roles for 10 of the 23 GoF patterns, are promising.},
booktitle = {Proceedings of the 18th Conference on Pattern Languages of Programs},
articleno = {11},
numpages = {16},
keywords = {design patterns, modularity, libraries, roles},
location = {Portland, Oregon, USA},
series = {PLoP '11}
}

@inproceedings{10.1145/3485447.3512225,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Ni, Mingze and Li, Li},
title = {CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512225},
doi = {10.1145/3485447.3512225},
abstract = {Github Copilot, trained on billions of lines of public code, has recently become the buzzword in the computer science research and practice community. Although it is designed to help developers implement safe and effective code with powerful intelligence, practitioners and researchers raise concerns about its ethical and security problems, e.g., should the copyleft licensed code be freely leveraged or insecure code be considered for training in the first place? These problems pose a significant impact on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models, which are inevitably on the rise with the fast development of artificial intelligence. To mitigate such impacts, we argue that there is a need to invent effective mechanisms for protecting open-source code from being exploited by deep learning models. Here, we design and implement a prototype, CoProtector, which utilizes data poisoning techniques to arm source code repositories for defending against such exploits. Our large-scale experiments empirically show that CoProtector is effective in achieving its purpose, significantly reducing the performance of Copilot-like deep learning models while being able to stably reveal the secretly embedded watermark backdoors.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {652–660},
numpages = {9},
keywords = {deep learning, dataset protection, open-source code, data poisoning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3350768.3353903,
author = {de Andrade, Stev\~{a}o Alves and de Oliveira Neves, V\^{a}nia and Delamaro, M\'{a}rcio Eduardo},
title = {Software Testing Education: Dreams and Challenges When Bringing Academia and Industry Closer Together},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3353903},
doi = {10.1145/3350768.3353903},
abstract = {Software systems are present in people's lives and they are increasing to the same extent as their complexity and their criticality. Therefore, we must ensure that these systems maintain high quality in order to behave as expected. To develop high quality software, it is essential to have qualified people who are knowledgeable about Validation and Verification (V&amp;V) techniques, especially software testing. This paper reports on the teaching process in two undergraduate courses in two different contexts: Computer Science students, who can dedicate more time during the day to studying, and Information Systems students, who can only study during the evenings. To engage and motivate the students in the context of software testing learning, we studied ways to bring real industry problems to the classroom in order to adopt the Problem-based Learning (PBL) approach. We chose two real open source projects which, considering the feedback from students and professors, was a good decision. However, the approach requires students to take extra classes rather than teacher-centered approaches. Extra classes may hinder the approach when the class consists of students who work during the day, thus developing a balance between student-centered and teacher-centered can be a good solution in such contexts.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {47–56},
numpages = {10},
keywords = {Software testing, education, problem-based learning, industry, PBL},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.5555/1251503.1251509,
author = {Mikulin, Dmitry and Vijayasundaram, Murali and Wong, Loreena},
title = {Incremental Linking on HP-UX},
year = {2000},
publisher = {USENIX Association},
address = {USA},
abstract = {The linker is often a time bottleneck in the development of large applications. Traditional linkers process all input files, even if only one or two objects have changed since the previous link. To shorten link time, we have developed an incremental linker for HP-UX which only processes modified files. Users can take advantage of the performance gains without modifying their usage patterns of the existing HP-UX linker since the incremental linker is implemented on top of the regular 64-bit linker. In addition to the tasks of the normal linker, the incremental linker must save extra information about input files, symbols and relocations, allow for the expansion of existing files and addition of new ones by allocating padding spaces in the output file and use this information to perform in-place updates. The results of several different design considerations and tradeoffs are materialized in link-time performance gains of up to thirteen times that of a normal link for large applications.},
booktitle = {Proceedings of the 1st Conference on Industrial Experiences with Systems Software - Volume 1},
pages = {6},
numpages = {1},
location = {San Diego, California},
series = {WIESS'00}
}

@inproceedings{10.1145/2753476.2753481,
author = {Rosenkranz, Philipp and W\"{a}hlisch, Matthias and Baccelli, Emmanuel and Ortmann, Ludwig},
title = {A Distributed Test System Architecture for Open-Source IoT Software},
year = {2015},
isbn = {9781450335027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2753476.2753481},
doi = {10.1145/2753476.2753481},
abstract = {In this paper, we discuss challenges that are specific to testing of open IoT software systems. The analysis reveals gaps compared to wireless sensor networks as well as embedded software. We propose a testing framework which (a) supports continuous integration techniques, (b) allows for the integration of project contributors to volunteer hardware and software resources to the test system, and (c) can function as a permanent distributed plugtest for network interoperability testing. The focus of this paper lies in open-source IoT development but many aspects are also applicable to closed-source projects.},
booktitle = {Proceedings of the 2015 Workshop on IoT Challenges in Mobile and Industrial Systems},
pages = {43–48},
numpages = {6},
keywords = {open-source iot, interoperability, test system architecture},
location = {Florence, Italy},
series = {IoT-Sys '15}
}

@inproceedings{10.1145/2462932.2462951,
author = {Consiglio, Teresa and van der Veer, Gerrit C.},
title = {Design for Free Learning: A Case Study on Supporting a Service Design Course},
year = {2012},
isbn = {9781450316057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462932.2462951},
doi = {10.1145/2462932.2462951},
abstract = {In this experience report, we provide a case study on the use of information and communication technology (ICT) in higher education, developing an open source interactive learning environment to support a blended course.Our aim is to improve the quality of adult distance learning, ultimately involving peers worldwide, by developing learning environments as flexible as possible regardless of the culture and context of use, of individual learning style and age of the learners. Our example concerns a course of Service Design where the teacher was physically present only intermittently for part of the course while in the remaining time students worked in teams using our online learning environment.We developed a structure where students are guided through discovery learning and mutual teaching. We will show how we started from the students' authentic goals and how we supported them by a simple structure of pacing the discovery process and merging theoretical understanding with practice in real life.Based on these first empirical results practical guidelines have been developed regarding improvements on the structure provided for the learning material and on the interaction facilities for students, teachers and instructional designers.},
booktitle = {Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration},
articleno = {14},
numpages = {13},
keywords = {learner centered design, e-learning, service design, experience report, open source, cultural diversity},
location = {Linz, Austria},
series = {WikiSym '12}
}

@inproceedings{10.1145/1081706.1081755,
author = {Li, Zhenmin and Zhou, Yuanyuan},
title = {PR-Miner: Automatically Extracting Implicit Programming Rules and Detecting Violations in Large Software Code},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081755},
doi = {10.1145/1081706.1081755},
abstract = {Programs usually follow many implicit programming rules, most of which are too tedious to be documented by programmers. When these rules are violated by programmers who are unaware of or forget about them, defects can be easily introduced. Therefore, it is highly desirable to have tools to automatically extract such rules and also to automatically detect violations. Previous work in this direction focuses on simple function-pair based programming rules and additionally requires programmers to provide rule templates.This paper proposes a general method called PR-Miner that uses a data mining technique called frequent itemset mining to efficiently extract implicit programming rules from large software code written in an industrial programming language such as C, requiring little effort from programmers and no prior knowledge of the software. Benefiting from frequent itemset mining, PR-Miner can extract programming rules in general forms (without being constrained by any fixed rule templates) that can contain multiple program elements of various types such as functions, variables and data types. In addition, we also propose an efficient algorithm to automatically detect violations to the extracted programming rules, which are strong indications of bugs.Our evaluation with large software code, including Linux, PostgreSQL Server and the Apache HTTP Server, with 84K--3M lines of code each, shows that PR-Miner can efficiently extract thousands of general programming rules and detect violations within 2 minutes. Moreover, PR-Miner has detected many violations to the extracted rules. Among the top 60 violations reported by PR-Miner, 16 have been confirmed as bugs in the latest version of Linux, 6 in PostgreSQL and 1 in Apache. Most of them violate complex programming rules that contain more than 2 elements and are thereby difficult for previous tools to detect. We reported these bugs and they are currently being fixed by developers.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {306–315},
numpages = {10},
keywords = {automated specification generation, pattern recognition, static analysis, programming rules, data mining for software engineering, automated violation detection},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@inproceedings{10.1145/1879211.1879222,
author = {Aftandilian, Edward E. and Kelley, Sean and Gramazio, Connor and Ricci, Nathan and Su, Sara L. and Guyer, Samuel Z.},
title = {Heapviz: Interactive Heap Visualization for Program Understanding and Debugging},
year = {2010},
isbn = {9781450300285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879211.1879222},
doi = {10.1145/1879211.1879222},
abstract = {Understanding the data structures in a program is crucial to understanding how the program works, or why it doesn't work. Inspecting the code that implements the data structures, however, is an arduous task and often fails to yield insights into the global organization of a program's data. Inspecting the actual contents of the heap solves these problems but presents a significant challenge of its own: finding an effective way to present the enormous number of objects it contains.In this paper we present Heapviz, a tool for visualizing and exploring snapshots of the heap obtained from a running Java program. Unlike existing tools, such as traditional debuggers, Heapviz presents a global view of the program state as a graph, together with powerful interactive capabilities for navigating it. Our tool employs several key techniques that help manage the scale of the data. First, we reduce the size and complexity of the graph by using algorithms inspired by static shape analysis to aggregate the nodes that make up a data structure. Second, we introduce a dominator-based layout scheme that emphasizes hierarchical containment and ownership relations. Finally, the interactive interface allows the user to expand and contract regions of the heap to modulate data structure detail, inspect individual objects and field values, and search for objects based on type or connectivity. By applying Heapviz to both constructed and real-world examples, we show that Heapviz provides programmers with a powerful and intuitive tool for exploring program behavior.},
booktitle = {Proceedings of the 5th International Symposium on Software Visualization},
pages = {53–62},
numpages = {10},
keywords = {software visualization, program understanding, graphs, interactive visualization, debugging},
location = {Salt Lake City, Utah, USA},
series = {SOFTVIS '10}
}

@inproceedings{10.1145/2181037.2181073,
author = {Auer, Liisa and Juntunen, Jouni and Ojala, Pekka},
title = {Open Source Project as a Pedagogical Tool in Higher Education},
year = {2011},
isbn = {9781450308168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181037.2181073},
doi = {10.1145/2181037.2181073},
abstract = {Nowadays open source is widely used approach to software development. In Finland the government is also encouraging communities to use open source software instead of closed software.Requirements for the future professionals are becoming more complex and transforming rapidly. Internet and Web 2.0 are changing the way people learn and work. Higher education has to take this into account in order to improve teaching and study methodologies.School of Business and Information Management, a school part of Oulu University of Applied Sciences (OUAS), is launching an open source project called OpixProject to support education and research related to information processing science. Software under development will be a project management application that can be used to manage traditional and agile projects. The purpose of this project is to use an open source project as a pedagogical tool within the context of computer science education.},
booktitle = {Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments},
pages = {207–213},
numpages = {7},
keywords = {open source, learning, higher education, pedagogy},
location = {Tampere, Finland},
series = {MindTrek '11}
}

@inproceedings{10.1145/1104973.1104979,
author = {Shanks, Bayle},
title = {WikiGateway: A Library for Interoperability and Accelerated Wiki Development},
year = {2005},
isbn = {1595931112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1104973.1104979},
doi = {10.1145/1104973.1104979},
abstract = {WikiGateway is an open-source suite of tools for automated interaction with wikis:• Python and Perl modules with functions like getPage, putPage, getRecentChanges, and more.• A mechanism to add DAV, Atom, or XMLRPC capabilities to any supported wiki server.• A command-line tool with functionality similar to the Perl and Python modules.• Demo applications built on top of these tools include a wiki copy command, a spam-cleaning bot, and a tool to recursively upload text files inside a directory structure as wiki pages.All WikiGateway tools are compatible with a number of different wiki engines. Developers can use WikiGateway to hide the differences between wiki engines and build applications which interoperate with many different wiki engines.},
booktitle = {Proceedings of the 2005 International Symposium on Wikis},
pages = {53–66},
numpages = {14},
keywords = {client-side wiki, middleware, interoperability, wiki, WikiClient, WikiRPCInterface, atom, interwiki, WikiGateway, wiki XMLRPC, WebDAV},
location = {San Diego, California},
series = {WikiSym '05}
}

@inproceedings{10.1145/3597926.3598074,
author = {Huang, Huaxun and Xu, Chi and Wen, Ming and Liu, Yepang and Cheung, Shing-Chi},
title = {ConfFix: Repairing Configuration Compatibility Issues in Android Apps},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598074},
doi = {10.1145/3597926.3598074},
abstract = {XML configuration files are widely-used to specify the user interfaces (UI) of Android apps. Configuration compatibility (CC) issues are induced owing to the inconsistent handling of such XML configuration files across different Android framework versions. CC issues can cause software crashes and inconsistent look-and-feels, severely impacting the user experience of Android apps. However, there is no universal solution to resolve CC issues and app developers need to handle CC issues case by case. Existing tools are designed based on predefined rules or visual features that are possibly manifested by CC issues. Unfortunately, they can fail or generate overfitting patches when the CC issues are beyond their capabilities. To fill the above research gaps, we first empirically studied the app developers' common strategies in patching real-world CC issues. Based on the findings, we propose ConfFix, an automatic approach to repair CC issues in Android apps. ConfFix is driven by the knowledge of how an XML element is handled inconsistently in different versions of the Android framework and generates patches to eliminate such inconsistencies. We evaluated ConfFix on a set of 77 reproducible CC issues in 13 open-source Android apps. The results show that ConfFix outperforms baselines in successfully repairing 64 CC issues with a high precision. Encouragingly, the patches for 38 CC issues have been confirmed and merged by app developers.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {514–525},
numpages = {12},
keywords = {Android, XML Configuration, Compatibility, Automated Repair},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/1137983.1138012,
author = {Knab, Patrick and Pinzger, Martin and Bernstein, Abraham},
title = {Predicting Defect Densities in Source Code Files with Decision Tree Learners},
year = {2006},
isbn = {1595933972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137983.1138012},
doi = {10.1145/1137983.1138012},
abstract = {With the advent of open source software repositories the data available for defect prediction in source files increased tremendously. Although traditional statistics turned out to derive reasonable results the sheer amount of data and the problem context of defect prediction demand sophisticated analysis such as provided by current data mining and machine learning techniques.In this work we focus on defect density prediction and present an approach that applies a decision tree learner on evolution data extracted from the Mozilla open source web browser project. The evolution data includes different source code, modification, and defect measures computed from seven recent Mozilla releases. Among the modification measures we also take into account the change coupling, a measure for the number of change-dependencies between source files. The main reason for choosing decision tree learners, instead of for example neural nets, was the goal of finding underlying rules which can be easily interpreted by humans. To find these rules, we set up a number of experiments to test common hypotheses regarding defects in software entities. Our experiments showed, that a simple tree learner can produce good results with various sets of input data.},
booktitle = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
pages = {119–125},
numpages = {7},
keywords = {data mining, defect prediction, decision tree learner},
location = {Shanghai, China},
series = {MSR '06}
}

@inproceedings{10.1109/FLOSS.2009.5071352,
author = {Shibuya, Bianca and Tamai, Tetsuo},
title = {Understanding the Process of Participating in Open Source Communities},
year = {2009},
isbn = {9781424437207},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2009.5071352},
doi = {10.1109/FLOSS.2009.5071352},
abstract = {The number of participants in Open Source Software (OSS) communities has increased. Not only volunteers participate, but also companies and their employees. The motivation of the participants vary from extrinsic to intrinsic values. Community-managed and sponsored OSS projects try to explore these motivations to attract and keep these participants. This paper analyses three different OSS projects: MySQL, OpenOffice.org, and GNOME. Each has a different organizational structure that influences participants behavior. This study analyzes qualitative data from publicly available documents, such as project's wiki pages and project's webpages, and quantitative data from bug tracking systems and source code repositories. One of our findings is that the number of active developers does not change significantly when the total number of committers increases for the selected OSS projects.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
pages = {1–6},
numpages = {6},
series = {FLOSS '09}
}

@inproceedings{10.5555/2048416.2048420,
author = {Rizvi, Syed S. and Elleithy, Khaled M.},
title = {A Generic Optimized Time Management Algorithms (OTMA) Framework for Simulating Large-Scale Overlay Networks},
year = {2011},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Recent evolutions in wireless networks will require more efficient use of the underlying parallel discrete-event simulation (PDES) synchronization protocols to accommodate the demand for large-scale network simulation. In this paper, we investigate underlying synchronization protocols to improve the performance of large-scale network simulators operating over PDES systems. We begin by proposing a generic optimized time management algorithms (OTMA) framework that combines the improved forms of synchronization protocols on a single platform. Particularly, for the proposed OTMA framework, we use the layered architecture approach to combine the optimized forms of conservative and optimistic time management algorithms. To support the implementation of the OTMA framework, a new m-LP (logical process) simulation model is proposed along with the varying parameters network topology that can show the implementation of different components of discrete-event simulation (DES).},
booktitle = {Proceedings of the 14th Communications and Networking Symposium},
pages = {27–34},
numpages = {8},
keywords = {synchronization algorithms, distributed algorithms, overlay networks, discrete-event simulation},
location = {Boston, Massachusetts},
series = {CNS '11}
}

@inproceedings{10.5555/2820690.2820701,
author = {Armenise, Valentina},
title = {Continuous Delivery with Jenkins: Jenkins Solutions to Implement Continuous Delivery},
year = {2015},
publisher = {IEEE Press},
abstract = {This paper illustrates how Jenkins evolved from being a pure Continuous Integration Platform to a Continuous Delivery one, embracing the new design tendency where not only the build but also the release and the delivery process of the product is automated. In this scenario Jenkins becomes the orchestrator tool for all the teams/roles involved in the software lifecycle, thanks to which Development, Quality&amp;Assurance and Operations teams can work closely together.Goal of this paper is not only to position Jenkins as hub for CD, but also introduce the challenges that still need to be solved in order to strengthen Jenkins' tracking capabilities.},
booktitle = {Proceedings of the Third International Workshop on Release Engineering},
pages = {24–27},
numpages = {4},
location = {Florence, Italy},
series = {RELENG '15}
}

@inproceedings{10.1145/1163653.1163670,
author = {Davies, S. E. and Gardner, S.},
title = {Direct IP-Based Mobile-to-Mobile Communications: First Phase Results},
year = {2006},
isbn = {1595935029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1163653.1163670},
doi = {10.1145/1163653.1163670},
abstract = {The networking world is showing increasing interest in changing from circuit-based to a complete packet-based system. The change has been slow for many years but is now accelerating from the dated Tim-Division Multiplexed (TDM) narrowband networks to an IP-based broadband network system. This offers several key advantages, including, simpler up-grades, lower costs, and integrated support for a wide range of high-value voice, data, and video applications and services.Many factors have converged to make the market ripe for the change from circuit to packet. The emergence of Digital Subscriber Line (DSL) as a access technology for broadband along with Voice over Internet Protocol (VoIP) is now proving increasingly competitive with TDM networks and is just one example of many that are pushing for the migration of circuit to packet based networks. TVoIP and 3G are amongst the other examples that will inventively lead to an entire IP-based world.This paper investigates a disadvantage that mobile packet-based networks presently hold. It addresses the difficult task of transmitting IP -based services from one mobile device to another when using standard mobile services in the UK. The issues surround the type of IP address provided to a device from the mobile provider on connection. The IP number is both DHCP served and not publicly routable. Although it is possible to undertake such a process via a private APN, the cost for such an arrangement, together with a suitable backhaul, would be a major barrier to the practical implementation of such a service for many users},
booktitle = {Proceedings of the ACM International Workshop on Performance Monitoring, Measurement, and Evaluation of Heterogeneous Wireless and Wired Networks},
pages = {90–94},
numpages = {5},
keywords = {mobile IP-based, multi-network architecture, 3G/UMTS},
location = {Terromolinos, Spain},
series = {PM2HW2N '06}
}

@inproceedings{10.1145/3297858.3304057,
author = {Hu, Yigong and Liu, Suyi and Huang, Peng},
title = {A Case for Lease-Based, Utilitarian Resource Management on Mobile Devices},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304057},
doi = {10.1145/3297858.3304057},
abstract = {Mobile apps have become indispensable in our daily lives, but many apps are not designed to be energy-aware that they may consume the constrained resources on mobile devices in a wasteful manner. Blindly throttling heavy resource usage, while helps reducing energy consumption, prohibits apps from taking advantages of the resources to do useful work. We argue that addressing this issue requires mobile OS to continuously assess if a resource is still truly needed even after it is granted to an app.This paper proposes that lease, a mechanism commonly used in distributed systems, is a well-suited abstraction in resource-constrained mobile devices to mitigate app energy misbehavior. We design a lease-based, utilitarian resource management mechanism, LeaseOS, that analyzes the utility of a resource to an app at each lease term, and then makes lease decisions based on the utility. We implement LeaseOS on top of the latest Android OS and evaluate it with 20 real-world apps with energy bugs. LeaseOS reduces wasted power by 92% on average and significantly outperforms the state-of-the-art Android Doze and DefDroid. It also did not cause any usability disruption to the evaluated apps. LeaseOS itself incurs negligible energy overhead.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {301–315},
numpages = {15},
keywords = {lease, energy efficiency, operating system, mobile apps},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.1145/3524842.3528479,
author = {Warrick, Melanie and Rosenblatt, Samuel F. and Young, Jean-Gabriel and Casari, Amanda and H\'{e}bert-Dufresne, Laurent and Bagrow, James},
title = {The OCEAN Mailing List Data Set: Network Analysis Spanning Mailing Lists and Code Repositories},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528479},
doi = {10.1145/3524842.3528479},
abstract = {Communication surrounding the development of an open source project largely occurs outside the software repository itself. Historically, large communities often used a collection of mailing lists to discuss the different aspects of their projects. Multimodal tool use, with software development and communication happening on different channels, complicates the study of open source projects as a sociotechnical system. Here, we combine and standardize mailing lists of the Python community, resulting in 954,287 messages from 1995 to the present. We share all scraping and cleaning code to facilitate reproduction of this work, as well as smaller datasets for the Golang (122,721 messages), Angular (20,041 messages) and Node.js (12,514 messages) communities. To showcase the usefulness of these data, we focus on the CPython repository and merge the technical layer (which GitHub account works on what file and with whom) with the social layer (messages from unique email addresses) by identifying 33% of GitHub contributors in the mailing list data. We then explore correlations between the valence of social messaging and the structure of the collaboration network. We discuss how these data provide a laboratory to test theories from standard organizational science in large open source projects.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {338–342},
numpages = {5},
keywords = {sociotechnical systems, text tagging, datasets, network analysis},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1109/MSR.2019.00064,
author = {Ponta, Serena E. and Plate, Henrik and Sabetta, Antonino and Bezzi, Michele and Dangremont, C\'{e}dric},
title = {A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00064},
doi = {10.1109/MSR.2019.00064},
abstract = {Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure.While operating a vulnerability assessment tool, which we developed, and that is currently used by hundreds of development units at SAP, we manually collected and curated a dataset of vulnerabilities of open-source software, and the commits fixing them. The data were obtained both from the National Vulnerability Database (NVD), and from project-specific web resources, which we monitor on a continuous basis.From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct open-source Java projects, used in SAP products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a CVE (Common Vulnerability and Exposure) identifier at all, and 46, which do have such identifier assigned by a numbering authority, are not available in the NVD yet.The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories, and to augment the attributes available for each instance. Moreover, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications).Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; it also represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {383–387},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1109/ICSE.2017.23,
author = {Joblin, Mitchell and Apel, Sven and Hunsen, Claus and Mauerer, Wolfgang},
title = {Classifying Developers into Core and Peripheral: An Empirical Study on Count and Network Metrics},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.23},
doi = {10.1109/ICSE.2017.23},
abstract = {Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {164–174},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3360596,
author = {Shen, Bo and Zhang, Wei and Zhao, Haiyan and Liang, Guangtai and Jin, Zhi and Wang, Qianxiang},
title = {IntelliMerge: A Refactoring-Aware Software Merging Technique},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360596},
doi = {10.1145/3360596},
abstract = {In modern software development, developers rely on version control systems like Git to collaborate in the branch-based development workflow. One downside of this workflow is the conflicts occurred when merging contributions from different developers: these conflicts are tedious and error-prone to be correctly resolved, reducing the efficiency of collaboration and introducing potential bugs. The situation becomes even worse, with the popularity of refactorings in software development and evolution, because current merging tools (usually based on the text or tree structures of source code) are unaware of refactorings. In this paper, we present IntelliMerge, a graph-based refactoring-aware merging algorithm for Java programs. We explicitly enhance this algorithm's ability in detecting and resolving refactoring-related conflicts. Through the evaluation on 1,070 merge scenarios from 10 popular open-source Java projects, we show that IntelliMerge reduces the number of merge conflicts by 58.90% comparing with GitMerge (the prevalent unstructured merging tool) and 11.84% comparing with jFSTMerge (the state-of-the-art semi-structured merging tool) without sacrificing the auto-merging precision (88.48%) and recall (90.22%). Besides, the evaluation of performance shows that IntelliMerge takes 539 milliseconds to process one merge scenario on the median, which indicates its feasibility in real-world applications.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {170},
numpages = {28},
keywords = {conflicts resolution, Revision control system, refactoring, software merging}
}

@inproceedings{10.1145/1233341.1233418,
author = {Gyllstrom, Karl and Miller, Dorian and Stotts, David},
title = {Techniques for Improving the Visibility and "Sharability" of Semi-Transparent Video in Shared Workspaces},
year = {2007},
isbn = {9781595936295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233341.1233418},
doi = {10.1145/1233341.1233418},
abstract = {Semi-transparency increases the amount of information that interfaces can expose in a given screen space by allowing content from a window to remain partially visible while other windows overlap it. We have previously explored the application of a full screen, semi-transparent video window in a collaborative, distributed software environment in an effort to more seamlessly incorporate face-to-face communication into group software development. Our experience with the system suggests that, while semi-transparent video can improve aspects of distributed collaborations, two problems emerge. First, the alpha blending of pixels from video overlays can obfuscate certain types of visual data, making either the video or the underlying content diffcult to see. Second, video overlays complicate the ability to provide application sharing at the framebuffer - the most general sharing layer. In this paper, we present methods to overcome these problems.},
booktitle = {Proceedings of the 45th Annual Southeast Regional Conference},
pages = {425–430},
numpages = {6},
keywords = {semi-transparency, collaboration, videoconferencing},
location = {Winston-Salem, North Carolina},
series = {ACM-SE 45}
}

@inproceedings{10.1145/3330393.3330415,
author = {Gilani, Syed Sherjeel A. and Khosa, Inam Ullah and Bin Waheed, M. Hamza and Qayyum, Amir and Bano, Mukhtiar},
title = {QoENGN: A QoE Framework for Video Streaming over Next Generation Mobile Networks},
year = {2019},
isbn = {9781450371711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330393.3330415},
doi = {10.1145/3330393.3330415},
abstract = {Migration of conventional TV services to modern day IPTV solutions brought many serious challenges. Scalability is one of the critical goal for the multimedia traffic which is normally achieved through Content Delivery Networks (CDNs) by providing Caching servers near the requesting clients. Moreover, Quality of Experience to the user is another issue when rendering the services over the IP based networks. Bandwidth Constraints and Smoothness of Video requires Adaptive &amp; Scalable Video Streaming. This paper presents an End- to-End model based on IPTV Service Delivery Platform over next generation IMS/EPC Network Infrastructure. The proposed model is an IMS standard compliant, scalable, adaptive and extensible mobile IPTV framework which have distinct key features like, support for Context aware and Personalized Broadcast (BC) and Video on Demand (VoD) services and IMS and SIP Compliant Andriod OS based IPTV set-top box.},
booktitle = {Proceedings of the 2019 4th International Conference on Multimedia Systems and Signal Processing},
pages = {109–113},
numpages = {5},
keywords = {Content Delivery Network, IP Multimedia Subsystems, IPTV, Quality of Experience, Next Generation Networks},
location = {Guangzhou, China},
series = {ICMSSP '19}
}

@article{10.1145/1836216.1836236,
author = {Dubberly, Hugh},
title = {The Space of Design},
year = {2010},
issue_date = {September + October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/1836216.1836236},
doi = {10.1145/1836216.1836236},
journal = {Interactions},
month = {sep},
pages = {74–79},
numpages = {6}
}

@inproceedings{10.1145/3173574.3174101,
author = {Yeh, Tom and Kim, Jeeeun},
title = {CraftML: 3D Modeling is Web Programming},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174101},
doi = {10.1145/3173574.3174101},
abstract = {We explore web programming as a new paradigm for programmatic 3D modeling. Most existing approaches subscribe to the imperative programming paradigm. While useful, there exists a gulf of evaluation between procedural steps and the intended structure. We present CraftML, a language providing a declarative syntax where the code is the structure. CraftML offers a rich set of programming features familiar to web developers of all skill levels, such as tags, hyperlinks, document object model, cascade style sheet, JQuery, string interpolation, template engine, data injection, and scalable vector graphics. We develop an online IDE to support CraftML development, with features such as live preview, search, module import, and parameterization. Using examples and case studies, we demonstrate that CraftML offers a low floor for beginners to make simple designs, a high ceiling for experts to build complex computational models, and wide walls to support many application domains such as education, data physicalization, tactile graphics, assistive devices, and mechanical components.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {3d modeling, programming, 3d printing, creativity support, fabrication},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2424563.2424570,
author = {Osman, Hafeez and van Zadelhoff, Arjan and Stikkolorum, Dave R. and Chaudron, Michel R. V.},
title = {UML Class Diagram Simplification: What is in the Developer's Mind?},
year = {2012},
isbn = {9781450318112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2424563.2424570},
doi = {10.1145/2424563.2424570},
abstract = {Class diagrams play an important role in software development. However, in some cases, these diagrams contain a lot of information. This makes it hard for software maintainers to use them to understand a system. In this paper, we aim to discover how to simplify class diagrams in a such way that they make systems easier to understand. To this end, we performed a survey to analyze what type of information software developers find important to include or exclude in order to simplify a class diagram. This survey involved 32 software developers with 75% of the participants having more than 5 years of experience with class diagrams. As the result, we found that the important elements in a class diagram are class relationship, meaningful class names and class properties. We also found that information that should be excluded in a simplified class diagram is GUI related information, private and protected operations, helper classes and library classes. In this survey we also tried to discover what types of features are needed for class diagram simplification tools.},
booktitle = {Proceedings of the Second Edition of the International Workshop on Experiences and Empirical Studies in Software Modelling},
articleno = {5},
numpages = {6},
keywords = {class diagram, reverse engineering, UML, simplification},
location = {Innsbruck, Austria},
series = {EESSMod '12}
}

@inproceedings{10.1145/2212776.2212815,
author = {Masita-Mwangi, Mokeira and Mwakaba, Nancy and Impio, Jussi},
title = {Taking Micro-Enterprise Online: The Case of Kenyan Businesses},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2212815},
doi = {10.1145/2212776.2212815},
abstract = {In this paper we describe the findings of a research study recently carried out amongst micro-entrepreneurs and freelance web developers in Kenya. The objective was to understand the level of need for website creation by such entrepreneurs for their businesses and further, the challenges associated with website design and maintenance. The study was inspired by the phenomenal uptake of Internet use in the country coupled with a need to explore how micro-entrepreneurs are faring in this space, what potential exists, and how it could be realized. The findings of the study show that the Internet can be the new frontier for many micro-entrepreneurs who want to take their businesses to the next level. The study also provides critical insights into the realities of micro-enterprise, and hence relevant issues to take into consideration in seeking to take micro-enterprise online. The insights therein cover such issues as affordability of solutions, quick return on investment, convergence of current business methods and practices with those presented by an online environment for greater impact, and need for very simple, intuitive web design tools and platforms. Innovation may be required so as to come up with more website options that are better suited to the needs of micro-entrepreneurs and that are cost-effective. Alternatively other internet-based tools or platforms could be developed to help micro-entrepreneurs conduct business online. This is because the typical websites of today are not necessarily suitable for their needs.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {367–382},
numpages = {16},
keywords = {online, business promotion, website design, micro-enterprise},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.1145/1454247.1454251,
author = {Happel, Hans-J\"{o}rg and Maalej, Walid},
title = {Potentials and Challenges of Recommendation Systems for Software Development},
year = {2008},
isbn = {9781605582283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454247.1454251},
doi = {10.1145/1454247.1454251},
abstract = {By surveying recommendation systems in software development, we found that existing approaches have been focusing on "you might like what similar developers like" scenarios. However structured artifacts and semantically well-defined development activities bear large potentials for further recommendation scenarios. We introduce a novel "landscape" of software development recommendation systems and line out several scenarios for knowledge sharing and collaboration. Basic challenges are improving context-awareness and particularly addressing information providers.},
booktitle = {Proceedings of the 2008 International Workshop on Recommendation Systems for Software Engineering},
pages = {11–15},
numpages = {5},
location = {Atlanta, Georgia},
series = {RSSE '08}
}

@inproceedings{10.1145/2016741.2016788,
author = {Uram, Thomas D. and Papka, Michael E. and Hereld, Mark and Wilde, Michael},
title = {A Solution Looking for Lots of Problems: Generic Portals for Science Infrastructure},
year = {2011},
isbn = {9781450308885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016741.2016788},
doi = {10.1145/2016741.2016788},
abstract = {Science gateways have dramatically simplified the work required by science communities to run their codes on TeraGrid resources. Gateway development typically spans the duration of a particular grant, with the first production runs occurring some months after the award and concluding near the end of the project. Scientists use gateways as a means to interface with large resources. Our gateway infrastructure facilitates this by hiding away the various details of the underlying resources and presents an intuitive way to interact with the resource. In this paper, we present our work on GPSI, a general-purpose science gateway infrastructure that can be easily customized to meet the needs of an application. This reduces the time to deployment and improves scientific productivity. Our contribution in this paper is two-fold: to elaborate our vision for a user-driven gateway infrastructure that includes components required by multiple science domains, thus aiding the speedy development of gateways, and presenting our experience in moving from our initial portal implementations to the current effort based on Python [15] and Django [16].},
booktitle = {Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery},
articleno = {44},
numpages = {7},
keywords = {workflow, Swift, Python, web2.0, Django, science gateway},
location = {Salt Lake City, Utah},
series = {TG '11}
}

@proceedings{10.1145/3549034,
title = {MaLTeSQuE 2022: Proceedings of the 6th International Workshop on Machine Learning Techniques for Software Quality Evaluation},
year = {2022},
isbn = {9781450394567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 6th edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2022), held in Singapore, on November 18th, 2022, co-located with the 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). MaLTeSQuE received a total of six submissions from all over the world, from which five papers were included in the program. The program also features two keynotes, by Yuriy Brun and Mike Papadakis, on the promises, dangers, and best practices of working at the intersection of machine learning and software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2925426.2926286,
author = {Grosser, Tobias and Hoefler, Torsten},
title = {Polly-ACC Transparent Compilation to Heterogeneous Hardware},
year = {2016},
isbn = {9781450343619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2925426.2926286},
doi = {10.1145/2925426.2926286},
abstract = {Programming today's increasingly complex heterogeneous hardware is difficult, as it commonly requires the use of data-parallel languages, pragma annotations, specialized libraries, or DSL compilers. Adding explicit accelerator support into a larger code base is not only costly, but also introduces additional complexity that hinders long-term maintenance. We propose a new heterogeneous compiler that brings us closer to the dream of automatic accelerator mapping. Starting from a sequential compiler IR, we automatically generate a hybrid executable that - in combination with a new data management system - transparently offloads suitable code regions. Our approach is almost regression free for a wide range of applications while improving a range of compute kernels as well as two full SPEC CPU applications. We expect our work to reduce the initial cost of accelerator usage and to free developer time to investigate algorithmic changes.},
booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
articleno = {1},
numpages = {13},
keywords = {Auto-Parallelization, Polyhedral Compilation, GPGPU},
location = {Istanbul, Turkey},
series = {ICS '16}
}

@article{10.1145/3095812,
author = {Fetter, Mirko and Bimamisa, David and Gross, Tom},
title = {TUIOFX: A JavaFX Toolkit for Shared Interactive Surfaces},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {EICS},
url = {https://doi.org/10.1145/3095812},
doi = {10.1145/3095812},
abstract = {Building multi-touch multi-user applications for Shared Interactive Surfaces is a complex endeavour that requires fundamental knowledge in touch enabling hardware, gesture recognition, graphical representation of digital information and multi-user interaction. While several specialised toolkits help developers in this effort, we identified a variety of challenges with these toolkits, as for example the lack of cross-platform support, the limited number of touch-enabled multi-user widgets, missing documentation, and lacking community support -- all raising the barriers to entry. In this paper, we present TUIOFX, a toolkit for developing multi-touch, multi-user applications for Shared Interactive Surfaces in Java, which tackles all of the identified problems. The sophisticated implementation of TUIOFX adds support for TUIO-enabled hardware and multi-user interaction under the hood of JavaFX, and leaves the well-learned JavaFX API for the developers fully intact -- thus allowing particularly novices a very quick start. In this paper we provide the technical insights, in the concepts and their elegant implementation.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {10},
numpages = {18},
keywords = {multi-touch, toolkit, sdk, javafx, shared interactive surfaces, multi-user}
}

@inproceedings{10.1145/2331714.2331731,
author = {Barth, Iwan and Vuillerme, Nicolas},
title = {Social Concerns of Ubiquitous Computing: Between "User Experience" and "Social Imaginary" Theory},
year = {2012},
isbn = {9781450312431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2331714.2331731},
doi = {10.1145/2331714.2331731},
abstract = {In this paper, we deal with roots and limits of "user experience" approach to address social concerns of ubiquitous computing. Then we give an overview of "social imaginary" theory, used to investigate social outcomes of ubiquitous system design for domestic space. In conclusion, we try to apply autonomy and heteronomy concepts to the practice in general technology and medicine technology.},
booktitle = {Proceedings of the 2012 Virtual Reality International Conference},
articleno = {14},
numpages = {7},
keywords = {social imaginary, ubiquitous computing, acceptability, significations, user centered design, user experience},
location = {Laval, France},
series = {VRIC '12}
}

@inproceedings{10.1145/2832920.2832924,
author = {Cancila, Daniela and Zaatiti, Hadi and Passerone, Roberto},
title = {Cyber-Physical System and Contract-Based Design: A Three Dimensional View},
year = {2015},
isbn = {9781450338974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832920.2832924},
doi = {10.1145/2832920.2832924},
abstract = {This work reports on the experience arising from the master internship contract-based design tailored to safety issues for cyber-physical systems (CPS). The main educational goal is to confront the student with realistic mixed-critical smart CPS systems, using the railway domain and autonomous trains as a case study. The results show that, for this class of systems, education should transition from a 2D to a 3D modeling design space, which is much better suited to visualizing the evolution and the underlying properties of the system. We use contract-based design to properly deal with the integration and composition of heterogeneous components, where safety aspects require special attention. The main scientific and technical results concern the implementation of contract-based design in a 3D tool. Finally, we discuss the teaching methodology underlying the internship and the competences required to address the design of a (critical) CPS by the new generation of students.},
booktitle = {Proceedings of the WESE'15: Workshop on Embedded and Cyber-Physical Systems Education},
articleno = {4},
numpages = {4},
keywords = {Railway, Cyber-Physical System, 3D tool, Contract-Based Design},
location = {Amsterdam, Netherlands},
series = {WESE'15}
}

@article{10.1145/3596908,
author = {Zakeri-Nasrabadi, Morteza and Parsa, Saeed and Esmaili, Ehsan and Palomba, Fabio},
title = {A Systematic Literature Review on the Code Smells Datasets and Validation Mechanisms},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3596908},
doi = {10.1145/3596908},
abstract = {The accuracy reported for code smell-detecting tools varies depending on the dataset used to evaluate the tools. Our survey of 45 existing datasets reveals that the adequacy of a dataset for detecting smells highly depends on relevant properties such as the size, severity level, project types, number of each type of smell, number of smells, and the ratio of smelly to non-smelly samples in the dataset. Most existing datasets support God Class, Long Method, and Feature Envy, while six smells in Fowler and Beck's catalog are not supported by any datasets. We conclude that existing datasets suffer from imbalanced samples, lack of supporting severity level, and restriction to Java language.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {298},
numpages = {48},
keywords = {Software smell, code smell prediction, systematic literature review, code smell dataset, source code metrics}
}

@proceedings{10.5555/3623288,
title = {ICSE-NIER '23: Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3379337.3415814,
author = {Chi, Peggy and Sun, Zheng and Panovich, Katrina and Essa, Irfan},
title = {Automatic Video Creation From a Web Page},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415814},
doi = {10.1145/3379337.3415814},
abstract = {Creating marketing videos from scratch can be challenging, especially when designing for multiple platforms with different viewing criteria. We present URL2Video, an automatic approach that converts a web page into a short video given temporal and visual constraints. URL2Video captures quality materials and design styles extracted from a web page, including fonts, colors, and layouts. Using constraint programming, URL2Video's design engine organizes the visual assets into a sequence of shots and renders to a video with user-specified aspect ratio and duration. Creators can review the video composition, modify constraints, and generate video variation through a user interface. We learned the design process from designers and compared our automatically generated results with their creation through interviews and an online survey. The evaluation shows that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {279–292},
numpages = {14},
keywords = {video creation, video storyboard, web document, creativity tools, storytelling, web design},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{10.1145/3457913.3457932,
author = {Yang, Huaiwei and Liu, Shuang and Gui, Lin and Zhao, Yongxin and Sun, Jun and Chen, Junjie},
title = {What Makes Open Source Software Projects Impactful: A Data-Driven Approach},
year = {2021},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457932},
doi = {10.1145/3457913.3457932},
abstract = {With the wide adoption and acceptance of open source version control and hosting systems, more and more companies, including Google, Microsoft, Apple and Facebook, are putting their projects on such platforms, e.g., GitHub. It is very important for open source projects to be impactful, i.e., to attract attentions from the open source development community, so as to gain support on development, testing as well as maintenance from the community. However, the question of what factors affect open source project impact, remains largely open. Given the numerous confounding factors and the complex correlations among the factors, it is a challenge to answer the question. In this study, we gather a large dataset from GitHub and provide empirical insights on this question base on a data-driven approach. We randomly collect 146,286 projects from GitHub and then adopt data analysis techniques to automatically analyze the correlations of different features with the software project impact. We also provide suggestions on how to potentially make open source projects impactful base on our analysis results.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {126–135},
numpages = {10},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@proceedings{10.1145/3616131,
title = {ICCBDC '23: Proceedings of the 2023 7th International Conference on Cloud and Big Data Computing},
year = {2023},
isbn = {9798400707339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manchester, United Kingdom}
}

@article{10.1145/3508484.3508486,
author = {Stafford, Tom and Tazkarji, Mohamed},
title = {Whole Lotta COVID Goin' On},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3508484.3508486},
doi = {10.1145/3508484.3508486},
abstract = {It's been a long year-and-a-half or since the initial outbreak arose down here in Louisiana, right after Mardi Gras 2020, February. New Orleans and its widely attended Mardi Gras looked to be the "spreader event" of our region, it seemed like.},
journal = {SIGMIS Database},
month = {dec},
pages = {7–8},
numpages = {2}
}

@article{10.1145/2830554,
author = {CACM Staff},
title = {Who Will Read PACM?},
year = {2015},
issue_date = {November 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2830554},
doi = {10.1145/2830554},
journal = {Commun. ACM},
month = {oct},
pages = {8–9},
numpages = {2}
}

@inproceedings{10.1145/3342559.3365333,
author = {Mast, Kai and Chen, Lequn and Sirer, Emin G\"{u}n},
title = {A Vision for Autonomous Blockchains Backed by Secure Hardware},
year = {2019},
isbn = {9781450368889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342559.3365333},
doi = {10.1145/3342559.3365333},
abstract = {Blockchains have emerged as a potential mechanism to enable immutable and consistent sharing of data across organizational boundaries. While much of the discussion on blockchains to date has been structured around public versus permissioned blockchains, both of these architectures have significant drawbacks. Public blockchains are energy inefficient, hard to scale and suffer from limited throughput and high latencies, while permissioned blockchains depend on specially designated nodes, potentially leak metainformation, and also suffer from scale and performance bottlenecks. This raises the question if blockchains, in their current form, are the only class of datastores that can provide such strong integrity guarantees.We introduce autonomous blockchains, an architecture based on free-standing, immutable, eidetic databases that implement independent timelines, linked together through interactions. Autonomous blockchains can be realized using trusted execution environments in combination with audit mechanisms. This architecture does not only provide block-chain-like integrity and auditability guarantees but also supports storing and querying private data. Further, multiple autonomous blockchains can be linked together through federated transactions to exchange data and order mutual operations. These transactions are amenable to audits and yield tamper-proof witnesses. Evaluation shows that this design can achieve high throughput while providing stronger integrity guarantees than conventional datastores.},
booktitle = {Proceedings of the 4th Workshop on System Software for Trusted Execution},
articleno = {1},
numpages = {6},
location = {Huntsville, Ontario, Canada},
series = {SysTEX '19}
}

@inproceedings{10.1145/3576914.3587524,
author = {Nice, Matthew and Bunting, Matthew and Work, Daniel and Sprinkle, Jonathan},
title = {Middleware for a Heterogeneous CAV Fleet},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576914.3587524},
doi = {10.1145/3576914.3587524},
abstract = {This paper introduces CAN to ROS, a model-based code generation tool used in development, testing, and deployment of a heterogeneous fleet of vehicles with robotic sensing in ROS. Code generation supports two main features: (1) self-configuration for deployment in a heterogeneous vehicle fleet, and (2) quick iteration for testing and development of reading vehicle sensors and robotic control. This tool features the ability to detect the vehicle it is in and regenerate and rebuild itself at runtime to provide the proper two-way bridge between ROS and the sensed on-board vehicle sensor network. Code generation relies on a per-model defined JSON to map a CAN database (DBC) to the desired ROS topic names and message types. The live ROS publishing of CAN messages allows for instant feedback, and the code regeneration allows for adjustments in DBC or vehicle JSON to iteratively hone in on new vehicle signals. Generated ROS nodes are written in C++ for runtime use in lightweight embedded computers. This has been tested in vehicles from three different Original Equipment Manufacturers (OEMs), and can be extended to support a wide array of vehicles. By using a unifying ROS specification, a heterogeneous set of vehicles can be unified into a fleet with abstracted model-specific details; this opens the door for developing cross-model software applications for vehicle control, connected vehicle applications, or fleet monitoring systems.},
booktitle = {Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
pages = {86–91},
numpages = {6},
keywords = {ROS, CAVs, Connected Automated Vehicles, CAN},
location = {San Antonio, TX, USA},
series = {CPS-IoT Week '23}
}

@inproceedings{10.1145/1052380.1052388,
author = {Sanneblad, Johan and Holmquist, Lars Erik},
title = {The GapiDraw Platform: High-Performance Cross-Platform Graphics on Mobile Devices},
year = {2004},
isbn = {1581139810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1052380.1052388},
doi = {10.1145/1052380.1052388},
abstract = {The GapiDraw platform supports the creation of high-performance graphical applications across a variety of handheld hardware configurations, including Palm, Symbian and Windows Mobile devices. Handheld computers makes it possible to create applications and services not possible with stationary computers, thus there is a need for a high performance development platform for rapid prototyping on mobile devices. Unlike desktop computers there has not yet evolved a single standard for graphics on handheld devices. Typically, handheld computers only provide direct frame buffer access, and there are major differences in implementation details across different hardware configurations, making it difficult to use mobile devices for prototyping. Using GapiDraw, developers can re-use the same code across a variety of devices and do not have to focus on device-specific implementation details. GapiDraw is actively used as an enabler platform in numerous research labs, and has also been used in over one hundred commercial games. We give an overview of the platform, and highlight some new mobile application concepts made possible through the use of GapiDraw.},
booktitle = {Proceedings of the 3rd International Conference on Mobile and Ubiquitous Multimedia},
pages = {47–53},
numpages = {7},
keywords = {graphics framework, graphics API, prototyping, mobile phones, mobile games, handheld computers, graphics middleware},
location = {College Park, Maryland, USA},
series = {MUM '04}
}

@inproceedings{10.1145/3383219.3383256,
author = {Kr\"{u}ger, Jacob and Nielebock, Sebastian and Heum\"{u}ller, Robert},
title = {How Can I Contribute? A Qualitative Analysis of Community Websites of 25 Unix-Like Distributions},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383256},
doi = {10.1145/3383219.3383256},
abstract = {Developers collaboratively implement large-scale industrial and open-source projects. Such projects pose several challenges for developers, as they require considerable knowledge about the project and its development processes, for instance, to fix bugs or implement new features. Understanding what information developer communities codify on how to contribute to their project is crucial, for example, to onboard new developers or for researchers to scope analysis techniques. In this paper, we report the results of a qualitative analysis of 25 Unix-like distributions, focusing on what information the communities codify publicly on contributing. The results reveal no dedicated strategies to codify information on contribution or development practices. Still, non-technical contributions are easy to identify, while information on the development is hard to collect---and mostly concerned with versioning and bug reporting. Our insights help to understand information-provisioning strategies, identify information sources, and scope analyses.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {324–329},
numpages = {6},
keywords = {Development, Bug fixing, Linux, Information provisioning, Unix},
location = {Trondheim, Norway},
series = {EASE '20}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@inproceedings{10.1145/3377811.3380412,
author = {Zhou, Shurui and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {How Has Forking Changed in the Last 20 Years? A Study of Hard Forks on GitHub},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380412},
doi = {10.1145/3377811.3380412},
abstract = {The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed changed dramatically, seeing them often as a positive noncompetitive alternative to the original project.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {445–456},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3295500.3356141,
author = {Li, Lingda and Chapman, Barbara},
title = {Compiler Assisted Hybrid Implicit and Explicit GPU Memory Management under Unified Address Space},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356141},
doi = {10.1145/3295500.3356141},
abstract = {To improve programmability and productivity, recent GPUs adopt a virtual memory address space shared with CPUs (e.g., NVIDIA's unified memory). Unified memory migrates the data management burden from programmers to system software and hardware, and enables GPUs to address datasets that exceed their memory capacity. Our experiments show that while the implicit data transfer of unified memory may bring better data movement efficiency, page fault overhead and data thrashing can erase its benefits. In this paper, we propose several user-transparent unified memory management schemes to 1) achieve adaptive implicit and explicit data transfer and 2) prevent data thrashing. Unlike previous approaches which mostly rely on the runtime and thus suffer from large overhead, we demonstrate the benefits of exploiting key information from compiler analyses, including data locality, access density, and target reuse distance, to accomplish our goal. We implement the proposed schemes to improve OpenMP GPU offloading performance. Our evaluation shows that our schemes improve the GPU performance and memory efficiency significantly.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {51},
numpages = {16},
keywords = {GPU, unified memory management, compiler analysis, runtime, OpenMP, reuse distance, implicit and explicit data transfer},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3379597.3387483,
author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
title = {An Empirical Study of Build Failures in the Docker Context},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387483},
doi = {10.1145/3379597.3387483},
abstract = {Docker containers have become the de-facto industry standard. Docker builds often break, and a large amount of efforts are put into troubleshooting broken builds. Prior studies have evaluated the rate at which builds in large organizations fail. However, little is known about the frequency and fix effort of failures that occur in Docker builds of open-source projects. This paper provides a first attempt to present a preliminary study on 857,086 Docker builds from 3,828 open-source projects hosted on GitHub. Using the Docker build data, we measure the frequency of broken builds and report their fix time. Furthermore, we explore the evolution of Docker build failures across time. Our findings help to characterize and understand Docker build failures and motivate the need for collecting more empirical evidence.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {76–80},
numpages = {5},
keywords = {Docker, Open-source, Build failure},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/2830168.2830172,
author = {Devresse, Adrien and Delalondre, Fabien and Sch\"{u}rmann, Felix},
title = {Nix Based Fully Automated Workflows and Ecosystem to Guarantee Scientific Result Reproducibility across Software Environments and Systems},
year = {2015},
isbn = {9781450340120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2830168.2830172},
doi = {10.1145/2830168.2830172},
abstract = {Reproducibility is a key requirement to scientific development. Any scientific process, including software simulations, must be able to be replicated in order to prove the robustness of its process and the validity of its results. If an approach based on the extensive documentation of the process itself maybe considered as sufficient to guarantee reproducibility of results in domains like Physics or Biology, such a requirement proves to be incomplete for software being executed on high performance computing platforms. The specifics of the customized and exotic HPC architectures, the fast evolution of the software development environment as well as the various variables that can pollute the software development and building process are just few of the many possible sources of scientific result corruption. We describe in this paper how the developers of the Blue Brain Project built a software development ecosystem based on the Nix packaging and build system in order to guarantee the full portability, traceability and reproducibility of scientific results.},
booktitle = {Proceedings of the 3rd International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering},
pages = {25–31},
numpages = {7},
keywords = {traceability, software engineering, high performance computing, software deployment, scientific reproducibility},
location = {Austin, Texas},
series = {SE-HPCCSE '15}
}

@inproceedings{10.1007/11424529_21,
author = {Akolkar, Rahul P. and Faruquie, Tanveer and Huerta, Juan and Kankar, Pankaj and Rajput, Nitendra and Raman, T. V. and Udupa, Raghavendra U. and Verma, Abhishek},
title = {Reusable Dialog Component Framework for Rapid Voice Application Development},
year = {2005},
isbn = {3540258779},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424529_21},
doi = {10.1007/11424529_21},
abstract = {Voice application development requires specialized speech related skills besides the general programming ability. Encapsulating the speech specific behavior and complexities in prepackaged, configurable User Interface (UI) components will ease and expedite the voice application development. These components can be used across applications and are called as Reusable Dialog Components (RDCs). In this paper we propose a programming model and the framework for developing reusable dialog components. Our framework facilitates the development of voice applications via the encapsulation of interaction mechanisms, the encapsulation of best-of-breed practices (ie. grammars, prompts, and configuration parameters), a modular design and through pluggable dialog management strategies. The framework extends the standard J2EE/JSP based programming model to make it suitable for voice applications.},
booktitle = {Proceedings of the 8th International Conference on Component-Based Software Engineering},
pages = {306–321},
numpages = {16},
location = {St. Louis, MO},
series = {CBSE'05}
}

@proceedings{10.1145/3528227,
title = {SERP4IoT '22: Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
year = {2022},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SERP4IoT begins to be recognised as an annual venue gathering researchers, industrials, and practitioners to share their vision, experience, and opinion on how to address the challenges of, find solutions for, and share experiences with the development, release, and testing of robust software systems for IoT devices.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/1254960.1254975,
author = {Bruns, Axel},
title = {Produsage},
year = {2007},
isbn = {9781595937124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1254960.1254975},
doi = {10.1145/1254960.1254975},
abstract = {This paper outlines the concept of produsage as a model of describing today's emerging user-led content creation environments. Produsage overcomes some of the systemic problems associated with translating industrial-age ideas of content production into an informational-age, social software, Web 2.0 environment. Instead, it offers new ways of understanding the collaborative content creation and development practices found in contemporary informational environments.},
booktitle = {Proceedings of the 6th ACM SIGCHI Conference on Creativity &amp; Cognition},
pages = {99–106},
numpages = {8},
keywords = {web 2.0, content, creativity, information age, produser, collaboration, produsage, social software, user-led},
location = {Washington, DC, USA},
series = {C&amp;C '07}
}

@inproceedings{10.1145/3503221.3508427,
author = {Reidys, Benjamin and Huang, Jian},
title = {Understanding and Detecting Deep Memory Persistency Bugs in NVM Programs with DeepMC},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508427},
doi = {10.1145/3503221.3508427},
abstract = {To facilitate programming with non-volatile memory (NVM), a set of memory persistency models, such as strict and epoch persistency, have been proposed. Although these models provide high-level guidance for reasoning about the data persistence, implementing them correctly is nontrivial. Our study of the well-developed NVM frameworks and libraries reveals that many of them have deep semantic bugs that are strongly relevant to the model specifications. Furthermore, it is difficult to detect them with existing testing and bug-finding tools.To further understand these persistency bugs, we conduct a characterization study, and present a taxonomy of these persistency bugs. We find that many persistency bugs are caused by the semantic mismatches between the model specifications and their real implementation in NVM programs. To identify these deep persistency bugs, we build a toolkit named DeepMC with both static and dynamic analysis. DeepMC is driven by a set of rules based on our characterization study and persistency model specifications. Our results show that DeepMC can efficiently pinpoint various persistency bugs in a variety of NVM programming frameworks/libraries, and their example programs, including PMDK and persistent memory file system (PMFS) from Intel, the NVM-Direct library from Oracle, and Mnemosyne framework from academia.},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {322–336},
numpages = {15},
keywords = {performance bugs, persistency bugs, memory persistency, non-volatile memory},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}

@inproceedings{10.1145/3387904.3389258,
author = {Stapleton, Sean and Gambhir, Yashmeet and LeClair, Alexander and Eberhart, Zachary and Weimer, Westley and Leach, Kevin and Huang, Yu},
title = {A Human Study of Comprehension and Code Summarization},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389258},
doi = {10.1145/3387904.3389258},
abstract = {Software developers spend a great deal of time reading and understanding code that is poorly-documented, written by other developers, or developed using differing styles. During the past decade, researchers have investigated techniques for automatically documenting code to improve comprehensibility. In particular, recent advances in deep learning have led to sophisticated summary generation techniques that convert functions or methods to simple English strings that succinctly describe that code's behavior. However, automatic summarization techniques are assessed using internal metrics such as BLEU scores, which measure natural language properties in translational models, or ROUGE scores, which measure overlap with human-written text. Unfortunately, these metrics do not necessarily capture how machine-generated code summaries actually affect human comprehension or developer productivity.We conducted a human study involving both university students and professional developers (n = 45). Participants reviewed Java methods and summaries and answered established program comprehension questions. In addition, participants completed coding tasks given summaries as specifications. Critically, the experiment controlled the source of the summaries: for a given method, some participants were shown human-written text and some were shown machine-generated text.We found that participants performed significantly better (p = 0.029) using human-written summaries versus machine-generated summaries. However, we found no evidence to support that participants perceive human- and machine-generated summaries to have different qualities. In addition, participants' performance showed no correlation with the BLEU and ROUGE scores often used to assess the quality of machine-generated summaries. These results suggest a need for revised metrics to assess and guide automatic summarization techniques.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {2–13},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.14778/3461535.3461552,
author = {Wang, Xiaoying and Qu, Changbo and Wu, Weiyuan and Wang, Jiannan and Zhou, Qingqing},
title = {Are We Ready for Learned Cardinality Estimation?},
year = {2021},
issue_date = {May 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3461535.3461552},
doi = {10.14778/3461535.3461552},
abstract = {Cardinality estimation is a fundamental but long unresolved problem in query optimization. Recently, multiple papers from different research groups consistently report that learned models have the potential to replace existing cardinality estimators. In this paper, we ask a forward-thinking question: Are we ready to deploy these learned cardinality models in production? Our study consists of three main parts. Firstly, we focus on the static environment (i.e., no data updates) and compare five new learned methods with nine traditional methods on four real-world datasets under a unified workload setting. The results show that learned models are indeed more accurate than traditional methods, but they often suffer from high training and inference costs. Secondly, we explore whether these learned models are ready for dynamic environments (i.e., frequent data updates). We find that they cannot catch up with fast data updates and return large errors for different reasons. For less frequent updates, they can perform better but there is no clear winner among themselves. Thirdly, we take a deeper look into learned models and explore when they may go wrong. Our results show that the performance of learned methods can be greatly affected by the changes in correlation, skewness, or domain size. More importantly, their behaviors are much harder to interpret and often unpredictable. Based on these findings, we identify two promising research directions (control the cost of learned models and make learned models trustworthy) and suggest a number of research opportunities. We hope that our study can guide researchers and practitioners to work together to eventually push learned cardinality estimators into real database systems.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {1640–1654},
numpages = {15}
}

@proceedings{10.1145/3528575,
title = {MobileHCI '22: Adjunct Publication of the 24th International Conference on Human-Computer Interaction with Mobile Devices and Services},
year = {2022},
isbn = {9781450393416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/1408647.1408648,
author = {Agostinho, S\'{e}rgio and Moreira, Ana and Guerreiro, Pedro},
title = {Contracts for Aspect-Oriented Design},
year = {2008},
isbn = {9781605581446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1408647.1408648},
doi = {10.1145/1408647.1408648},
abstract = {The Java approach to Object-Oriented Design by Contract can be extended and applied to Aspect-Oriented Software. For doing so, we need to address how current Object-Oriented Design by Contract can be modified to tackle aspect advising, as well as to actually extend the Design by Contract approach for aspect modules. Our approach is general enough to be applied to Java Aspect-Oriented extensions such as AspectJ, CaesarJ, and FuseJ.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Software Engineering Properties of Languages and Aspect Technologies},
articleno = {1},
numpages = {6},
keywords = {object-oriented programming, object-oriented design, aspect-oriented programming, aspect-oriented design, design by contract},
location = {Brussels, Belgium},
series = {SPLAT '08}
}

@inproceedings{10.1145/3581784.3627042,
author = {Ltaief, Hatem and Hong, Yuxi and Wilson, Leighton and Jacquelin, Mathias and Ravasi, Matteo and Keyes, David Elliot},
title = {Scaling the “Memory Wall” for Multi-Dimensional Seismic Processing with Algebraic Compression on Cerebras CS-2 Systems},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3627042},
doi = {10.1145/3581784.3627042},
abstract = {We exploit the high memory bandwidth of AI-customized Cerebras CS-2 systems for seismic processing. By leveraging low-rank matrix approximation, we fit memory-hungry seismic applications onto memory-austere SRAM wafer-scale hardware, thus addressing a challenge arising in many wave-equation-based algorithms that rely on Multi-Dimensional Convolution (MDC) operators. Exploiting sparsity inherent in seismic data in the frequency domain, we implement embarrassingly parallel tile low-rank matrix-vector multiplications (TLR-MVM), which account for most of the elapsed time in MDC operations, to successfully solve the Multi-Dimensional Deconvolution (MDD) inverse problem. By reducing memory footprint along with arithmetic complexity, we fit a standard seismic benchmark dataset into the small local memories of Cerebras processing elements. Deploying TLR-MVM execution onto 48 CS-2 systems in support of MDD gives a sustained memory bandwidth of 92.58PB/s on 35, 784, 000 processing elements, a significant milestone that highlights the capabilities of AI-customized architectures to enable a new generation of seismic algorithms that will empower multiple technologies of our low-carbon future.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {6},
numpages = {12},
keywords = {extreme parallelism, low-carbon energy applications, energy efficiency, AI-optimized architecture, high memory bandwidth, seismic processing, low-rank matrix approximation},
location = {Denver, CO, USA},
series = {SC '23}
}

@inproceedings{10.1145/2390045.2390048,
author = {Mansmann, Svetlana and Rehman, Nafees Ur and Weiler, Andreas and Scholl, Marc H.},
title = {Discovering OLAP Dimensions in Semi-Structured Data},
year = {2012},
isbn = {9781450317214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390045.2390048},
doi = {10.1145/2390045.2390048},
abstract = {With the standard OLAP technology, cubes are constructed from the input data based on the available data fields and known relationships between them. Structuring the data into a set of numeric measures distributed along a set of uniformly structured dimensions may be unrealistic for applications dealing with semi-structured data. We propose to extend the capabilities of OLAP via content-driven discovery of measures and dimensional characteristics in the original dataset. New structural elements are discovered by means of data mining and other techniques and are therefore prone to changes as the underlying dataset evolves. In this work we focus on the challenge of generating, maintaining, and querying such discovered elements of the cube.We demonstrate the benefits of our approach by providing OLAP to the public stream of user-generated content of the popular microblogging service Twitter. We were able to enrich the original set by discovering dynamic characteristics such as user activity, popularity, messaging behavior, as well as classifying messages by topic, impact, origin, method of generation, etc. Application of knowledge discovery techniques coupled with human expertise enable structural enrichment of the original data beyond the scope of the existing methods for generating multidimensional models from relational or semi-structured data.},
booktitle = {Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP},
pages = {9–16},
numpages = {8},
keywords = {OLAP, semi-structured data, data mining, multidimensional data model},
location = {Maui, Hawaii, USA},
series = {DOLAP '12}
}

@inproceedings{10.1145/3274192.3274225,
author = {Chagas, Bruno A. and Redmiles, David F. and de Souza, Clarisse S.},
title = {Observed Appropriation of IoT Technology: A Semiotic Account},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274225},
doi = {10.1145/3274192.3274225},
abstract = {The Internet of Things (IoT) is an emerging technology powered by smart devices that increasingly pervades our environments and practices. The process of incorporating technology into one's practices involves use and adaptation of the technology and is usually referred to as appropriation. In research about IoT, appropriation of IoT technology is a rather new topic. It is necessary to understand how appropriation takes place in order to be able to provide strong support for it in the design of technology. In this paper, we present the initial results of an in-situ study observing how people use and adapt commercially available smart home technology, in practice, over time. We took Semiotic Engineering as a theoretical lens to understand the appropriation we observed in actual settings. Our contribution is to propose a semiotic account of appropriation and two derived qualities that can help designers address appropriation support in IoT technologies.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {33},
numpages = {10},
keywords = {Semiotic Engineering, Internet of Things, Appropriation},
location = {Bel\'{e}m, Brazil},
series = {IHC 2018}
}

@article{10.1145/2499962.2499963,
author = {Gill, T. Grandon and Hevner, Alan R.},
title = {A Fitness-Utility Model for Design Science Research},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2499962.2499963},
doi = {10.1145/2499962.2499963},
abstract = {Current thinking in design science research (DSR) defines the usefulness of the design artifact in a relevant problem environment as the primary research goal. Here we propose a complementary evaluation model for DSR. Drawing from evolutionary economics, we define a fitness-utility model that better captures the evolutionary nature of design improvements and the essential DSR nature of searching for a satisfactory design across a fitness landscape. Our goal is to move DSR to more meaningful evaluations of design artifacts for sustainable impacts. A key premise of this new thinking is that the evolutionary fitness of a design artifact is more valuable than its immediate usefulness. We conclude with a discussion of the strengths and challenges of the fitness-utility model for the performance of rigorous and relevant DSR.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {5},
numpages = {24},
keywords = {Design science research, sustainability, research paradigms, evolutionary economics, utility, fitness}
}

@inproceedings{10.1145/3368089.3409745,
author = {Gao, Jun and Li, Li and Kong, Pingfan and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
title = {Borrowing Your Enemy’s Arrows: The Case of Code Reuse in Android via Direct Inter-App Code Invocation},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409745},
doi = {10.1145/3368089.3409745},
abstract = {The Android ecosystem offers different facilities to enable communication among app components and across apps to ensure that rich services can be composed through functionality reuse. At the heart of this system is the Inter-component communication (ICC) scheme, which has been largely studied in the literature. Less known in the community is another powerful mechanism that allows for direct inter-app code invocation which opens up for different reuse scenarios, both legitimate or malicious. This paper exposes the general workflow for this mechanism, which beyond ICCs, enables app developers to access and invoke functionalities (either entire Java classes, methods or object fields) implemented in other apps using official Android APIs. We experimentally showcase how this reuse mechanism can be leveraged to “plagiarize" supposedly-protected functionalities. Typically, we were able to leverage this mechanism to bypass security guards that a popular video broadcaster has placed for preventing access to its video database from outside its provided app. We further contribute with a static analysis toolkit, named DICIDer, for detecting direct inter-app code invocations in apps. An empirical analysis of the usage prevalence of this reuse mechanism is then conducted. Finally, we discuss the usage contexts as well as the implications of this studied reuse mechanism.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {939–951},
numpages = {13},
keywords = {DICI, Java Reflection, Android},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2785592.2795364,
author = {Kishida, Kouichi},
title = {Process Model for Open Innovation},
year = {2015},
isbn = {9781450333467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785592.2795364},
doi = {10.1145/2785592.2795364},
abstract = {This position paper presents some useful metaphors to think about new conceptual model for open innovation process.},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process},
pages = {185–186},
numpages = {2},
keywords = {Rhizome Model, Immaterial Labor, Polyphony, Process Innovation},
location = {Tallinn, Estonia},
series = {ICSSP 2015}
}

@article{10.1145/2885256,
author = {Wang, Xi and Zeldovich, Nickolai and Kaashoek, M. Frans and Solar-Lezama, Armando},
title = {A Differential Approach to Undefined Behavior Detection},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/2885256},
doi = {10.1145/2885256},
journal = {Commun. ACM},
month = {feb},
pages = {99–106},
numpages = {8}
}

@article{10.1145/2148436.2148452,
author = {Levine, Trudy},
title = {Reusable Software Components},
year = {2012},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1094-3641},
url = {https://doi.org/10.1145/2148436.2148452},
doi = {10.1145/2148436.2148452},
abstract = {This column consists of our June 2011 listing of sources for reusable software components. All information is obtained directly from web sites or from the parties involved. As always, no recommendation or guarantee by this column is implied.},
journal = {Ada Lett.},
month = {feb},
pages = {59–69},
numpages = {11}
}

@article{10.1145/2007461.2007476,
author = {Levine, Trudy},
title = {Reusable Software Components},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/2007461.2007476},
doi = {10.1145/2007461.2007476},
abstract = {This column consists of our June 2011 listing of sources for reusable software components. All information is obtained directly from web sites or from the parties involved. As always, no recommendation or guarantee by this column is implied.We would appreciate comments, corrections, and suggestions from our readers.},
journal = {Ada Lett.},
month = {jul},
pages = {53–63},
numpages = {11}
}

@inproceedings{10.1145/1370750.1370763,
author = {Gobeille, Robert},
title = {The FOSSology Project},
year = {2008},
isbn = {9781605580241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370750.1370763},
doi = {10.1145/1370750.1370763},
abstract = {By its nature, the availability of FOSS has given computer scientists a large body of software and software projects to analyze. By having available source, version control system metadata, and open project communities, much can be learned about a software project, software development and collaborative project development. The goal of the FOSSology project is to create a public, open source software repository, together with tools to facilitate analysis, storage, and sharing of open source software and its metadata. FOSSology does license detection today.},
booktitle = {Proceedings of the 2008 International Working Conference on Mining Software Repositories},
pages = {47–50},
numpages = {4},
keywords = {license analysis},
location = {Leipzig, Germany},
series = {MSR '08}
}

@article{10.14778/3603581.3603604,
author = {Pedreira, Pedro and Erling, Orri and Karanasos, Konstantinos and Schneider, Scott and McKinney, Wes and Valluri, Satya R and Zait, Mohamed and Nadeau, Jacques},
title = {The Composable Data Management System Manifesto},
year = {2023},
issue_date = {June 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3603581.3603604},
doi = {10.14778/3603581.3603604},
abstract = {The requirement for specialization in data management systems has evolved faster than our software development practices. After decades of organic growth, this situation has created a siloed landscape composed of hundreds of products developed and maintained as monoliths, with limited reuse between systems. This fragmentation has resulted in developers often reinventing the wheel, increased maintenance costs, and slowed down innovation. It has also affected the end users, who are often required to learn the idiosyncrasies of dozens of incompatible SQL and non-SQL API dialects, and settle for systems with incomplete functionality and inconsistent semantics. In this vision paper, considering the recent popularity of open source projects aimed at standardizing different aspects of the data stack, we advocate for a paradigm shift in how data management systems are designed. We believe that by decomposing these into a modular stack of reusable components, development can be streamlined while creating a more consistent experience for users. Towards that goal, we describe the state-of-the-art, principal open source technologies, and highlight open questions and areas where additional research is needed. We hope this work will foster collaboration, motivate further research, and promote a more composable future for data management.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {2679–2685},
numpages = {7}
}

@inproceedings{10.1109/ICSE.2019.00089,
author = {Nguyen, Hoan Anh and Nguyen, Tien N. and Dig, Danny and Nguyen, Son and Tran, Hieu and Hilton, Michael},
title = {Graph-Based Mining of in-the-Wild, Fine-Grained, Semantic Code Change Patterns},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00089},
doi = {10.1109/ICSE.2019.00089},
abstract = {Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies.We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {819–830},
numpages = {12},
keywords = {semantic change pattern mining, graph mining},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/1095408.1095420,
author = {Barroso, Luiz Andr\'{e}},
title = {The Price of Performance: An Economic Case for Chip Multiprocessing},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {7},
issn = {1542-7730},
url = {https://doi.org/10.1145/1095408.1095420},
doi = {10.1145/1095408.1095420},
abstract = {In the late 1990s, our research group at DEC was one of a growing number of teams advocating the CMP (chip multiprocessor) as an alternative to highly complex single-threaded CPUs. We were designing the Piranha system,1 which was a radical point in the CMP design space in that we used very simple cores (similar to the early RISC designs of the late ’80s) to provide a higher level of thread-level parallelism. Our main goal was to achieve the best commercial workload performance for a given silicon budget. Today, in developing Google’s computing infrastructure, our focus is broader than performance alone. The merits of a particular architecture are measured by answering the following question: Are you able to afford the computational capacity you need? The high-computational demands that are inherent in most of Google’s services have led us to develop a deep understanding of the overall cost of computing, and continually to look for hardware/software designs that optimize performance per unit of cost.},
journal = {Queue},
month = {sep},
pages = {48–53},
numpages = {6}
}

@article{10.14778/3476311.3476390,
author = {Power, Conor and Patel, Hiren and Jindal, Alekh and Leeka, Jyoti and Jenkins, Bob and Rys, Michael and Triou, Ed and Zhu, Dexin and Katahanas, Lucky and Talapady, Chakrapani Bhat and Rowe, Joshua and Zhang, Fan and Draves, Rich and Friedman, Marc and Filho, Ivan Santa Maria and Kumar, Amrish},
title = {The Cosmos Big Data Platform at Microsoft: Over a Decade of Progress and a Decade to Look Forward},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476390},
doi = {10.14778/3476311.3476390},
abstract = {The twenty-first century has been dominated by the need for large scale data processing, marking the birth of big data platforms such as Cosmos. This paper describes the evolution of the exabyte-scale Cosmos big data platform at Microsoft; our journey right from scale and reliability all the way to efficiency and usability, and our next steps towards improving security, compliance, and support for heterogeneous analytics scenarios. We discuss how the evolution of Cosmos parallels the evolution of the big data field, and how the changes in the Cosmos workloads over time parallel the changing requirements of users across industry.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3148–3161},
numpages = {14}
}

@inproceedings{10.1145/3368089.3409711,
author = {Larios Vargas, Enrique and Aniche, Maur\'{\i}cio and Treude, Christoph and Bruntink, Magiel and Gousios, Georgios},
title = {Selecting Third-Party Libraries: The Practitioners’ Perspective},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409711},
doi = {10.1145/3368089.3409711},
abstract = {The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision.  In this paper, we study the factors that influence the selection process of libraries, as perceived by industry developers. To that aim, we perform a cross-sectional interview study with 16 developers from 11 different businesses and survey 115 developers that are involved in the selection of libraries. We systematically devised a comprehensive set of 26 technical, human, and economic factors that developers take into consideration when selecting a software library. Eight of these factors are new to the literature. We explain each of these factors and how they play a role in the decision. Finally, we discuss the implications of our work to library maintainers, potential library users, package manager developers, and empirical software engineering researchers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {245–256},
numpages = {12},
keywords = {library selection, APIs, library adoption, software libraries, empirical software engineering},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2593882.2593898,
author = {Murphy-Hill, Emerson and Grossman, Dan},
title = {How Programming Languages Will Co-Evolve with Software Engineering: A Bright Decade Ahead},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593898},
doi = {10.1145/2593882.2593898},
abstract = {Programming languages are an indispensable foundation of software engineering, so it is essential that innovations in software engineering anticipate and influence innovations in programming languages and vice-versa. We discuss seven emerging trends in the design, adoption, and use of programming languages that have clear and valuable overlap with software engineering. These themes include language design that assumes modern development ecosystems; languages that support multiple views; data-driven language design; formal and machine-checked verification that works for real systems; gradual typing; languages that embrace a distributed, asynchronous world of large external data sources; and the increasing influence of functional-programming concepts. We discuss how the time is now for software-engineering research to influence and improve these significant changes to the field.},
booktitle = {Future of Software Engineering Proceedings},
pages = {145–154},
numpages = {10},
keywords = {Programming languages, future, software engineering},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3382494.3422166,
author = {Hazhirpasand, Mohammadreza and Ghafari, Mohammad and Nierstrasz, Oscar},
title = {Java Cryptography Uses in the Wild},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422166},
doi = {10.1145/3382494.3422166},
abstract = {[Background] Previous research has shown that developers commonly misuse cryptography APIs. [Aim] We have conducted an exploratory study to find out how crypto APIs are used in open-source Java projects, what types of misuses exist, and why developers make such mistakes. [Method] We used a static analysis tool to analyze hundreds of open-source Java projects that rely on Java Cryptography Architecture, and manually inspected half of the analysis results to assess the tool results. We also contacted the maintainers of these projects by creating an issue on the GitHub repository of each project, and discussed the misuses with developers. [Results] We learned that 85% of Cryptography APIs are misused, however, not every misuse has severe consequences. Developer feedback showed that security caveats in the documentation of crypto APIs are rare, developers may overlook misuses that originate in third-party code, and the context where a Crypto API is used should be taken into account. [Conclusion] We conclude that using Crypto APIs is still problematic for developers but blindly blaming them for such misuses may lead to erroneous conclusions.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {40},
numpages = {6},
keywords = {Java cryptography, empirical study, security},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.5555/948785.948817,
author = {Zaritski, Roman M.},
title = {Using Open Source Software for Scientific Simulations, Data Visualization, and Publishing},
year = {2003},
issue_date = {December 2003},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {19},
number = {2},
issn = {1937-4771},
abstract = {There is a growing number of software packages that can be legally downloaded from the Internet and used in educational and research projects under licenses that involve no costs and few practical restrictions. Primarily, this is open source software. Using an investigation of waves in simulated excitable media as a case study, it is shown that an extensive project, from parallel computer simulations and data visualization stages to the final publication preparation stage, can be carried out completely based on free software. This creates favorable research and educational opportunities in low budget environments.},
journal = {J. Comput. Sci. Coll.},
month = {dec},
pages = {218–222},
numpages = {5}
}

@article{10.1145/3498568,
author = {Xin, Reynold and McKinney, Wes and Gates, Alan and McCubbin, Chris},
title = {It Takes a Community---: The Open Source Challenge},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3498568},
doi = {10.1145/3498568},
abstract = {A discussion with Reynold Xin, Wes McKinney, Alan Gates, and Chris McCubbin.},
journal = {Commun. ACM},
month = {apr},
pages = {48–55},
numpages = {8}
}

@article{10.1145/1378727.1388950,
author = {Baraniuk, Richard G. and Burrus, C. Sidney},
title = {ViewpointGlobal Warming toward Open Educational Resources},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/1378727.1388950},
doi = {10.1145/1378727.1388950},
abstract = {Seeking to realize the potential for significantly improving and advancing the world's standard of education.},
journal = {Commun. ACM},
month = {sep},
pages = {30–32},
numpages = {3}
}

@inproceedings{10.1145/2967360.2967366,
author = {Xiang, Yong and Li, Hu and Wang, Sen and Chen, Charley Peter and Xu, Wei},
title = {Debugging OpenStack Problems Using a State Graph Approach},
year = {2016},
isbn = {9781450342650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967360.2967366},
doi = {10.1145/2967360.2967366},
abstract = {It is hard to operate and debug systems like OpenStack that integrate many independently developed modules with multiple levels of abstractions. A major challenge is to navigate through the complex dependencies and relationships of the states in different modules or subsystems, to ensure the correctness and consistency of these states. We present a system that captures the runtime states and events from the entire OpenStack-Ceph stack, and automatically organizes these data into a graph that we call system operation state graph (SOSG). With SOSG we can use intuitive graph traversal techniques to solve problems like reasoning about the state of a virtual machine. Also, using a graph-based anomaly detection, we can automatically discover hidden problems in OpenStack. We have a scalable implementation of SOSG, and evaluate the approach on a 125-node production OpenStack cluster, finding a number of interesting problems.},
booktitle = {Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems},
articleno = {13},
numpages = {8},
keywords = {debugging, anomaly detection, OpenStack, state graph},
location = {Hong Kong, Hong Kong},
series = {APSys '16}
}

@article{10.1145/2699678,
author = {Wang, Xi and Zeldovich, Nickolai and Kaashoek, M. Frans and Solar-Lezama, Armando},
title = {A Differential Approach to Undefined Behavior Detection},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/2699678},
doi = {10.1145/2699678},
abstract = {This article studies undefined behavior arising in systems programming languages such as C/C++. Undefined behavior bugs lead to unpredictable and subtle systems behavior, and their effects can be further amplified by compiler optimizations. Undefined behavior bugs are present in many systems, including the Linux kernel and the Postgres database. The consequences range from incorrect functionality to missing security checks. This article proposes a formal and practical approach that finds undefined behavior bugs by finding “unstable code” in terms of optimizations that leverage undefined behavior. Using this approach, we introduce a new static checker called Stack that precisely identifies undefined behavior bugs. Applying Stack to widely used systems has uncovered 161 new bugs that have been confirmed and fixed by developers.},
journal = {ACM Trans. Comput. Syst.},
month = {mar},
articleno = {1},
numpages = {29},
keywords = {compiler optimizations, Undefined behavior}
}

@inproceedings{10.1145/3613372.3613413,
author = {Dantas, Carlos and Rocha, Adriano and Maia, Marcelo},
title = {Assessing the Readability of ChatGPT Code Snippet Recommendations: A Comparative Study},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613413},
doi = {10.1145/3613372.3613413},
abstract = {Developers often rely on code search engines to find high-quality and reusable code snippets online, such as those available on Stack Overflow. Recently, ChatGPT, a language model trained for dialog tasks, has been gaining attention as a promising approach for code snippet generation. However, there is still a need for in-depth analysis of the quality of its recommendations. In this work, we propose the evaluation of the readability of code snippets generated by ChatGPT, comparing them with those recommended by CROKAGE, a state-of-the-art code search engine for Stack Overflow. We compare the recommended snippets of both approaches using readability issues raised by the automated static analysis tool (ASAT) SonarQube. Our results show that ChatGPT can generate cleaner code snippets and more consistent naming and code conventions than those written by humans and recommended by CROKAGE. However, in some cases, ChatGPT generates code that lacks recent features from Java API such as try-with-resources, lambdas, and others. Overall, our findings suggest that ChatGPT can provide valuable assistance to developers searching for didactic and high-quality code snippets online. However, it is still important for developers to review the generated code, either manually or assisted by an ASAT, to prevent potential readability issues, as the correctness of the generated code snippets.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {283–292},
numpages = {10},
keywords = {code snippets, Stack Overflow, readability, ChatGPT, SonarQube},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3276945.3276968,
author = {Herrera, David and Chen, Hanfeng and Lavoie, Erick and Hendren, Laurie},
title = {Numerical Computing on the Web: Benchmarking for the Future},
year = {2018},
isbn = {9781450360302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276945.3276968},
doi = {10.1145/3276945.3276968},
abstract = {Recent advances in execution environments for JavaScript and WebAssembly that run on a broad range of devices, from workstations and mobile phones to IoT devices, provide new opportunities for portable and web-based numerical computing. Indeed, numerous numerical libraries and applications are emerging on the web, including Tensorflow.js, JSMapReduce, and the NLG Protein Viewer. This paper evaluates the current performance of numerical computing on the web, including both JavaScript and WebAssembly, over a wide range of devices from workstations to IoT devices. We developed a new benchmarking approach, which allowed us to perform centralized benchmarking, including benchmarking on mobile and IoT devices. Using this approach we performed four performance studies using the Ostrich benchmark suite, a collection of numerical programs representing the numerical dwarf categories identified by Colella. We studied the performance evolution of JavaScript, the relative performance of WebAssembly, the performance of server-side Node.js, and a comprehensive performance showdown for a wide range of devices.},
booktitle = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {88–100},
numpages = {13},
keywords = {JavaScript, performance, Ostrich benchmarks, scientific web benchmarking, web-based scientific computation, WebAssembly},
location = {Boston, MA, USA},
series = {DLS 2018}
}

@inproceedings{10.1145/3615335.3623004,
author = {Oswal, Sushil K and Oswal, Hitender K},
title = {A Story of Four Documentation Designs for One Medical Device: User Experience Analysis and Recommendations},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615335.3623004},
doi = {10.1145/3615335.3623004},
abstract = {This paper critiques the design of four instruction manuals in diverse modalities developed for a COVID-19 home test kit. Drawing from usability and user experience research and critical document analysis tools, the paper systematically describes the visual design and content of the print instruction manuals. Employing accessible design theory embedded in video evaluation and web accessibility guidelines from WCAG 2.0, it also analyzes a mobile app and a video guide included in this COVID-19 home test kit. The paper raises a number of questions about designing pandemic era emergency instructions and communications for blind users. The authors emphasize that the context of use is central to designing accessible instructional media in medical devices for disabled populations. The paper further stresses the need to understand the overall ecosystems within which products and their documentations function for these users, which have been complicated by the mixing of print and digital interfaces.},
booktitle = {Proceedings of the 41st ACM International Conference on Design of Communication},
pages = {10–17},
numpages = {8},
keywords = {Blind users, Documentation design, Medical devices, User experience, Context of use, COVID-19 home test},
location = {Orlando, FL, USA},
series = {SIGDOC '23}
}

@inproceedings{10.1145/1867635.1867649,
author = {Shubina, Anna and Bratus, Sergey and Ingersol, Wyllys and Smith, Sean W.},
title = {The Diversity of TPMs and Its Effects on Development: A Case Study of Integrating the TPM into OpenSolaris},
year = {2010},
isbn = {9781450300957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1867635.1867649},
doi = {10.1145/1867635.1867649},
abstract = {Broad adoption of secure programming primitives such as the TPM can be hurt by programmer confusion regarding the nature and representation of failures when using a primitive. Conversely, a clear understanding of the primitive's failure modes is essential for both debugging and reducing the attack surface in the mechanisms built on it. In particular, differences in error processing and reporting logic significantly detract from such understanding.We present a case study of diversity in TPM behaviors and its effects on a TSS implementation, which emerged from the Sun/Dartmouth TCG/OpenSolaris project, one of the goals of which was instrumenting TPM support on Solaris. At the start of the project, both parties believed the instrumentation to be well-defined and, although time-consuming, relatively straightforward. In the course of the project we had to reexamine our assumptions concerning the state of the hardware and the software involved and the view of the system as presented to someone unfamiliar with its internals. We describe some failure modes we encountered and suggest directions for remediation.},
booktitle = {Proceedings of the Fifth ACM Workshop on Scalable Trusted Computing},
pages = {85–90},
numpages = {6},
keywords = {trusted computing (tc), trusted platform module (tpm)},
location = {Chicago, Illinois, USA},
series = {STC '10}
}

@inproceedings{10.1145/2568225.2568282,
author = {Vakilian, Mohsen and Johnson, Ralph E.},
title = {Alternate Refactoring Paths Reveal Usability Problems},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568282},
doi = {10.1145/2568225.2568282},
abstract = {Modern Integrated Development Environments (IDEs) support many refactorings. Yet, programmers greatly underuse automated refactorings. Recent studies have applied traditional usability testing methodologies such as surveys, lab studies, and interviews to find the usability problems of refactoring tools. However, these methodologies can identify only certain kinds of usability problems. The critical incident technique (CIT) is a general methodology that uncovers usability problems by analyzing troubling user interactions. We adapt CIT to refactoring tools and show that alternate refactoring paths are indicators of the usability problems of refactoring tools. We define an alternate refactoring path as a sequence of user interactions that contains cancellations, reported messages, or repeated invocations of the refactoring tool. We evaluated our method on a large corpus of refactoring usage data, which we collected during a field study on 36 programmers over three months. This method revealed 15 usability problems, 13 of which were previously unknown. We reported these problems and proposed design improvements to Eclipse developers. The developers acknowledged all of the problems and have already fixed four of them. This result suggests that analyzing alternate paths is effective at discovering the usability problems of interactive program transformation (IPT) tools.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {1106–1116},
numpages = {11},
keywords = {empirical, Refactoring, usability, evaluation, critical incident},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@article{10.1145/1555392.1555396,
author = {Rajan, Hridesh and Sullivan, Kevin J.},
title = {Unifying Aspect- and Object-Oriented Design},
year = {2009},
issue_date = {August 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/1555392.1555396},
doi = {10.1145/1555392.1555396},
abstract = {The contribution of this work is the design and evaluation of a programming language model that unifies aspects and classes as they appear in AspectJ-like languages. We show that our model preserves the capabilities of AspectJ-like languages, while improving the conceptual integrity of the language model and the compositionality of modules. The improvement in conceptual integrity is manifested by the reduction of specialized constructs in favor of uniform orthogonal constructs. The enhancement in compositionality is demonstrated by better modularization of integration and higher-order crosscutting concerns.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {aug},
articleno = {3},
numpages = {41},
keywords = {Classpect, Eos, unified aspect language model, instance-level advising, first class aspect instances, binding, aspect-oriented programming}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@inproceedings{10.1145/3433174.3433611,
author = {Ovasapyan, Tigran and Moskvin, Dmitry and Tsvetkov, Artem},
title = {Detection of Attacks on the Internet of Things Based on Intelligent Analysis of Devices Functioning Indicators},
year = {2021},
isbn = {9781450387514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433174.3433611},
doi = {10.1145/3433174.3433611},
abstract = {The work is devoted to the effectiveness study of applying intelligent data analysis to devices functioning indicators in order to detect attacks on the Internet of Things networks.The research work included a study on basic principles of functioning of the Internet of Things networks and a comparative analysis of their existing platforms. A new method of attack detection for the Internet of Things networks through the analysis of device functioning indicators was suggested.The method is based on the use of machine learning methods. All research data was collected in an emulator during the simulation of common attacks on the Internet of Things networks.},
booktitle = {13th International Conference on Security of Information and Networks},
articleno = {3},
numpages = {7},
keywords = {Security, Cyber-Physical Systems, Machine Learning, Attacks Detection, Internet of Things},
location = {Merkez, Turkey},
series = {SIN 2020}
}

@article{10.1145/2934240.2934245,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2934240.2934245},
doi = {10.1145/2934240.2934245},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {11–18},
numpages = {8}
}

@inproceedings{10.1145/1367497.1367564,
author = {Mannan, Mohammad and van Oorschot, Paul C.},
title = {Privacy-Enhanced Sharing of Personal Content on the Web},
year = {2008},
isbn = {9781605580852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1367497.1367564},
doi = {10.1145/1367497.1367564},
abstract = {Publishing personal content on the web is gaining increased popularity with dramatic growth in social networking websites, and availability of cheap personal domain names and hosting services. Although the Internet enables easy publishing of any content intended to be generally accessible, restricting personal content to a selected group of contacts is more difficult. Social networking websites partially enable users to restrict access to a selected group of users of the same network by explicitly creating a "friends' list." While this limited restriction supports users' privacy on those (few) selected websites, personal websites must still largely be protected manually by sharing passwords or obscure links. Our focus is the general problem of privacy-enabled web content sharing from any user-chosen web server. By leveraging the existing "circle of trust" in popular Instant Messaging (IM) networks, we propose a scheme called IM-based Privacy-Enhanced Content Sharing (IMPECS) for personal web content sharing. IMPECS enables a publishing user's personal data to be accessible only to her IM contacts. A user can put her personal web page on any web server she wants (vs. being restricted to a specific social networking website), and maintain privacy of her content without requiring site-specific passwords. Our prototype of IMPECS required only minor modifications to an IM server, and PHP scripts on a web server. The general idea behind IMPECS extends beyond IM and IM circles of trust; any equivalent scheme, (ideally) containing pre-arranged groups, could similarly be leveraged.},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
pages = {487–496},
numpages = {10},
keywords = {access control, circle of trust, sharing, privacy},
location = {Beijing, China},
series = {WWW '08}
}

@inproceedings{10.1145/3581783.3612444,
author = {Lee, Joo Chan and Rho, Daniel and Ko, Jong Hwan and Park, Eunbyung},
title = {FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612444},
doi = {10.1145/3581783.3612444},
abstract = {Neural fields, also known as coordinate-based or implicit neural representations, have shown a remarkable capability of representing, generating, and manipulating various forms of signals. For video representations, however, mapping pixel-wise coordinates to RGB colors has shown relatively low compression performance and slow convergence and inference speed. Frame-wise video representation, which maps a temporal coordinate to its entire frame, has recently emerged as an alternative method to represent videos, improving compression rates and encoding speed. While promising, it has still failed to reach the performance of state-of-the-art video compression algorithms. In this work, we propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to exploit the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, improving the continuity of spatial features. Experimental results show that FFNeRV yields the best performance for video compression and frame interpolation among the methods using frame-wise representations or neural fields. To reduce the model size even further, we devise a more compact convolutional architecture using the group and pointwise convolutions. With model compression techniques, including quantization-aware training and entropy coding, FFNeRV outperforms widely-used standard video codecs (H.264 and HEVC) and performs on par with state-of-the-art video compression algorithms.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {7859–7870},
numpages = {12},
keywords = {video representation, neural representation, video compression},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1109/ISCA52012.2021.00010,
author = {Jouppi, Norman P. and Yoon, Doe Hyun and Ashcraft, Matthew and Gottscho, Mark and Jablin, Thomas B. and Kurian, George and Laudon, James and Li, Sheng and Ma, Peter and Ma, Xiaoyu and Norrie, Thomas and Patil, Nishant and Prasad, Sushma and Young, Cliff and Zhou, Zongwei and Patterson, David},
title = {Ten Lessons from Three Generations Shaped Google's TPUv4i},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00010},
doi = {10.1109/ISCA52012.2021.00010},
abstract = {Google deployed several TPU generations since 2015, teaching us lessons that changed our views: semi-conductor technology advances unequally; compiler compatibility trumps binary compatibility, especially for VLIW domain-specific architectures (DSA); target total cost of ownership vs initial cost; support multi-tenancy; deep neural networks (DNN) grow 1.5X annually; DNN advances evolve workloads; some inference tasks require floating point; inference DSAs need air-cooling; apps limit latency, not batch size; and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUv4i, an inference DSA deployed since 2020.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {1–14},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1145/3578360.3580254,
author = {Wang, Sheng-Hong and Coffman, Hunter James and Mayer, Kenneth and Garg, Sakshi and Renau, Jose},
title = {A Multi-Threaded Fast Hardware Compiler for HDLs},
year = {2023},
isbn = {9798400700880},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578360.3580254},
doi = {10.1145/3578360.3580254},
abstract = {A set of new Hardware Description Languages (HDLs) are  
emerging to ease hardware design. HDL compilation time is  
a major bottleneck in the designer’s productivity. Moreover,  
as the HDLs are developed independently, the possibility to  
share innovations in compilation technology is limited.  

We design and implement LiveHD, a new multi-threaded,  
fast, and generic compilation framework across many HDLs  
(FIRRTL, Verilog, and Pyrope). We propose new parallel full  
and bottom-up passes to handle HDLs. The resulting compiler  
can parallelize all the compiler steps.  

LiveHD can achieve 5.5x scalability speedup when elaborating  
a CHISEL RISC-V Manycore. It also gets from 7.7x to 8.4x  
scalability speedup for a benchmark designed in all three HDLs.  
This is achieved with a fast single-threaded LiveHD baseline  
with 6x speedup compared to compilers such as Scala-FIRRTL and  
8.6x against Yosys on Verilog.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
pages = {25–36},
numpages = {12},
keywords = {Compiler Design, Parallel Compilation, HDL},
location = {Montr\'{e}al, QC, Canada},
series = {CC 2023}
}

@inproceedings{10.1145/2181176.2181186,
author = {Olteanu, Alexandra and Pierre, Guillaume},
title = {Towards Robust and Scalable Peer-to-Peer Social Networks},
year = {2012},
isbn = {9781450311649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2181176.2181186},
doi = {10.1145/2181176.2181186},
abstract = {Peer-to-peer Online Social Networks (OSNs) promise to combine the functionalities of centralized OSNs with the good properties of peer-to-peer systems. However, in time, the number of connections between users of OSNs grows super-linearly in the number of users: the average node degree increases with the overall system size. In large-scale settings, mapping each friendship relation into an overlay link will thus overwhelm popular nodes. We propose simple techniques to build and maintain the peer-to-peer OSN overlay while significantly shifting node degrees towards lower ranges. We evaluate our algorithms using real large-scale datasets, and show that they can disseminate information efficiently while controlling node degrees, even in the presence of high churn.},
booktitle = {Proceedings of the Fifth Workshop on Social Network Systems},
articleno = {10},
numpages = {6},
keywords = {accesibility, online social networks, availability, peer-to-peer, scalability},
location = {Bern, Switzerland},
series = {SNS '12}
}

@inproceedings{10.1145/3307681.3326604,
author = {Duan, Jun and Hamlen, Kevin W. and Ferrell, Benjamin},
title = {Better Late Than Never: An n-Variant Framework of Verification for Java Source Code on CPU x GPU Hybrid Platform},
year = {2019},
isbn = {9781450366700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307681.3326604},
doi = {10.1145/3307681.3326604},
abstract = {A method of detecting malicious intrusions and runtime faults in software is proposed, which replicates untrusted computations onto two diverse but often co-located instruction architectures: CPU and GPU. Divergence between the replicated computations signals an intrusion or fault, such as a zero-day exploit. A prototype implementation for Java demonstrates that the approach is realizable in practice, and can successfully detect exploitation of Java VM and runtime system vulnerabilities even when the vulnerabilities are not known in advance to defenders. To achieve acceptable performance, it is shown that GPU parallelism can be leveraged to rapidly validate CPU computations that would otherwise exhibit unacceptable performance if executed on GPU alone. The resulting system detects anomalies in CPU computations on a short delay, during which the GPU replica quickly validates many CPU computation fragments in parallel in order to catch up with the CPU computation. Significant differences between the CPU and GPU computational models lead to high natural diversity between the replicas, affording detection of large exploit classes without laborious manual diversification of the code.},
booktitle = {Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
pages = {207–218},
numpages = {12},
keywords = {intrusion detection, software engineering, software exploit detection, n-variant, java, software reliability},
location = {Phoenix, AZ, USA},
series = {HPDC '19}
}

@article{10.1145/3547639,
author = {Thomson, Patrick and Rix, Rob and Wu, Nicolas and Schrijvers, Tom},
title = {Fusing Industry and Academia at GitHub (Experience Report)},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {ICFP},
url = {https://doi.org/10.1145/3547639},
doi = {10.1145/3547639},
abstract = {GitHub hosts hundreds of millions of code repositories written in hundreds of different programming languages. In addition to its hosting services, GitHub provides data and insights into code, such as vulnerability analysis and code navigation, with which users can improve and understand their software development process. GitHub has built Semantic, a program analysis tool capable of parsing and extracting detailed information from source code. The development of Semantic has relied extensively on the functional programming literature; this paper describes how connections to academic research inspired and informed the development of an industrial-scale program analysis toolkit.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {108},
numpages = {16},
keywords = {industry, Haskell, data types, effects}
}

@techreport{10.1145/3087874,
title = {Report from the NSF Large-Scale Networking Platforms "Communities of Practice" Workshop},
year = {2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The NSF 'Large-scale Networking Platforms "Communities of Practice"' workshop was held on October 24-25, 2016 in Washington DC. The main aim of the workshop was to bring together researchers from both academia and industry involved in (data) networking and community infrastructure projects to distill lessons learned from starting/building/operating/sustaining research-focused networking testbeds. Another aim was to use past experiences and see how they could be applied to the Platforms for Advanced Wireless Research (PAWR) initiative and on the establishment of 'communities of practice' in order to help the PAWR Project Office (PPO) with design, development, deployment and management of a set of advanced wireless research platforms/testbeds.}
}

@article{10.1145/3630614.3630626,
author = {Anderson, Thomas and Belay, Adam and Chowdhury, Mosharaf and Cidon, Asaf and Zhang, Irene},
title = {Treehouse: A Case For Carbon-Aware Datacenter Software},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3630614.3630626},
doi = {10.1145/3630614.3630626},
abstract = {The end of Dennard scaling and the slowing of Moore's Law has put the energy use of datacenters on an unsustainable path. Datacenters are already a significant fraction of worldwide electricity use, with application demand scaling at a rapid rate. We argue that substantial reductions in the carbon intensity of datacenter computing are possible with a software-centric approach: by making energy and carbon visible to application developers on a fine-grained basis, by modifying system APIs to make it possible to make informed trade offs between performance and carbon emissions, and by raising the level of application programming to allow for flexible use of more energy efficient means of compute and storage. We also lay out a research agenda for systems software to reduce the carbon footprint of datacenter computing.},
journal = {SIGENERGY Energy Inform. Rev.},
month = {oct},
pages = {64–70},
numpages = {7},
keywords = {service-level agreements, energy efficiency, carbon, energy provenance, microfunctions}
}

@inproceedings{10.1145/3324989.3325722,
author = {Jackson, Adrian and Turner, Andrew and Weiland, Mich\`{e}le and Johnson, Nick and Perks, Olly and Parsons, Mark},
title = {Evaluating the Arm Ecosystem for High Performance Computing},
year = {2019},
isbn = {9781450367707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324989.3325722},
doi = {10.1145/3324989.3325722},
abstract = {In recent years, Arm-based processors have arrived on the HPC scene, offering an alternative the existing status quo, which was largely dominated by x86 processors. In this paper, we evaluate the Arm ecosystem, both the hardware offering and the software stack that is available to users, by benchmarking a production HPC platform that uses Marvell's ThunderX2 processors. We investigate the performance of complex scientific applications across multiple nodes, and we also assess the maturity of the software stack and the ease of use from a users' perspective. This papers finds that the performance across our benchmarking applications is generally as good as, or better, than that of well-established platforms, and we can conclude from our experience that there are no major hurdles that might hinder wider adoption of this ecosystem within the HPC community.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {12},
numpages = {11},
keywords = {Benchmarking, Distributed Processing, Marvell, Performance, Arm, ThunderX2},
location = {Zurich, Switzerland},
series = {PASC '19}
}

@inproceedings{10.1145/1978942.1979004,
author = {Assogba, Yannick and Ros, Irene and DiMicco, Joan and McKeon, Matt},
title = {Many Bills: Engaging Citizens through Visualizations of Congressional Legislation},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979004},
doi = {10.1145/1978942.1979004},
abstract = {US federal legislation is a common subject of discussion and advocacy on the web, inspired by the open government movement. While the contents of these bills are freely available for download, understanding them is a significant challenge to experts and average citizens alike due to their length, complex language, and obscure topics. To make these important documents more accessible to the general public, we present Many Bills (http://manybills.us): a web-based set of visualization tools that reveals the underlying semantics of a bill. Using machine learning techniques, we classify each bill's sections based on existing document-level categories. We then visualize the resulting topic substructure of these bills. These visualizations provide an overview-and-detail view of bills, enabling users to read individual sections of a bill and compare topic patterns across multiple bills. Through an overview of the site's user activity and interviews with active users, this paper highlights how Many Bills makes the tasks of reading bills, identifying outlier sections in bills, and understanding congressperson's legislative activity more manageable.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {433–442},
numpages = {10},
keywords = {legislation, text classification, information visualization, government transparency, government},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@article{10.1145/3425709,
author = {Shah, Syed Iftikhar Hussain and Peristeras, Vassilios and Magnisalis, Ioannis},
title = {Government Big Data Ecosystem: Definitions, Types of Data, Actors, and Roles and the Impact in Public Administrations},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3425709},
doi = {10.1145/3425709},
abstract = {The public sector, private firms, business community, and civil society are generating data that are high in volume, veracity, and velocity and come from a diversity of sources. This type of data is today known as big data. Public administrations pursue big data as “new oil” and implement data-centric policies to collect, generate, process, share, exploit, and protect data for promoting good governance, transparency, innovative digital services, and citizens’ engagement in public policy. All of the above constitute the Government Big Data Ecosystem (GBDE). Despite the great interest in this ecosystem, there is a lack of clear definitions, the various important types of government data remain vague, the different actors and their roles are not well defined, while the impact in key public administration sectors is not yet deeply understood and assessed. Such research and literature gaps impose a crucial obstacle for a better understanding of the prospects and nascent issues in exploiting GBDE. With this study, we aim to start filling the above-mentioned gaps by organizing our findings from an extended Systematic Literature Review into a framework to organise and address the above-mentioned challenges. Our goal is to contribute in this fast-evolving area by bringing some clarity and establishing common understanding around key elements of the emerging GBDE.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {25},
keywords = {data-driven government, government big data ecosystem, Big data, big data actors and roles, data and information}
}

@inproceedings{10.5555/3433701.3433767,
author = {Brunie, Hugo and Iancu, Costin and Ibrahim, Khaled Z. and Brisk, Philip and Cook, Brandon},
title = {Tuning Floating-Point Precision Using Dynamic Program Information and Temporal Locality},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {We present a methodology for precision tuning of full applications. These techniques must select a search space composed of either variables or instructions and provide a scalable search strategy. In full application settings one cannot assume compiler support for practical reasons. Thus, an additional important challenge is enabling code refactoring. We argue for an instruction-based search space and we show: 1) how to exploit dynamic program information based on call stacks; and 2) how to exploit the iterative nature of scientific codes, combined with temporal locality. We applied the methodology to tune the implementation of scientific codes written in a combination of Python, CUDA, C++ and Fortran, tuning calls to math exp library functions. The iterative search refinement always reduces the search complexity and the number of steps to solution. Dynamic program information increases search efficacy. Using this approach, we obtain application runtime performance improvements up to 27%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {50},
numpages = {14},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/3402942.3403015,
author = {Gustafsson, Viktor and Holme, Benjamin and Mackay, Wendy E.},
title = {Narrative Substrates: Reifying and Managing Emergent Narratives in Persistent Game Worlds},
year = {2020},
isbn = {9781450388078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402942.3403015},
doi = {10.1145/3402942.3403015},
abstract = {Players in modern Massively Multiplayer Online Role-Playing Games progress through ambitiously designed narratives, but have no real influence on the game, since only their characters’ data, not the game environment, persists. Although earlier games supported player influence by persisting changes in the world, they relied on players’ capacity to form their own stories and lacked guidance for character progression. We explore how persistence and narrative emergence let us build upon players’ influence rather than restrict it. We ran four studies and found that players highly value first-time and unique events, and often externalize their experiences to the Web to collaborate and socialize, but unintentionally also disrupt some aspects of in-game play. We introduce Narrative Substrates, a theoretical framework for designing game architectures that represent, manage, and persist traces of player activity as unique, interactive content. To illustrate and test the theory, we developed the game We Ride and deployed it as a two-phase technology probe over one year. We identify key benefits and challenges of our approach, and argue that reification of emergent narratives offers new design opportunities for creating truly interactive games.},
booktitle = {Proceedings of the 15th International Conference on the Foundations of Digital Games},
articleno = {46},
numpages = {12},
keywords = {Emergent narratives, Multiplayer game worlds, Persistence},
location = {Bugibba, Malta},
series = {FDG '20}
}

@inproceedings{10.1145/3385032.3385041,
author = {Clark, Tony and Barn, Balbir and Kulkarni, Vinay and Barat, Souvik},
title = {Language Support for Multi Agent Reinforcement Learning},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385041},
doi = {10.1145/3385032.3385041},
abstract = {Software Engineering must increasingly address the issues of complexity and uncertainty that arise when systems are to be deployed into a dynamic software ecosystem. There is also interest in using digital twins of systems in order to design, adapt and control them when faced with such issues. The use of multi-agent systems in combination with reinforcement learning is an approach that will allow software to intelligently adapt to respond to changes in the environment. This paper proposes a language extension that encapsulates learning-based agents and system building operations and shows how it is implemented in ESL. The paper includes examples the key features and describes the application of agent-based learning implemented in ESL applied to a real-world supply chain.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {7},
numpages = {12},
keywords = {Agents, Reinforcement Learning},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@article{10.1145/3471930,
author = {Rauf, Irum and Petre, Marian and Tun, Thein and Lopez, Tamara and Lunn, Paul and Van Der Linden, Dirk and Towse, John and Sharp, Helen and Levine, Mark and Rashid, Awais and Nuseibeh, Bashar},
title = {The Case for Adaptive Security Interventions},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3471930},
doi = {10.1145/3471930},
abstract = {Despite the availability of various methods and tools to facilitate secure coding, developers continue to write code that contains common vulnerabilities. It is important to understand why technological advances do not sufficiently facilitate developers in writing secure code. To widen our understanding of developers' behaviour, we considered the complexity of the security decision space of developers using theory from cognitive and social psychology. Our interdisciplinary study reported in this article (1) draws on the psychology literature to provide conceptual underpinnings for three categories of impediments to achieving security goals, (2) reports on an in-depth meta-analysis of existing software security literature that identified a catalogue of factors that influence developers' security decisions, and (3) characterises the landscape of existing security interventions that are available to the developer during coding and identifies gaps. Collectively, these show that different forms of impediments to achieving security goals arise from different contributing factors. Interventions will be more effective where they reflect psychological factors more sensitively and marry technical sophistication, psychological frameworks, and usability. Our analysis suggests “adaptive security interventions” as a solution that responds to the changing security needs of individual developers and a present a proof-of-concept tool to substantiate our suggestion.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {9},
numpages = {52},
keywords = {cognitive psychology, developers, Security decisions, security interventions, security goals, adaptive software engineering, social psychology}
}

@inproceedings{10.1145/3528535.3565255,
author = {Messaoud, Aghiles Ait and Mokhtar, Sonia Ben and Nitu, Vlad and Schiavoni, Valerio},
title = {Shielding Federated Learning Systems against Inference Attacks with ARM TrustZone},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528535.3565255},
doi = {10.1145/3528535.3565255},
abstract = {Federated Learning (FL) opens new perspectives for training machine learning models while keeping personal data on the users premises. Specifically, in FL, models are trained on the users' devices and only model updates (i.e., gradients) are sent to a central server for aggregation purposes. However, the long list of inference attacks that leak private data from gradients, published in the recent years, have emphasized the need of devising effective protection mechanisms to incentivize the adoption of FL at scale. While there exist solutions to mitigate these attacks on the server side, little has been done to protect users from attacks performed on the client side. In this context, the use of Trusted Execution Environments (TEEs) on the client side are among the most proposing solutions. However, existing frameworks (e.g., DarkneTZ) require statically putting a large portion of the machine learning model into the TEE to effectively protect against complex attacks or a combination of attacks. We present GradSec, a solution that allows protecting in a TEE only sensitive layers of a machine learning model, either statically or dynamically, hence reducing both the Trusted Computing Base (TCB) size and the overall training time by up to 30% and 56%, respectively compared to state-of-the-art competitors.},
booktitle = {Proceedings of the 23rd ACM/IFIP International Middleware Conference},
pages = {335–348},
numpages = {14},
keywords = {TrustZone, privacy, federated learning, trusted execution environment},
location = {Quebec, QC, Canada},
series = {Middleware '22}
}

@inproceedings{10.1145/2837185.2837195,
author = {Trunde, Hannes and Weippl, Edgar},
title = {WordPress Security: An Analysis Based on Publicly Available Exploits},
year = {2015},
isbn = {9781450334914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837185.2837195},
doi = {10.1145/2837185.2837195},
abstract = {The danger of SQL injections has been known for more than a decade but injection attacks have led the OWASP top 10 for years and still are one of the major reasons for devastating attacks on web sites. As about 24% percent of the top 10 million web sites are built upon the content management system WordPress, it's no surprise that content management systems in general and WordPress in particular are frequently targeted. To understand how the underlying security bugs can be discovered and exploited by attackers, 199 publicly disclosed SQL injection exploits for WordPress and its plugins have been analyzed. The steps an attacker would take to uncover and utilize these bugs are followed in order to gain access to the underlying database through automated, dynamic vulnerability scanning with well-known, freely available tools. Previous studies have shown that the majority of the security bugs are caused by the same programming errors as 10 years ago and state that the complexity of finding and exploiting them has not increased significantly. Furthermore, they claim that although the complexity has not increased, automated tools still do not detect the majority of bugs. The results of this paper show that tools for automated, dynamic vulnerability scanning only play a subordinate role for developing exploits. The reason for this is that only a small percentage of attack vectors can be found during the detection phase. So even if the complexity of exploiting an attack vector has not increased, this attack vector has to be found in the first place, which is the major challenge for this kind of tools. Therefore, from today's perspective, a combination with manual and/or static analysis is essential when testing for security vulnerabilities.},
booktitle = {Proceedings of the 17th International Conference on Information Integration and Web-Based Applications &amp; Services},
articleno = {81},
numpages = {7},
keywords = {black-box testing, SQL injection, web application, vulnerability scanning, exploit},
location = {Brussels, Belgium},
series = {iiWAS '15}
}

@inproceedings{10.1145/3609021.3609306,
author = {Dejaeghere, Jules and Gbadamosi, Bolaji and Pulls, Tobias and Rochet, Florentin},
title = {Comparing Security in EBPF and WebAssembly},
year = {2023},
isbn = {9798400702938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609021.3609306},
doi = {10.1145/3609021.3609306},
abstract = {This paper examines the security of eBPF and WebAssembly (Wasm), two technologies that have gained widespread adoption in recent years, despite being designed for very different use cases and environments. While eBPF is a technology primarily used within operating system kernels such as Linux, Wasm is a binary instruction format designed for a stack-based virtual machine with use cases extending beyond the web. Recognizing the growth and expanding ambitions of eBPF, Wasm may provide instructive insights, given its design around securely executing arbitrary untrusted programs in complex and hostile environments such as web browsers and clouds. We analyze the security goals, community evolution, memory models, and execution models of both technologies, and conduct a comparative security assessment, exploring memory safety, control flow integrity, API access, and side-channels. Our results show that eBPF has a history of focusing on performance first and security second, while Wasm puts more emphasis on security at the cost of some runtime overheads. Considering language-based restrictions for eBPF and a security model for API access are fruitful directions for future work.},
booktitle = {Proceedings of the 1st Workshop on EBPF and Kernel Extensions},
pages = {35–41},
numpages = {7},
keywords = {eBPF, webassembly, threat model, API access, memory safety, security comparison, side-channels, control flow integrity},
location = {New York, NY, USA},
series = {eBPF '23}
}

@inproceedings{10.1145/1453101.1453114,
author = {Lin, Zhiqiang and Zhang, Xiangyu},
title = {Deriving Input Syntactic Structure from Execution},
year = {2008},
isbn = {9781595939951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1453101.1453114},
doi = {10.1145/1453101.1453114},
abstract = {Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution.},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {83–93},
numpages = {11},
keywords = {control dependence, input lineage, syntax tree, top-down grammar, reverse engineering, bottom-up grammar},
location = {Atlanta, Georgia},
series = {SIGSOFT '08/FSE-16}
}

@inproceedings{10.1145/354908.354923,
author = {Davis, Mike and O'Donovan, Will and Fritz, John and Childress, Carlisle},
title = {Linux and Open Source in the Academic Enterprise},
year = {2000},
isbn = {1581132298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/354908.354923},
doi = {10.1145/354908.354923},
booktitle = {Proceedings of the 28th Annual ACM SIGUCCS Conference on User Services: Building the Future},
pages = {65–69},
numpages = {5},
keywords = {open source, commodity hardware, Linux, Beowulf},
location = {Richmond, Virginia, USA},
series = {SIGUCCS '00}
}

@inproceedings{10.1145/355274.355289,
author = {Sorokine, Alexandre and Ackermann, Kurt},
title = {Scripting in GIS Applications: Experimental Standards-Based Framework for Perl},
year = {2000},
isbn = {1581133197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/355274.355289},
doi = {10.1145/355274.355289},
abstract = {Scripting languages have long been utilized by GIS application developers to achieve higher levels of programming and shorter development times. Modern general-purpose scripting languages — like Tcl/Tk, Perl or Python — allow the smooth integration of various software components, while at the same time providing rich programming capabilities. Increases in processor speed and the development of industry-wide standards are removing obstacles to the proliferation of universal scripting languages in GIS applications.This paper examines the feasibility, architecture and early results of the building of a framework of object-oriented modules for Perl that is designed to provide uniform access to various sources of spatial data from Perl scripts. The framework is independent of any particular GIS application and thus based on the object model detailed in the Abstract Specification by The OpenGIS Consortium. The framework is intended to be used for rapid application development, gluing various software components together and prototyping.},
booktitle = {Proceedings of the 8th ACM International Symposium on Advances in Geographic Information Systems},
pages = {102–107},
numpages = {6},
keywords = {perl, geographic information systems, OpenGIS, spatial data access},
location = {Washington, D.C., USA},
series = {GIS '00}
}

@inproceedings{10.1145/3510003.3510118,
author = {Li, Zhenming and Wang, Ying and Lin, Zeqi and Cheung, Shing-Chi and Lou, Jian-Guang},
title = {Nufix: Escape from NuGet Dependency Maze},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510118},
doi = {10.1145/3510003.3510118},
abstract = {Developers usually suffer from &lt;u&gt;d&lt;/u&gt;ependency &lt;u&gt;m&lt;/u&gt;aze (DM) issues, i.e., package dependency constraints are violated when a project's platform or dependencies are changed. This problem is especially serious in .NET ecosystem due to its fragmented platforms (e.g., .NET Framework, .NET Core, and .NET Standard). Fixing DM issues is challenging due to the complexity of dependency constraints: multiple DM issues often occur in one project; solving one DM issue usually causes another DM issue cropping up; the exponential search space of possible dependency combinations is also a barrier.In this paper, we aim to help .NET developers tackle the DM issues. First, we empirically studied a set of real DM issues, learning their common fixing strategies and developers' preferences in adopting these strategies. Based on these findings, we propose NuFix, an automated technique to repair DM issues. NuFix formulates the repair task as a binary integer linear optimization problem to effectively derive an optimal fix in line with the learnt developers' preferences. The experiment results and expert validation show that NuFix can generate high-quality fixes for all the DM issues with 262 popular .NET projects. Encouragingly, 20 projects (including affected projects such as Dropbox) have approved and merged our generated fixes, and shown great interests in our technique.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1545–1557},
numpages = {13},
keywords = {.NET, empirical study, NuGet, dependencies},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/1411273.1411283,
author = {Li, Huiqing and Thompson, Simon and Orosz, Gy\"{o}rgy and T\'{o}th, Melinda},
title = {Refactoring with Wrangler, Updated: Data and Process Refactorings, and Integration with Eclipse},
year = {2008},
isbn = {9781605580654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411273.1411283},
doi = {10.1145/1411273.1411283},
abstract = {Wrangler is a refactoring tool for Erlang, implemented in Erlang. This paper reports the latest developments in Wrangler, which include improved user experience, the introduction of a number of data- and process-related refactorings, and also the implementation of an Eclipse plug-in which, together with Erlide, provides refactoring support for Erlang in Eclipse.},
booktitle = {Proceedings of the 7th ACM SIGPLAN Workshop on ERLANG},
pages = {61–72},
numpages = {12},
keywords = {wrangler, tuple, slicing, erlide, refactoring, erlang, eclipse, record, process},
location = {Victoria, BC, Canada},
series = {ERLANG '08}
}

@inproceedings{10.1145/3457913.3457927,
author = {Zhang, Yinyuan and Zhang, Yang and Wu, Yiwen and Lu, Yao and Wang, Tao and Mao, Xinjun},
title = {Exploring the Dependency Network of Docker Containers: Structure, Diversity, and Relationship},
year = {2021},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457927},
doi = {10.1145/3457913.3457927},
abstract = {Container technologies are being widely used in large scale production cloud environments, of which Docker has become the de-facto industry standard. As a key step, containers need to define their dependent base image, which makes complex dependencies exist in a large number of containers. Prior studies have shown that references between software packages could form technical dependencies, thus forming a dependency network. However, little is known about the details of docker container dependency networks. In this paper, we perform an empirical study on the dependency network of docker containers from more than 120,000 dockerfiles. We construct the container dependency network and analyze its network structure. Further, we focus on the Top-100 dominant containers and investigate their subnetworks, including diversity and relationships. Our findings help to characterize and understand the container dependencies in the docker community and motivate the need for developing container dependency management tools.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {199–208},
numpages = {10},
keywords = {Network analysis, Docker container, Dependency network},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@article{10.1145/966789.966801,
author = {Turnlund, Michael},
title = {Distributed Development: Lessons Learned: Why Repeat the Mistakes of the Past If You Don’t Have to?},
year = {2003},
issue_date = {December/January 2003-2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {9},
issn = {1542-7730},
url = {https://doi.org/10.1145/966789.966801},
doi = {10.1145/966789.966801},
abstract = {Delivery of a technology-based project is challenging, even under well-contained, familiar circumstances. And a tight-knit team can be a major factor in success. It is no mystery, therefore, why most small, new technology teams opt to work in a garage (at times literally). Keeping the focus of everyone’s energy on the development task at hand means a minimum of non-engineering overhead.},
journal = {Queue},
month = {dec},
pages = {26–31},
numpages = {6}
}

@inproceedings{10.1145/1278940.1278943,
author = {Locasto, Michael E. and Stavrou, Angelos and Keromytis, Angelos D.},
title = {Dark Application Communities},
year = {2006},
isbn = {9781595939234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1278940.1278943},
doi = {10.1145/1278940.1278943},
abstract = {In considering new security paradigms, it is often worthwhile to anticipate the direction and nature of future attack paradigms. We identify a class of attacks based on the idea of a "Dark" Application Community (DAC) - a collection of bots and zombie machines that actively performs binary-level supervision of applications to help an attacker automate the process of finding vulnerabilities. A collection of such hosts can observe and attempt to influence the behavior of automatic defense systems. An attacker can use the DAC as both a test platform for subverting security applications and as a reconnaissance network for exploiting commonly deployed automatic update and early warning systems.An instance of this type of Application Community can host what we call an automorphic worm. An automorphic worm is application-agnostic and vulnerability-generic. Such a worm attempts to remain stealthy by cycling through the portfolio of vulnerabilities that the DAC has identified. We examine the underlying principles of a DAC, which are based on the existing paradigm of using security tools to help violate security.},
booktitle = {Proceedings of the 2006 Workshop on New Security Paradigms},
pages = {11–18},
numpages = {8},
keywords = {automorphic, dark application communities, network security, application communities},
location = {Germany},
series = {NSPW '06}
}

@inproceedings{10.1145/2660517.2660539,
author = {Lukovnikov, Denis and Stadler, Claus and Lehmann, Jens},
title = {LD Viewer - Linked Data Presentation Framework},
year = {2014},
isbn = {9781450329279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660517.2660539},
doi = {10.1145/2660517.2660539},
abstract = {With the growing interest in publishing data according to the Linked Data principles, it becomes more important to provide intuitive tools for users to view and interact with those resources. The characteristics of Linked Data pose several challenges for user-friendly presentation of information. In this work, we present the LD Viewer as a customizable framework that can easily be fitted for different datasets while addressing Linked Data presentation challenges. With this framework, we aim to provide dataset maintainers with easy means to expose their RDF resources. Moreover, we aim to make the interface intuitive and engaging for both expert users and lay users.},
booktitle = {Proceedings of the 10th International Conference on Semantic Systems},
pages = {124–131},
numpages = {8},
location = {Leipzig, Germany},
series = {SEM '14}
}

@inproceedings{10.5555/2664446.2664451,
author = {Keivanloo, Iman and Forbes, Christopher and Hmood, Aseel and Erfani, Mostafa and Neal, Christopher and Peristerakis, George and Rilling, Juergen},
title = {A Linked Data Platform for Mining Software Repositories},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {32–35},
numpages = {4},
keywords = {software mining, linked data, fact sharing},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@proceedings{10.1145/3609437,
title = {Internetware '23: Proceedings of the 14th Asia-Pacific Symposium on Internetware},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@inproceedings{10.1145/1167473.1167481,
author = {Cohen, Tal and Gil, Joseph (Yossi) and Maman, Itay},
title = {JTL: The Java Tools Language},
year = {2006},
isbn = {1595933484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1167473.1167481},
doi = {10.1145/1167473.1167481},
abstract = {We present an overview of JTL (the Java Tools Language, pronounced "Gee-tel"), a novel language for querying JAVA [8] programs. JTL was designed to serve the development of source code software tools for JAVA, and as a small language which to aid programming language extensions to JAVA. Applications include definition of pointcuts for aspect-oriented programming, fixing type constraints for generic programming, specification of encapsulation policies, definition of micro-patterns, etc. We argue that the JTL expression of each of these is systematic, concise, intuitive and general.JTL relies on a simply-typed relational database for program representation, rather than an abstract syntax tree. The underlying semantics of the language is restricted to queries formulated in First Order Predicate Logic augmented with transitive closure (FOPL).Special effort was taken to ensure terse, yet readable expression of logical conditions. The JTL pattern &lt;B&gt;public abstract class&lt;/B&gt;, for example, matches all abstract classes which are publicly accessible, while &lt;B&gt;class&lt;/B&gt; (&lt;B&gt;public&lt;/B&gt; clone();) matches all classes in which method clone is public. To this end, JTL relies on a DATALOG-like syntax and semantics, enriched with quantifiers and pattern matching which all but entirely eliminate the need for recursive calls.JTL's query analyzer gives special attention to the fragility of the "closed world assumption" in examining JAVA software, and determines whether a query relies on such an assumption.The performance of the JTL interpreter is comparable to that of JQuery after it generated its database cache, and at least an order of magnitude faster when the cache has to be rebuilt.},
booktitle = {Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications},
pages = {89–108},
numpages = {20},
keywords = {reverse engineering, declarative programming},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@article{10.14778/1687553.1687568,
author = {Gates, Alan F. and Natkovich, Olga and Chopra, Shubham and Kamath, Pradeep and Narayanamurthy, Shravan M. and Olston, Christopher and Reed, Benjamin and Srinivasan, Santhosh and Srivastava, Utkarsh},
title = {Building a High-Level Dataflow System on Top of Map-Reduce: The Pig Experience},
year = {2009},
issue_date = {August 2009},
publisher = {VLDB Endowment},
volume = {2},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1687553.1687568},
doi = {10.14778/1687553.1687568},
abstract = {Increasingly, organizations capture, transform and analyze enormous data sets. Prominent examples include internet companies and e-science. The Map-Reduce scalable dataflow paradigm has become popular for these applications. Its simple, explicit dataflow programming model is favored by some over the traditional high-level declarative approach: SQL. On the other hand, the extreme simplicity of Map-Reduce leads to much low-level hacking to deal with the many-step, branching dataflows that arise in practice. Moreover, users must repeatedly code standard operations such as join by hand. These practices waste time, introduce bugs, harm readability, and impede optimizations.Pig is a high-level dataflow system that aims at a sweet spot between SQL and Map-Reduce. Pig offers SQL-style high-level data manipulation constructs, which can be assembled in an explicit dataflow and interleaved with custom Map- and Reduce-style functions or executables. Pig programs are compiled into sequences of Map-Reduce jobs, and executed in the Hadoop Map-Reduce environment. Both Pig and Hadoop are open-source projects administered by the Apache Software Foundation.This paper describes the challenges we faced in developing Pig, and reports performance comparisons between Pig execution and raw Map-Reduce execution.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1414–1425},
numpages = {12}
}

@inproceedings{10.1145/1490283.1490296,
author = {Johansen, H\r{a}vard and Johansen, Dag},
title = {Resilient Software Mirroring with Untrusted Third Parties},
year = {2008},
isbn = {9781605583044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1490283.1490296},
doi = {10.1145/1490283.1490296},
abstract = {Open-source communities depend on donated third-party servers, known as mirrors, to distribute their software to millions of end-users. However, existing mirror infrastructures lack the mechanisms to deal with the wide-range of faults that can occur. In this paper we describe our ongoing work to construct a mirror infrastructure that is highly resilient to failures. In particular, our infrastructure is constructed to tolerate Byzantine failures so that a potential attacker cannot deny user service even if he is in control of one or more mirrors.},
booktitle = {Proceedings of the 1st International Workshop on Hot Topics in Software Upgrades},
articleno = {10},
numpages = {5},
keywords = {denial-of-service attack, intrusion-tolerance, overlay network, software distribution},
location = {Nashville, Tennessee},
series = {HotSWUp '08}
}

@inproceedings{10.1145/2633301.2633306,
author = {Arrighi, Pablo and Girard, Johan and Lezama, Miguel and Mazet, K\'{e}vin},
title = {The GOOL System: A Lightweight Object-Oriented Programming Language Translator},
year = {2014},
isbn = {9781450329149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2633301.2633306},
doi = {10.1145/2633301.2633306},
abstract = {The GOOL system is a lightweight translator between OOP languages (Java, C++, C#, Objective C, ...). It relies upon a minimal, abstract OOP language called GOOL (General Object-Oriented Language) in order to represent the common features between these languages.},
booktitle = {Proceedings of the 9th International Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems PLE},
articleno = {5},
numpages = {7},
keywords = {porting, code migration},
location = {Uppsala, Sweden},
series = {ICOOOLPS '14}
}

@inproceedings{10.1145/2494188.2494211,
author = {B\"{u}low, Christina and Metscher, Johannes and Schnurr, Jan-Mathis and Koch, Michael},
title = {Shaping a Social Software for a Distributed Military Organisation},
year = {2013},
isbn = {9781450323000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494188.2494211},
doi = {10.1145/2494188.2494211},
abstract = {While the deployment of social software has become widespread in private enterprises in the past years, the usage in military institutions is not common practice yet. This paper documents the implementation and usage of a social networking service in the medical service of the German armed forces -- the San-Netz. The case study documents the whole process from a user needs analysis, which took place in two waves of interviews in 2011 and 2012, to conceptualization, development and rollout of the social networking service. Analysis shows several challenges for platform adoption, which stem from the socio-technical context. The paper details possible solutions for these challenges. One proposed solution that is discussed in details is the employment of use-case-centred documentation.},
booktitle = {Proceedings of the 13th International Conference on Knowledge Management and Knowledge Technologies},
articleno = {27},
numpages = {8},
keywords = {Military, Training, Social Software, Enterprise 2.0, Social Networking Service, Case Study},
location = {Graz, Austria},
series = {i-Know '13}
}

@article{10.5555/3455716.3455846,
author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilovi\'{c}, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John T. and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
title = {AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {As artificial intelligence algorithms make further inroads in high-stakes societal applications, there are increasing calls from multiple stakeholders for these algorithms to explain their outputs. To make matters more challenging, different personas of consumers of explanations have different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360, an open-source Python toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics (http://aix360.mybluemix.net). Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of interpretation and explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. The toolkit is not only the software, but also guidance material, tutorials, and an interactive web demo to introduce AI explainability to different audiences. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {130},
numpages = {6},
keywords = {interpretability, transparency, explainability, taxonomy, open source}
}

@inproceedings{10.1145/2393596.2393619,
author = {Caglayan, Bora and Misirli, Ayse Tosun and Calikli, Gul and Bener, Ayse and Aytac, Turgay and Turhan, Burak},
title = {Dione: An Integrated Measurement and Defect Prediction Solution},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393619},
doi = {10.1145/2393596.2393619},
abstract = {We present an integrated measurement and defect prediction tool: Dione. Our tool enables organizations to measure, monitor, and control product quality through learning based defect prediction. Similar existing tools either provide data collection and analytics, or work just as a prediction engine. Therefore, companies need to deal with multiple tools with incompatible interfaces in order to deploy a complete measurement and prediction solution. Dione provides a fully integrated solution where data extraction, defect prediction and reporting steps fit seamlessly. In this paper, we present the major functionality and architectural elements of Dione followed by an overview of our demonstration.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {20},
numpages = {2},
keywords = {software tool, software defect prediction, measurement},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1145/2766330.2766338,
author = {Gross-Brown, Rafael and Ficek, Michal and Agundez, Jose Luis and Dressler, Patrick and Laoutaris, Nikolaos},
title = {Data Transparency Lab Kick Off Workshop (DTL 2014) Report},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2766330.2766338},
doi = {10.1145/2766330.2766338},
abstract = {On November 20 and 21 2014, Telefonica I+D hosted the Data Transparency Lab ("DTL") Kickoff Workshop on Personal Data Transparency and Online Privacy at its headquarters in Barcelona, Spain. This workshop provided a forum for technologists, researchers, policymakers and industry representatives to share and discuss current and emerging issues around privacy and transparency on the Internet. The objective of this workshop was to kick-start the creation of a community of research, industry, and public interest parties that will work together towards the following objectives: -The development of methodologies and user-friendly tools to promote transparency and empower users to understand online privacy issues and consequences; -The sharing of datasets and research results, and; -The support of research through grants and the provision of infrastructure to deploy tools. With the above activities, the DTL community aims to improve our understanding of technical, ethical, economic and regulatory issues related to the use of personal data by online services. It is hoped that successful execution of such activities will help sustain a fair and transparent exchange of personal data online. This report summarizes the presentations, discussions and questions that resulted from the workshop.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {44–48},
numpages = {5},
keywords = {privacy, web tracking, economics of the web, internet advertising, measurement tools}
}

@inproceedings{10.1145/1899475.1899476,
author = {Laine, Teemu H. and Sedano, Carolina Islas and Sutinen, Erkki and Joy, Mike},
title = {Viable and Portable Architecture for Pervasive Learning Spaces},
year = {2010},
isbn = {9781450304245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1899475.1899476},
doi = {10.1145/1899475.1899476},
abstract = {A Pervasive Learning Space (PLS) uses context-awareness to link a virtual world with real world objects. We define viability as the extent to which a given PLS can be adapted to different purposes, and portability to be the extent to which a given PLS can be transferred to a new physical context. Heroes of Koskenniska is a game-based PLS combining mobile technology and a wireless sensor network in a forest context to raise the environmental awareness in a Biosphere Reserve in Finland. The game was built upon a screen-based architecture, and our analysis shows that it has higher portability and viability than a selection of related PLSs. The screen-based architecture is highly viable and portable because it is based on the model-view-controller division. Our preliminary observations indicate that the game helps to increase visitor count of the area and to promote interaction between visitors and nature.},
booktitle = {Proceedings of the 9th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {1},
numpages = {10},
keywords = {pervasive learning, biosphere reserve, wireless sensor network, model-view-controller, viability, portability, environmental education},
location = {Limassol, Cyprus},
series = {MUM '10}
}

@inproceedings{10.1145/2940299.2940300,
author = {Maceli, Monica},
title = {Co-Design in the Wild: A Case Study on Meme Creation Tools},
year = {2016},
isbn = {9781450340465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2940299.2940300},
doi = {10.1145/2940299.2940300},
abstract = {The internet meme has become a vital form of self-expression in social communities throughout the Internet. The tools facilitating meme-creation, specifically image macros, have been little-studied but endow non-technical users with the ability to create the multi-layered graphics typical to such memes. The use of these creativity tools provides a unique setting in which to explore the concept of co-design, wherein tools are shaped in response to emergent user needs. Users and designers of meme-creation tools have evolved ways of collaborating and communicating over time, in a fully naturalistic setting. This study explores these processes through survey and interviews of tool designer/developers and an analysis of users' design ideas generated over time. The study finds that, while co-design may be commonly practiced today, these activities raise numerous challenges to participatory design, including: creating trust between designers and users, managing unwieldy system growth, and supporting features specific to aspiring end-user crafters.},
booktitle = {Proceedings of the 14th Participatory Design Conference: Full Papers - Volume 1},
pages = {161–170},
numpages = {10},
keywords = {participatory design, internet memes, co-design, social technologies, image macros},
location = {Aarhus, Denmark},
series = {PDC '16}
}

@inproceedings{10.1145/1065579.1065761,
author = {Chen, Guangyu and Kandemir, Mahmut},
title = {Improving Java Virtual Machine Reliability for Memory-Constrained Embedded Systems},
year = {2005},
isbn = {1595930582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065579.1065761},
doi = {10.1145/1065579.1065761},
abstract = {Dual-execution/checkpointing based transient error tolerance techniques have been widely used in the high-end mission critical systems. These techniques, however, are not very attractive for cost-sensitive embedded systems because they require extra resources (e.g., large memory, special hardware, etc), and thus increase overall cost of the system. In this paper, we propose a transient error tolerant Java Virtual Machine (JVM) implementation for embedded systems. Our JVM uses dual-execution and checkpointing to detect and recover from transient errors. However, our technique does not require any special hardware support (except for the memory page protection mechanism, which is commonly available in modern embedded processors), and the memory space overhead it incurs is not excessive. Therefore, it is suitable for memory-constrained embedded systems. We implemented our approach and performed experiments with seven embedded Java applications.},
booktitle = {Proceedings of the 42nd Annual Design Automation Conference},
pages = {690–695},
numpages = {6},
keywords = {java virtual machine, transient error, dual execution},
location = {Anaheim, California, USA},
series = {DAC '05}
}

@inproceedings{10.1145/2723372.2742785,
author = {Huang, Yanxiang and Cui, Bin and Zhang, Wenyu and Jiang, Jie and Xu, Ying},
title = {TencentRec: Real-Time Stream Recommendation in Practice},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742785},
doi = {10.1145/2723372.2742785},
abstract = {With the arrival of the big data era, opportunities as well as challenges arise in both industry and academia. As an important service in most web applications, accurate real-time recommendation in the context of big data is of high demand. Traditional recommender systems that analyze data and update models at regular time intervals cannot satisfy the requirements of modern web applications, calling for real-time recommender systems.In this paper, we tackle the ``big", ``real-time" and ``accurate" challenges in real-time recommendation, and propose a general real-time stream recommender system built on Storm named TencentRec from three aspects, i.e., ``system", ``algorithm", and ``data". We analyze the large amount of data streams from a wide range of applications leveraging the considerable computation ability of Storm, together with a data access component and a data storage component developed by us. To deal with various application specific demands, we have implemented several classic practical recommendation algorithms in TencentRec, including the item-based collaborative filtering, the content based, and the demographic based algorithms. Specially, we present a practical scalable item-based CF algorithm in detail, with the super characteristics such as robust to the implicit feedback problem, incremental update and real-time pruning. With the enhancement of real-time data collection and processing, we can capture the recommendation changes in real-time. We deploy the TencentRec in a series of production applications, and observe the superiority of TencentRec in providing accurate real-time recommendations for 10 billion user requests everyday.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {227–238},
numpages = {12},
keywords = {real-time recommendation, scalability, application, big data, practice},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3493425.3502762,
author = {Stoyanov, Radostin and Wolnikowski, Adam and Soul\'{e}, Robert and Laki, S\'{a}ndor and Zilberman, Noa},
title = {Building an Internet Router with P4Pi},
year = {2022},
isbn = {9781450391689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493425.3502762},
doi = {10.1145/3493425.3502762},
abstract = {Building an Internet Router is a popular, hands-on project used to teach computer networks. However, there is currently no hardware target that allows students to develop the project in P4 without incurring significant cost or encountering FPGA knowledge barriers. This paper presents P4Pi as a target for the Building an Internet Router project. P4Pi is a platform for developing, testing, and evaluating P4 programs on a Raspberry Pi device. We describe the architecture of the router project on P4Pi, and discuss the practical aspects of running it as a class project. The P4Pi-based router project is low-cost and easy to adopt, enabling students to focus on their P4 programming skills and to evaluate their designs on a physical target through interoperability tests with their colleagues.},
booktitle = {Proceedings of the Symposium on Architectures for Networking and Communications Systems},
pages = {151–156},
numpages = {6},
keywords = {Programmable Switches, P4, Network Education},
location = {Layfette, IN, USA},
series = {ANCS '21}
}

@article{10.1145/3623392,
author = {Kaplan, David},
title = {Hardware VM Isolation in the Cloud: Enabling Confidential Computing with AMD SEV-SNP Technology},
year = {2023},
issue_date = {July/August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1542-7730},
url = {https://doi.org/10.1145/3623392},
doi = {10.1145/3623392},
abstract = {Confidential computing is a security model that fits well with the public cloud. It enables customers to rent VMs while enjoying hardware-based isolation that ensures that a cloud provider cannot purposefully or accidentally see or corrupt their data. SEV-SNP was the first commercially available x86 technology to offer VM isolation for the cloud and is deployed in Microsoft Azure, AWS, and Google Cloud. As confidential computing technologies such as SEV-SNP develop, confidential computing is likely to simply become the default trust model for the cloud.},
journal = {Queue},
month = {sep},
pages = {49–67},
numpages = {19}
}

@inproceedings{10.1145/2901739.2901774,
author = {Ahmed, Tarek M. and Bezemer, Cor-Paul and Chen, Tse-Hsun and Hassan, Ahmed E. and Shang, Weiyi},
title = {Studying the Effectiveness of Application Performance Management (APM) Tools for Detecting Performance Regressions for Web Applications: An Experience Report},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901774},
doi = {10.1145/2901739.2901774},
abstract = {Performance regressions, such as a higher CPU utilization than in the previous version of an application, are caused by software application updates that negatively affect the performance of an application. Although a plethora of mining software repository research has been done to detect such regressions, research tools are generally not readily available to practitioners. Application Performance Management (APM) tools are commonly used in practice for detecting performance issues in the field by mining operational data.In contrast to performance regression detection tools that assume a changing code base and a stable workload, APM tools mine operational data to detect performance anomalies caused by a changing workload in an otherwise stable code base. Although APM tools are widely used in practice, no research has been done to understand 1) whether APM tools can identify performance regressions caused by code changes and 2) how well these APM tools support diagnosing the root-cause of these regressions.In this paper, we explore if the readily accessible APM tools can help practitioners detect performance regressions. We perform a case study using three commercial (AppDynamics, New Relic and Dynatrace) and one open source (Pinpoint) APM tools. In particular, we examine the effectiveness of leveraging these APM tools in detecting and diagnosing injected performance regressions (excessive memory usage, high CPU utilization and inefficient database queries) in three open source applications. We find that APM tools can detect most of the injected performance regressions, making them good candidates to detect performance regressions in practice. However, there is a gap between mining approaches that are proposed in state-of-the-art performance regression detection research and the ones used by APM tools. In addition, APM tools lack the ability to be extended, which makes it hard to enhance them when exploring novel mining approaches for detecting performance regressions.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {1–12},
numpages = {12},
location = {Austin, Texas},
series = {MSR '16}
}

@article{10.1145/3485516,
author = {Bao, Yuyan and Wei, Guannan and Bra\v{c}evac, Oliver and Jiang, Yuxuan and He, Qiyang and Rompf, Tiark},
title = {Reachability Types: Tracking Aliasing and Separation in Higher-Order Functional Programs},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485516},
doi = {10.1145/3485516},
abstract = {Ownership type systems, based on the idea of enforcing unique access paths, have been primarily focused on objects and top-level classes. However, existing models do not as readily reflect the finer aspects of nested lexical scopes, capturing, or escaping closures in higher-order functional programming patterns, which are increasingly adopted even in mainstream object-oriented languages. We present a new type system, λ* , which enables expressive ownership-style reasoning across higher-order functions. It tracks sharing and separation through reachability sets, and layers additional mechanisms for selectively enforcing uniqueness on top of it. Based on reachability sets, we extend the type system with an expressive flow-sensitive effect system, which enables flavors of move semantics and ownership transfer. In addition, we present several case studies and extensions, including applications to capabilities for algebraic effects, one-shot continuations, and safe parallelization.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {139},
numpages = {32},
keywords = {reachability types, effect systems, aliasing, type systems, ownership types}
}

@inproceedings{10.1145/3290420.3290466,
author = {Jia, Shuli and Ma, Liyong and Zhang, Shuting},
title = {Big Data Prototype Practice for Unmanned Surface Vehicle},
year = {2018},
isbn = {9781450365345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290420.3290466},
doi = {10.1145/3290420.3290466},
abstract = {In recent years, unmanned surface vehicle (USV) has made great progress with the development of communication and electronic technologies. In the future, USVs will be widely used in marine tasks. USV has begun to offer a wide variety of large heterogeneous data sources. The architecture of USV big data is discussed. A prototype system of USV big data is established, and a preliminary anomaly detection application based on isolation forest method is developed. The effective anomaly detection results show that the USV big data prototype system and the anomaly detection application is feasible. The establishment of USV big data system is meaningful and the system can provide effective application services.},
booktitle = {Proceedings of the 4th International Conference on Communication and Information Processing},
pages = {43–47},
numpages = {5},
keywords = {big data, unmanned surface vehicle (USV), isolation forest},
location = {Qingdao, China},
series = {ICCIP '18}
}

@inproceedings{10.1145/3397537.3397549,
author = {Jakubovic, Joel},
title = {What It Takes to Create with Domain-Appropriate Tools: Reflections on Implementing the “Id” System},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397549},
doi = {10.1145/3397537.3397549},
abstract = {There is a One-Size-Fits-All quality to languages, APIs and even programming itself. Whether you're making a mobile game or a scientific simulation, you will be using a text-based language with similar devices for structuring your code. This is a source of artificial difficulty in creating, understanding, and modifying software systems. No matter the domain, the author's design needs encoding into a form that does not resemble it.  This paper describes a vision where software can be built in a programming environment that is closer to the domain of the software itself. By doing so, users of the system can use familiar abstractions and tools for adapting it. A step towards this vision is presented: a Web version of a minimal OOP system, developed as an executable version of the diagrams of its design, in a substrate meant to facilitate this. The experience of creating such a substrate is analysed, and I suggest deficiencies in programming environments that stand in the way of making this practice commonplace, as well as ways to fill in these gaps.},
booktitle = {Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {197–207},
numpages = {11},
keywords = {visual programming, meta-circular, context-specific, object-oriented, adaptation},
location = {Porto, Portugal},
series = {Programming '20}
}

@inproceedings{10.1145/3510547.3517921,
author = {Shakarami, Mehrnoosh and Benson, James and Sandhu, Ravi},
title = {Blockchain-Based Administration of Access in Smart Home IoT},
year = {2022},
isbn = {9781450392297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510547.3517921},
doi = {10.1145/3510547.3517921},
abstract = {There is a rising concern about authorization in IoT environments to be appropriately designed and applied, due to smart things surge to be part of people's daily lives on one hand, and the amount of personal/private information they utilize, on the other hand. Different access control systems have been proposed for different IoT environments, many are remaining only at a conceptual level. In this paper, we propose a decentralized, ledger-based, publish-subscribe based architecture for the administration of access in a smart home IoT environment to preside at the assignments of underlying operational authorizations. Proposed architecture is endorsed by a proof-of-concept implementation, which utilizes smart contracts to ensure the integrity of administration supplemented by intrinsic benefits of blockchain to be distributed and transparent. Despite the rising hype around the blockchain technology that stokes its utilization in different domains, utilizing it for access control purposes is not yet promising. Our implementation results assure using blockchain for administrative access control is propitious, while is not yet appropriate for operational access control, which have been mainly the focus of previously proposed blockchain-based access control works.},
booktitle = {Proceedings of the 2022 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {57–66},
numpages = {10},
keywords = {decentralized access control, smart home iot, access administration, blockchain-based access control},
location = {Baltimore, MD, USA},
series = {Sat-CPS '22}
}

@article{10.1145/2567529.2567531,
author = {Koziolek, Heiko and Becker, Steffen and Happe, Jens and Tuma, Petr and de Gooijer, Thijmen},
title = {Towards Software Performance Engineering for Multicore and Manycore Systems},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/2567529.2567531},
doi = {10.1145/2567529.2567531},
abstract = {In the era of multicore and manycore processors, a systematic engineering approach for software performance becomes more and more crucial to the success of modern software systems. This article argues for more software performance engineering research specifically for multicore and manycore systems, which will have a profound impact on software engineering practices.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {2–11},
numpages = {10}
}

@proceedings{10.1145/3623759,
title = {PLOS '23: Proceedings of the 12th Workshop on Programming Languages and Operating Systems},
year = {2023},
isbn = {9798400704048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the proceedings of the Twelfth Workshop on Programming Languages and Operating Systems (PLOS 2023). The PLOS workshop series brings together people from the programming language and operating system communities to discuss emerging work at the intersection of these fields. It is a venue for discussing new visions, experiences, problems, and solutions arising from the application of advanced programming and software engineering concepts to operating systems construction, and vice versa.},
location = {Koblenz, Germany}
}

@inproceedings{10.1145/1529282.1529413,
author = {Rodrigues, Mauricio Chui and Malkowski, Simon and Ferreira, Jo\~{a}o Eduardo},
title = {Implementing Rigorous Web Services with Process Algebra: Navigation Plan for Web Services},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529413},
doi = {10.1145/1529282.1529413},
abstract = {Despite the popularity of standards such as BPEL in business-critical applications, rigorous approaches to web service composition remain an open research problem. Frameworks based on formal foundations (e.g., process algebra or Petri nets) have emerged as promising approach to address these challenges. This work introduces the Navigation Plan for Web Services (NPWS), a system module, which extends a process algebra based workflow engine with a web service interface. We systematically combine the web service paradigm and comprehensive real-world workflow functionality while guaranteeing sound properties through formal process specification. Process instantiation and execution monitoring are implemented with Enterprise JavaBeans, SQL extension, and Java Persistence API to ensure flexibility and scalable integration. Our primary contribution is an applied approach to implementing complex web services with formal properties through a well-defined process algebraic core. We further illustrate our system with sample client applications and a case study based on an actual deployment in a library environment.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {625–631},
numpages = {7},
keywords = {process algebra, software, implementation, BPEL, web services, workflow},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@article{10.1145/3544496,
author = {Nguyen, Thien and McCaskey, Alexander J.},
title = {Extending Python for Quantum-Classical Computing via Quantum Just-in-Time Compilation},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3544496},
doi = {10.1145/3544496},
abstract = {Python is a popular programming language known for its flexibility, usability, readability, and focus on developer productivity. The quantum software community has adopted Python on a number of large-scale efforts due to these characteristics, as well as the remote nature of near-term quantum processors. The use of Python has enabled quick prototyping for quantum code that directly benefits pertinent research and development efforts in quantum scientific computing. However, this rapid prototyping ability comes at the cost of future performant integration for tightly coupled CPU-QPU architectures with fast-feedback. Here, we present a language extension to Python that enables heterogeneous quantum-classical computing via a robust C++ infrastructure for quantum just-in-time (QJIT) compilation. Our work builds off the QCOR C++ language extension and compiler infrastructure to enable a single-source, quantum hardware-agnostic approach to quantum-classical computing that retains the performance required for tightly coupled CPU-QPU compute models. We detail this Python extension, its programming model and underlying software architecture, and provide a robust set of examples to demonstrate the utility of our approach.},
journal = {ACM Transactions on Quantum Computing},
month = {jul},
articleno = {24},
numpages = {25},
keywords = {quantum programming, domain specific languages, compilers, Quantum computing}
}

@inproceedings{10.1145/3242587.3242649,
author = {Oney, Steve and Lundgard, Alan and Krosnick, Rebecca and Nebeling, Michael and Lasecki, Walter S.},
title = {Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242649},
doi = {10.1145/3242587.3242649},
abstract = {Many web pages developed today require navigation by visual interaction-seeing, hovering, pointing, clicking, and dragging with the mouse over dynamic page content. These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based screen readers to navigate the web. Despite existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting guidelines and best practices, automatically generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which tend to diminish an end user's agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages. We introduce *Arboretum*, a new architecture that enables any web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a "propose and accept/reject" mechanism. We illustrate the benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We evaluate the entire system in a study with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {937–949},
numpages = {13},
keywords = {accessibility, non-visual access, web accessibility, web interfaces, crowdsourcing, remote collaboration, blind},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/1352135.1352201,
author = {Brylow, Dennis},
title = {An Experimental Laboratory Environment for Teaching Embedded Operating Systems},
year = {2008},
isbn = {9781595937995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352135.1352201},
doi = {10.1145/1352135.1352201},
abstract = {This paper describes Marquette University's efforts to build an experimental embedded systems laboratory for hands-on projects in an operating systems course. Our prototype laboratory is now serving as the basis for a coherent sequence of class projects threaded throughout courses in hardware systems, operating systems, networking, and embedded systems. We describe the major components of our Embedded XINU laboratory environment, the operating systems course, and related improvements in other core courses of our curriculum.},
booktitle = {Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education},
pages = {192–196},
numpages = {5},
keywords = {embedded XINU, curriculum, embedded systems education},
location = {Portland, OR, USA},
series = {SIGCSE '08}
}

@inproceedings{10.1145/3400302.3415791,
author = {Taylor, Michael Bedford},
title = {Your Agile Open Source HW Stinks (Because It is Not a System)},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400302.3415791},
doi = {10.1145/3400302.3415791},
abstract = {Exciting times in hardware design are upon us. Spearheaded by the RISC-V ISA, and the recent DARPA POSH/IDEA program which focuses on both open source IP and open source CAD, a large number of open source HW projects are underway. Academics are increasingly releasing their code online. Many agile open source HW projects envision a hypothetical user that may not actually exist. To acquire real users, we must be pragmatic about what kinds of systems our HW will go into and focus on the roadblocks unique to those systems.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {95},
numpages = {6},
keywords = {open source HW},
location = {Virtual Event, USA},
series = {ICCAD '20}
}

@article{10.1145/1562764.1562783,
author = {Asanovic, Krste and Bodik, Rastislav and Demmel, James and Keaveny, Tony and Keutzer, Kurt and Kubiatowicz, John and Morgan, Nelson and Patterson, David and Sen, Koushik and Wawrzynek, John and Wessel, David and Yelick, Katherine},
title = {A View of the Parallel Computing Landscape},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/1562764.1562783},
doi = {10.1145/1562764.1562783},
abstract = {Writing programs that scale with increasing numbers of cores should be as easy as writing programs for sequential computers.},
journal = {Commun. ACM},
month = {oct},
pages = {56–67},
numpages = {12}
}

@inproceedings{10.1145/3465481.3470064,
author = {Garc\'{\i}a Aguilar, Iv\'{a}n and Mu\~{n}oz Gallego, Antonio},
title = {A Threat Model Analysis of a Mobile Agent-Based System on Raspberry Pi},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470064},
doi = {10.1145/3465481.3470064},
abstract = {Security is considered one of the critical points in any computer system. Nowadays, a multitude of protocols and computer models are appearing along with new attacks increasing the need to develop solutions. This work focuses on the protection of the agent as well as the information it processes in a distributed environment throughout the network. Mobile agents move between various network-enabled platforms to process the information they manage. To simulate an environment based on the Internet of Things (IoT), a scheme has been presented which details the necessary steps to be carried out by the agent to perform the migration. Today it was proved that there is no infallible solution that guarantees the security of the whole system. However, the importance of security mechanisms to reduce and/or mitigate security threats is fundamental. This work is a study based on a mobile agent-based approach that travels from host to host. A review of different threats to this particular model is presented. Throughout this work a detailed study is presented based on the migration protocol of the agents, which will be determined by using modeling tools such as Microsoft Modeling Tool (MMT) used in this case, to discover and detail each of the threats presented by this protocol. Additionally, an alternative as a solution according to a protocol that runs thanks to the implementation of hardware elements is proposed, which makes use of a TPM, thus determining which threats are mitigated or solved by implementing such hardware in conjunction with the protocol developed for this purpose.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {111},
numpages = {10},
keywords = {IoT, Raspberry Pi, Cryptographic Hardware, Embedded Boards, Secure Mobile Agent, TPM(Trusted Platform Module)},
location = {Vienna, Austria},
series = {ARES '21}
}

@article{10.1145/3358224,
author = {Pan, Runyu and Parmer, Gabriel},
title = {MxU: Towards Predictable, Flexible, and Efficient Memory Access Control for the Secure IoT},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3358224},
doi = {10.1145/3358224},
abstract = {The advanced functionality requirements of modern embedded and Internet of Things (IoT) devices -- from autonomous vehicles, to city and power-grid management -- are driving an ever-increasing software complexity. At the same time, the pervasive internet connections of these systems necessitate the fundamental design of security into these devices. The isolation of complex features from those that are critical through protection domains is an effective means to constrain the scope of faults and security breaches. Common hardware-provided memory facilities to enforce protection domains through memory access control -- including Memory Management Units (MMUs) usually found in microprocessors, and Memory Protection Units (MPUs) usually found in microcontrollers -- must meet the goals of enabling flexible, efficient and dynamic management of memory, and must enable tight bounds on the worst-case execution of critical code. Unfortunately, current system memory management facilities are ill-prepared to handle this challenge: MMUs that use extensive caches to achieve strong average-case performance suffer from debilitating worst-case and even average-case behavior under hefty interference, while MPUs struggle to provide flexible memory management.This paper details MxU, a memory protection and allocation abstraction that integrates temporal specifications into the memory management subsystem, to enable portable code to achieve both predictable, tightly-bounded execution and dynamic management across both MMU- and MPU-based systems. We implement MxU in the Composite microkernel, and evaluate its flexibility and predictability over two different architectures: a MPU-based Cortex-M7 microcontroller and a MMU-based Cortex-A9 microprocessor using a suite of modern applications including neural network-based inference, SQLite, and a javascript runtime. For MMU-based systems, MxU reduces application TLB stall by up to 68.0%. For MPU-based systems, MxU enables flexible dynamic memory management often with application overheads of 1%, increasing to 6.1% under significant interference.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {oct},
articleno = {103},
numpages = {20},
keywords = {IoT, MPU, MMU, memory access control}
}

@inproceedings{10.1145/1518701.1518853,
author = {Yatani, Koji and Chung, Eunyoung and Jensen, Carlos and Truong, Khai N.},
title = {Understanding How and Why Open Source Contributors Use Diagrams in the Development of Ubuntu},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518853},
doi = {10.1145/1518701.1518853},
abstract = {Some of the most interesting differences between Open Source Software (OSS) development and commercial co-located software development lie in the communication and collaboration practices of these two groups of developers. One interesting practice is that of diagramming. Though well studied and important in many aspects of co-located software development (including communication and collaboration among developers), its role in OSS development has not been thoroughly studied. In this paper, we report our investigation on how and why Ubuntu contributors use diagrams in their work. Our study shows that diagrams are not actively used in many scenarios where they commonly would in co-located software development efforts. We describe differences in the use and practices of diagramming, their possible reasons, and present design considerations for potential systems aimed at better supporting diagram use in OSS development.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {995–1004},
numpages = {10},
keywords = {diagramming, software development, visual representation, open source software (oss)},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{10.1145/2491159.2491167,
author = {Hu, Pili and Li, Junbo and Lau, Wing Cheong},
title = {PIXS: Programmable Intelligence for Cross-Platform Socialization},
year = {2013},
isbn = {9781450321778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491159.2491167},
doi = {10.1145/2491159.2491167},
abstract = {With the proliferation of the emerging Online Social Networks and other conventional communication services, there is an increasing need for a tool which can facilitate individual users to effectively socialize across multiple, heterogeneous platforms. While the diverse nature of the heterogeneous services already makes the design of a cross-platform socialization tool challenging, an even more daunting task is to tackle the "noisy" nature of the Social Networking Services (SNS). Existing solutions all lack flexibility and extensibility, especially in supporting advanced users to better manage their cross-platform socialization via customized information processing. In this paper, we propose PIXS (Programmable Intelligence for Cross-platform Socialization) -- an open-source, extensible middleware which provides efficient information acquisition and dissemination across heterogeneous SNSs. A distinguishing feature of PIXS is its support of script-based operations. As a proof-of-concept to demonstrate the flexibility and effectiveness of PIXS, we have developed for it a Python-based semi-supervised learning application which can prioritize incoming messages from different platforms via a Rank Preserving Regression (RPR) framework. This framework can readily incorporate the domain knowledge of the end user. Our SGD-based approach also enables adaptive and incremental training of the ranking system according to the gradual evolution of the user preference. Performance evaluation based on real message traces shows that the proposed system can boost the user's efficiency in identifying and forwarding important messages across heterogeneous SNS platforms. Additional use-cases of PIXS are also discussed.},
booktitle = {Proceedings of the 5th ACM Workshop on HotPlanet},
pages = {33–38},
numpages = {6},
keywords = {personalization, social networking services, middleware},
location = {Hong Kong, China},
series = {HotPlanet '13}
}

@inproceedings{10.1145/3551349.3559570,
author = {Sarker, Jaydeb},
title = {Identification and Mitigation of Toxic Communications Among Open Source Software Developers},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559570},
doi = {10.1145/3551349.3559570},
abstract = {Toxic and unhealthy conversations during the developer’s communication may reduce the professional harmony and productivity of Free and Open Source Software (FOSS) projects. For example, toxic code review comments may raise pushback from an author to complete suggested changes. A toxic communication with another person may hamper future communication and collaboration. Research also suggests that toxicity disproportionately impacts newcomers, women, and other participants from marginalized groups. Therefore, toxicity is a barrier to promote diversity, equity, and inclusion. Since the occurrence of toxic communications is not uncommon among FOSS communities and such communications may have serious repercussions, the primary objective of my proposed dissertation is to automatically identify and mitigate toxicity during developers’ textual interactions. On this goal, I aim to: i) build an automated toxicity detector for Software Engineering (SE) domain, ii) identify the notion of toxicity across demographics, and iii) analyze the impacts of toxicity on the outcomes of Open Source Software (OSS) projects.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {124},
numpages = {5},
keywords = {toxicity, developers’ interactions, deep learning, NLP},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.14778/2732269.2732270,
author = {Mahmoud, Hatem A. and Arora, Vaibhav and Nawab, Faisal and Agrawal, Divyakant and El Abbadi, Amr},
title = {MaaT: Effective and Scalable Coordination of Distributed Transactions in the Cloud},
year = {2014},
issue_date = {Janary 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732269.2732270},
doi = {10.14778/2732269.2732270},
abstract = {The past decade has witnessed an increasing adoption of cloud database technology, which provides better scalability, availability, and fault-tolerance via transparent partitioning and replication, and automatic load balancing and fail-over. However, only a small number of cloud databases provide strong consistency guarantees for distributed transactions, despite decades of research on distributed transaction processing, due to practical challenges that arise in the cloud setting, where failures are the norm, and human administration is minimal. For example, dealing with locks left by transactions initiated by failed machines, and determining a multi-programming level that avoids thrashing without under-utilizing available resources, are some of the challenges that arise when using lock-based transaction processing mechanisms in the cloud context. Even in the case of optimistic concurrency control, most proposals in the literature deal with distributed validation but still require the database to acquire locks during two-phase commit when installing updates of a single transaction on multiple machines. Very little theoretical work has been done to entirely eliminate the need for locking in distributed transactions, including locks acquired during two-phase commit. In this paper, we re-design optimistic concurrency control to eliminate any need for locking even for atomic commitment, while handling the practical issues in earlier theoretical work related to this problem. We conduct an extensive experimental study to evaluate our approach against lock-based methods under various setups and workloads, and demonstrate that our approach provides many practical advantages in the cloud context.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {329–340},
numpages = {12}
}

@inproceedings{10.1145/3403746.3403904,
author = {Zhang, Qi and Yang, Shulin and Ren, Ruoyu},
title = {Research on Uni-App Based Cross-Platform Digital Textbook System},
year = {2020},
isbn = {9781450375528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3403746.3403904},
doi = {10.1145/3403746.3403904},
abstract = {The rapid development of mobile Internet has promoted the reform of mobile digital textbooks. Mobile terminals have opened up a new mode of people's life and study, and brand new platforms such as PC, tablet and smartphone are becoming more and more popular in people's life. Through the continuous application of new technology, digital textbooks have different product forms and organizational characteristics from traditional textbooks. However, due to the variety of mobile terminals, the development cost of digital textbooks increases. In view of the above situation, this paper puts forward the problem of cross-platform mobile digital teaching materials, introduces cross-platform technology into the field of education industry, and at the same time studies the key technologies and the overall structure used in digital teaching materials. Based on the analysis of mobile APP development technology, uni-app technology is proposed to solve the problems existing in mobile digital textbooks, and uni-app based cross-platform digital textbooks system implementation scheme and content presentation scheme are presented.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Software Engineering},
pages = {52–57},
numpages = {6},
keywords = {Digital textbooks, Hybrid application, Cross-platform technology},
location = {Beijing, China},
series = {CSSE '20}
}

@inproceedings{10.1145/3485447.3512236,
author = {Tan, Xin and Zhang, Yuan and Cao, Jiajun and Sun, Kun and Zhang, Mi and Yang, Min},
title = {Understanding the Practice of Security Patch Management across Multiple Branches in OSS Projects},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512236},
doi = {10.1145/3485447.3512236},
abstract = {Since the users of open source software (OSS) projects may not use the latest version all the time, OSS development teams often support code maintenance for old versions through maintaining multiple stable branches. Typically, the developers create a stable branch for each old stable version, deploy security patches on the branch, and release fixed versions at regular intervals. As such, old-version applications in production environments are protected from the disclosed vulnerabilities in a long time. However, the rapidly growing number of OSS vulnerabilities has greatly strained this patch deployment model, and a critical need has arisen for the security community to understand the practice of security patch management across stable branches. In this work, we conduct a large-scale empirical study of stable branches in OSS projects and the security patches deployed on them via investigating 608 stable branches belonging to 26 popular OSS projects as well as more than 2,000 security fixes for 806 CVEs deployed on stable branches. Our study distills several important findings: (i) more than 80% affected CVE-Branch pairs are unpatched; (ii) the unpatched vulnerabilities could pose a serious security risk to applications in use, with 47.39% of them achieving a CVSS score over 7 (High or Critical Severity); and (iii) the patch porting process requires great manual efforts and takes an average of 40.46 days, significantly extending the time window for N-day vulnerability attacks. Our results reveal the worrying state of security patch management across stable branches. We hope our study can shed some light on improving the practice of patch management in OSS projects.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {767–777},
numpages = {11},
keywords = {OSS Vulnerabilities, Patch Deployment Study, Security Patches},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3524842.3528464,
author = {Menon, Harshitha and Parasyris, Konstantinos and Scogland, Tom and Gamblin, Todd},
title = {Searching for High-Fidelity Builds Using Active Learning},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528464},
doi = {10.1145/3524842.3528464},
abstract = {Modern software is incredibly complex. A typical application may comprise hundreds or thousands of reusable components. Automated package managers can help to maintain a consistent set of dependency versions, but ultimately the solvers in these systems rely on constraints generated by humans. At scale, small errors add up, and it becomes increasingly difficult to find high-fidelity configurations. We cannot test all configurations, because the space is combinatorial, so exhaustive exploration is infeasible.In this paper, we present Reliabuild, an auto-tuning framework that efficiently explores the build configuration space and learns which package versions are likely to result in a successful configuration. We implement two models in Reliabuild to rank the different configurations and use adaptive sampling to select good configurations with fewer samples. We demonstrate Reliabuild's effectiveness by evaluating 31,186 build configurations of 61 packages from the Extreme-scale Scientific Software Stack (E4S). Reliabuild selects good configurations efficiently. For example, Reliabuild selects 3\texttimes{} the number of good configurations in comparison to random sampling for several packages including Abyss, Bolt, libnrm, OpenMPI. Our framework is also able to select all the high-fidelity builds in half the number of samples required by random sampling for packages such as Chai, OpenMPI, py-petsc4py, and slepc. We further use the model to learn statistics about the compatibility of different packages, which will enable package solvers to better select high-fidelity build configurations automatically.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {179–190},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3297858.3304065,
author = {Bai, Jia-Ju and Lawall, Julia and Tan, Wende and Hu, Shi-Min},
title = {DCNS: Automated Detection Of Conservative Non-Sleep Defects in the Linux Kernel},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304065},
doi = {10.1145/3297858.3304065},
abstract = {For waiting, the Linux kernel offers both sleep-able and non-sleep operations. However, only non-sleep operations can be used in atomic context. Detecting the possibility of execution in atomic context requires a complete inter-procedural flow analysis, often involving function pointers. Developers may thus conservatively use non-sleep operations even outside of atomic context, which may damage system performance, as such operations unproductively monopolize the CPU. Until now, no systematic approach has been proposed to detect such conservative non-sleep (CNS) defects.In this paper, we propose a practical static approach, named DCNS, to automatically detect conservative non-sleep defects in the Linux kernel. DCNS uses a summary-based analysis to effectively identify the code in atomic context and a novel file-connection-based alias analysis to correctly identify the set of functions referenced by a function pointer. We evaluate DCNS on Linux 4.16, and in total find 1629 defects. We manually check 943 defects whose call paths are not so difficult to follow, and find that 890 are real. We have randomly selected 300 of the real defects and sent them to kernel developers, and 251 have been confirmed.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {287–299},
numpages = {13},
keywords = {linux kernel, function-pointer analysis, atomic context, defect detection},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{10.5555/2738600.2738607,
author = {Stepanov, Evgeniy and Serebryany, Konstantin},
title = {MemorySanitizer: Fast Detector of Uninitialized Memory Use in C++},
year = {2015},
isbn = {9781479981618},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents MemorySanitizer, a dynamic tool that detects uses of uninitialized memory in C and C++. The tool is based on compile time instrumentation and relies on bit-precise shadow memory at run-time. Shadow propagation technique is used to avoid false positive reports on copying of uninitialized memory.MemorySanitizer finds bugs at a modest cost of 2.5x in execution time and 2x in memory usage; the tool has an optional origin tracking mode that provides better reports with moderate extra overhead. The reports with origins are more detailed compared to reports from other similar tools; such reports contain names of local variables and the entire history of the uninitialized memory including intermediate stores. In this paper we share our experience in deploying the tool at a large scale and demonstrate the benefits of compile-time instrumentation over dynamic binary instrumentation.},
booktitle = {Proceedings of the 13th Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {46–55},
numpages = {10},
location = {San Francisco, California},
series = {CGO '15}
}

@inproceedings{10.1145/1993478.1993495,
author = {Singer, Jeremy and Kovoor, George and Brown, Gavin and Luj\'{a}n, Mikel},
title = {Garbage Collection Auto-Tuning for Java Mapreduce on Multi-Cores},
year = {2011},
isbn = {9781450302630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993478.1993495},
doi = {10.1145/1993478.1993495},
abstract = {MapReduce has been widely accepted as a simple programming pattern that can form the basis for efficient, large-scale, distributed data processing. The success of the MapReduce pattern has led to a variety of implementations for different computational scenarios. In this paper we present MRJ, a MapReduce Java framework for multi-core architectures. We evaluate its scalability on a four-core, hyperthreaded Intel Core i7 processor, using a set of standard MapReduce benchmarks. We investigate the significant impact that Java runtime garbage collection has on the performance and scalability of MRJ. We propose the use of memory management auto-tuning techniques based on machine learning. With our auto-tuning approach, we are able to achieve MRJ performance within 10% of optimal on 75% of our benchmark tests.},
booktitle = {Proceedings of the International Symposium on Memory Management},
pages = {109–118},
numpages = {10},
keywords = {garbage collection, machine learning, java, mapreduce},
location = {San Jose, California, USA},
series = {ISMM '11}
}

@inproceedings{10.1145/3388763.3407760,
author = {Mokhov, Serguei A. and Song, Miao and Mudur, Sudhir P. and Grogono, Peter},
title = {Dataflow VFX Programming and Processing for Artists and OpenISS},
year = {2020},
isbn = {9781450379700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388763.3407760},
doi = {10.1145/3388763.3407760},
abstract = {We explore the notion of dataflow programming in realtime graphics artistic practice. We complement the previous editions of the course at SIGGRAPH Asia (2015, 2016, 2018) and SIGGRAPH (2017, 2019) to include OpenISS and PureData/GEM. First, we explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with RGBD cameras. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics on stage. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2 and its OpenISS core framework for creative near-realtime broadcasting, and the use of AI and HCI techniques in art.},
booktitle = {ACM SIGGRAPH 2020 Labs},
articleno = {2},
numpages = {32},
keywords = {OpenISS, human-computer interaction, RGBD cameras, Jitter/MAX, OpenGL, frameworks, PureData/GEM, Illimitable Space System (ISS), Processing, computer graphics education, real-time},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@proceedings{10.1145/3600160,
title = {ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Benevento, Italy}
}

@article{10.1145/1764810.1764815,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1764810.1764815},
doi = {10.1145/1764810.1764815},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {14–23},
numpages = {10}
}

@inproceedings{10.1145/3560834.3563830,
author = {Choudhari, Amit and Guilley, Sylvain and Karray, Khaled},
title = {SpecDefender: Transient Execution Attack Defender Using Performance Counters},
year = {2022},
isbn = {9781450398848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560834.3563830},
doi = {10.1145/3560834.3563830},
abstract = {Side-channel attacks based on speculative execution have gained enough traction for researchers. This has resulted in the development of more creative variants of Spectre and its defences. However, many of these defence strategies end up making speculative execution or branch prediction ineffective. While these techniques protect the system, they cut down performance by more than 50%. Hence, these solutions cannot be deployed. In this paper, we present a framework that not only protects against different variants of Spectre but also maintains the performance. We prototyped this framework using a novel tool SpecDefender. It leverages Hardware Performance Counter (HPC) registers to dynamically detect active Spectre attacks and performs dynamic instrumentation to defend against them. This makes the tool widely applicable without any need for static analysis. Overall, the tool brings back the balance between performance and security. The tool was evaluated based on its accuracy and precision to detect an attack in different scenarios. It exhibit &gt;90% precision when five out of ten processes were simultaneously attacked. The response time for the tool to detect is ~2 sec. Furthermore, the throughput of the process under attack was comparable to normal execution in presence of SpecDefender.},
booktitle = {Proceedings of the 2022 Workshop on Attacks and Solutions in Hardware Security},
pages = {15–24},
numpages = {10},
keywords = {efficient mitigation, speculative execution, spectre, transient execution attack},
location = {Los Angeles, CA, USA},
series = {ASHES'22}
}

@inproceedings{10.1145/1869459.1869487,
author = {Kell, Stephen},
title = {Component Adaptation and Assembly Using Interface Relations},
year = {2010},
isbn = {9781450302036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869459.1869487},
doi = {10.1145/1869459.1869487},
abstract = {Software's expense owes partly to frequent reimplementation of similar functionality and partly to maintenance of patches, ports or components targeting evolving interfaces. More modular non-invasive approaches are unpopular because they entail laborious wrapper code. We propose Cake, a rule-based language describing compositions using interface relations. To evaluate it, we compare several existing wrappers with reimplemented Cake versions, finding the latter to be simpler and better modularised.},
booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {322–340},
numpages = {19},
keywords = {cake, composition, assembly, components, adaptation, interoperation, interoperability},
location = {Reno/Tahoe, Nevada, USA},
series = {OOPSLA '10}
}

@article{10.1145/3630008,
author = {Li, Jia and Li, Zhuo and Zhang, HuangZhao and Li, Ge and Jin, Zhi and Hu, Xing and Xia, Xin},
title = {Poison Attack and Poison Detection on Deep Source Code Processing Models},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630008},
doi = {10.1145/3630008},
abstract = {In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely poison attacks. The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector. CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that ① CodePoisoner
conducts successful poison attacks with a high attack success rate (avg: 98.3%, max: 100%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. ② CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Deep Learning, Poison Detection, Source Code Processing, Poison Attack}
}

@inproceedings{10.1145/3027063.3053345,
author = {Lisowska Masson, Agnes and Amstutz, Timon and Lalanne, Denis},
title = {A Usability Refactoring Process for Large-Scale Open Source Projects: The ILIAS Case Study},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053345},
doi = {10.1145/3027063.3053345},
abstract = {This case study presents efforts to introduce and encourage consistent application of usability and user interface design principles in an existing large-scale open source project. We present the project in question, the ILIAS learning management system, the challenges involved in incorporating usability into an open source project, the novel solution we proposed, a set of tools called the Kitchen Sink, and the steps that were needed to have the solution accepted and used by the ILIAS community. We conclude with a discussion of the lessons learned and an assessment of the overall success of our efforts.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1135–1143},
numpages = {9},
keywords = {usability, open source software, learning management systems, open source communities},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@article{10.14778/3611540.3611560,
author = {Yang, Zhifeng and Xu, Quanqing and Gao, Shanyan and Yang, Chuanhui and Wang, Guoping and Zhao, Yuzhong and Kong, Fanyu and Liu, Hao and Wang, Wanhong and Xiao, Jinliang},
title = {OceanBase Paetica: A Hybrid Shared-Nothing/Shared-Everything Database for Supporting Single Machine and Distributed Cluster},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611560},
doi = {10.14778/3611540.3611560},
abstract = {In the ongoing evolution of the OceanBase database system, it is essential to enhance its adaptability to small-scale enterprises. The OceanBase database system has demonstrated its stability and effectiveness within the Ant Group and other commercial organizations, besides through the TPC-C and TPC-H tests. In this paper, we have designed a stand-alone and distributed integrated architecture named Paetica to address the overhead caused by the distributed components in the stand-alone mode, with respect to the OceanBase system. Paetica enables adaptive configuration of the database that allows OceanBase to support both serial and parallel executions in stand-alone and distributed scenarios, thus providing efficiency and economy. This design has been implemented in version 4.0 of the OceanBase system, and the experiments show that Paetica exhibits notable scalability and outperforms alternative stand-alone or distributed databases. Furthermore, it enables the transition of OceanBase from primarily serving large enterprises to truly catering to small and medium enterprises, by employing a single OceanBase database for the successive stages of enterprise or business development, without the requirement for migration. Our experiments confirm that Paetica has achieved linear scalability with the increasing CPU core number within the stand-alone mode. It also outperforms MySQL and Greenplum in the Sysbench and TPC-H evaluations.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3728–3740},
numpages = {13}
}

@inproceedings{10.1145/3338906.3338955,
author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
title = {A Comprehensive Study on Deep Learning Bug Characteristics},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338955},
doi = {10.1145/3338906.3338955},
abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {510–520},
numpages = {11},
keywords = {Bugs, Q&amp;A forums, Deep learning bugs, Deep learning software, Empirical Study of Bugs},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3440995,
author = {Aras, Emekcan and Delbruel, St\'{e}phane and Yang, Fan and Joosen, Wouter and Hughes, Danny},
title = {Chimera: A Low-Power Reconfigurable Platform for Internet of Things},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3440995},
doi = {10.1145/3440995},
abstract = {The Internet of Things (IoT) is being deployed in an ever-growing range of applications, from industrial monitoring to smart buildings to wearable devices. Each of these applications has specific computational requirements arising from their networking, system security, and edge analytics functionality. This diversity in requirements motivates the need for adaptable end-devices, which can be re-configured and re-used throughout their lifetime to handle computation-intensive tasks without sacrificing battery lifetime. To tackle this problem, this article presents Chimera, a low-power platform for research and experimentation with reconfigurable hardware for the IoT end-devices. Chimera achieves flexibility and re-usability through an architecture based on a Flash Field Programmable Gate Array (FPGA) with a reconfigurable software stack that enables over-the-air hardware and software evolution at runtime. This adaptability enables low-cost hardware/software upgrades on the end-devices and an increased ability to handle computationally-intensive tasks. This article describes the design of the Chimera hardware platform and software stack, evaluates it through three application scenarios, and reviews the factors that have thus far prevented FPGAs from being utilized in IoT end-devices.},
journal = {ACM Trans. Internet Things},
month = {mar},
articleno = {10},
numpages = {25},
keywords = {Reconfigurable sensor node, FPGA, sensor network}
}

@inproceedings{10.1145/1774674.1774684,
author = {Tikhanoff, V. and Cangelosi, A. and Fitzpatrick, P. and Metta, G. and Natale, L. and Nori, F.},
title = {An Open-Source Simulator for Cognitive Robotics Research: The Prototype of the ICub Humanoid Robot Simulator},
year = {2008},
isbn = {9781605582931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774674.1774684},
doi = {10.1145/1774674.1774684},
abstract = {This paper presents the prototype of a new computer simulator for the humanoid robot iCub. The iCub is a new open-source humanoid robot developed as a result of the "RobotCub" project, a collaborative European project aiming at developing a new open-source cognitive robotics platform. The iCub simulator has been developed as part of a joint effort with the European project "ITALK" on the integration and transfer of action and language knowledge in cognitive robots. This is available open-source to all researchers interested in cognitive robotics experiments with the iCub humanoid platform.},
booktitle = {Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems},
pages = {57–61},
numpages = {5},
keywords = {iCub humanoid robot, cognitive robotics, simulator, open-source},
location = {Gaithersburg, Maryland},
series = {PerMIS '08}
}

@article{10.1145/1378727.1378740,
author = {Garrett, Matthew},
title = {Powering Down},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/1378727.1378740},
doi = {10.1145/1378727.1378740},
abstract = {Smart power management is all about doing more with the resources we have.},
journal = {Commun. ACM},
month = {sep},
pages = {42–46},
numpages = {5}
}

@inproceedings{10.1145/3293881.3295781,
author = {Foster, Derek and White, Laurie and Adams, Joshua and Erdil, D. Cenk and Hyman, Harvey and Kurkovsky, Stan and Sakr, Majd and Stott, Lee},
title = {Cloud Computing: Developing Contemporary Computer Science Curriculum for a Cloud-First Future},
year = {2018},
isbn = {9781450362238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293881.3295781},
doi = {10.1145/3293881.3295781},
abstract = {Cloud Computing adoption has seen significant growth over the last five years. It offers a diverse range of scalable and redundant service deployment models, including Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), Software-as-a-Service (SaaS), and Containers-as-a-Service (CaaS). These models are applied to areas such as IoT, Cyber-Physical Systems, Social Media, Data Science, Media Streaming, Ecommerce, and Health Informatics. The growth in cloud presents challenges for companies to source cloud expertise to support their business, particularly small and medium-sized enterprises with limited resources. The UK Government recently published the Digital Skills Crisis report, identifying skill-set challenges facing industry, with a shortage in cloud skills negatively impacting business. While cloud technologies have evolved at significant pace, the development of Computer Science curriculum in the further and higher education sector has lagged behind. The challenges faced in the sector includes the training of educators, institutional gaps (software and hardware policies), regulatory constraints, and access to cloud platforms. By embedding fundamental cloud skills throughout the educator and student journey, both stakeholders will be better positioned to understand and practically apply the use of appropriate cloud services, and produce graduates to support the needs of industry. This working group has carried out work to: i) assess current cloud computing curricula in CS and similar programs, ii) document industry needs for in-demand cloud skills, iii) identify issues and gaps around cloud curriculum uptake, and iv) develop solutions to meet the skill demands on core Cloud Computing topics, technical skills exercises, and modules for integration with contemporary Computer Science curricula.},
booktitle = {Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {130–147},
numpages = {18},
keywords = {Distributed Computing, Computer Science, Education, Curriculum Development, Cloud Computing},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018 Companion}
}

@inproceedings{10.1109/FLOSS.2009.5071354,
author = {Wright, Hyrum K. and Perry, Dewayne E.},
title = {Subversion 1.5: A Case Study in Open Source Release Mismanagement},
year = {2009},
isbn = {9781424437207},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2009.5071354},
doi = {10.1109/FLOSS.2009.5071354},
abstract = {In June 2008, the Subversion development team released Subversion 1.5.0. This release contained a number of new features, but arrived only after a long and difficult development, test and release cycle. This protracted process confused and frustrated both users and developers. In this paper, we discuss the events which led to this breakdown, how the release process is being improved, and what lessons other open source projects can learn from the Subversion community's mistakes.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
pages = {13–18},
numpages = {6},
series = {FLOSS '09}
}

@article{10.1145/3149376,
author = {Vef, Marc-Andr\'{e} and Tarasov, Vasily and Hildebrand, Dean and Brinkmann, Andr\'{e}},
title = {Challenges and Solutions for Tracing Storage Systems: A Case Study with Spectrum Scale},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3149376},
doi = {10.1145/3149376},
abstract = {IBM Spectrum Scale’s parallel file system General Parallel File System (GPFS) has a 20-year development history with over 100 contributing developers. Its ability to support strict POSIX semantics across more than 10K clients leads to a complex design with intricate interactions between the cluster nodes. Tracing has proven to be a vital tool to understand the behavior and the anomalies of such a complex software product. However, the necessary trace information is often buried in hundreds of gigabytes of by-product trace records. Further, the overhead of tracing can significantly impact running applications and file system performance, limiting the use of tracing in a production system.In this research article, we discuss the evolution of the mature and highly scalable GPFS tracing tool and present the exploratory study of GPFS’ new tracing interface, FlexTrace, which allows developers and users to accurately specify what to trace for the problem they are trying to solve. We evaluate our methodology and prototype, demonstrating that the proposed approach has negligible overhead, even under intensive I/O workloads and with low-latency storage devices.},
journal = {ACM Trans. Storage},
month = {apr},
articleno = {18},
numpages = {24},
keywords = {GPFS, Parallel file system, trace analysis, performance}
}

@inproceedings{10.1145/1370114.1370131,
author = {Hossain, Liaquat and Zhou, David},
title = {Measuring OSS Quality Trough Centrality},
year = {2008},
isbn = {9781605580395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370114.1370131},
doi = {10.1145/1370114.1370131},
abstract = {In this study, we explore whether the degree of centrality, betweenness and density of the open source software or OSS team communications network have any bearing on the quality of the software developed. We measure the quality of OSS in terms of number of defect fixed per software promotion, the number of defects reported at different severity levels and the average number of days for a defect to be fixed for each project team. The data required to conduct the analysis needs to be of OSS projects, their team structure and also contribution of the projects user community and immediate development team. We extract the communications pattern of OSS projects development teams from online forums or message boards as the developers are usually located in different geographic areas. We use SorceForge.net for collecting relevant coordination related data for this study; which is the central resource for hosting more than 100,000 open source development projects and with over 1 million registered users that participate in the development of high profile OSS projects. The outcome of this study suggests that there is a correlation between social network characteristics and strong and poor performing projects in an OSS environment.},
booktitle = {Proceedings of the 2008 International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {65–68},
numpages = {4},
keywords = {coordination, social networks, open source software, distributed teams},
location = {Leipzig, Germany},
series = {CHASE '08}
}

@inproceedings{10.1145/2835043.2835062,
author = {Chawla, Mandeep K. and Chhabra, Indu},
title = {SQMMA: Software Quality Model for Maintainability Analysis},
year = {2015},
isbn = {9781450336505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835043.2835062},
doi = {10.1145/2835043.2835062},
abstract = {Software Quality is generally characterized by several factors which reveal the degree of success of the software developed and its future maintainability. Although many existing models provide frameworks of huge potential pertaining to quality analysis, yet offer little support to make them operational and handy to use. Nonetheless, by taking ideas from existing models, new metrics, their relationships with external quality factors and associated weights can be defined and evaluated. Keeping these views in mind, this paper proposes a new quality model (SQMMA) which offers ready-to-use mathematical formulas to quantify four quality attributes namely Analyzability, Changeability, Stability and Testability as weighted sum of a set of software code metrics. These attributes further act as determinants to evaluate `Maintainability' characteristic of a software, according to ISO 9126 definition. A revised formula to compute Maintainability, according to ISO 25010, has also been derived. The designed model is then implemented on four versions of Apache tomcat and results are presented. Finally results have been validated through their trend analysis and comparison with bugs/change metrics drawn additionally.},
booktitle = {Proceedings of the 8th Annual ACM India Conference},
pages = {9–17},
numpages = {9},
keywords = {Software Metrics, Quality Attributes, Maintainability, SQMMA, Apache Tomcat, ISO/IEC 9126, AHP},
location = {Ghaziabad, India},
series = {Compute '15}
}

@article{10.14778/3352063.3352143,
author = {Hubail, Murtadha AI and Alsuliman, Ali and Blow, Michael and Carey, Michael and Lychagin, Dmitry and Maxon, Ian and Westmann, Till},
title = {Couchbase Analytics: NoETL for Scalable NoSQL Data Analysis},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352143},
doi = {10.14778/3352063.3352143},
abstract = {Couchbase Server is a highly scalable document-oriented database management system. With a shared-nothing architecture, it exposes a fast key-value store with a managed cache for sub-millisecond data operations, indexing for fast queries, and a powerful query engine for executing declarative SQL-like queries. Its Query Service debuted several years ago and supports high volumes of low-latency queries and updates for JSON documents. Its recently introduced Analytics Service complements the Query Service. Couchbase Analytics, the focus of this paper, supports complex analytical queries (e.g., ad hoc joins and aggregations) over large collections of JSON documents. This paper describes the Analytics Service from the outside in, including its user model, its SQL++ based query language, and its MPP-based storage and query processing architecture. It also briefly touches on the relationship of Couchbase Analytics to Apache AsterixDB, the open source Big Data management system at the core of Couchbase Analytics.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2275–2286},
numpages = {12}
}

@article{10.1145/3610066,
author = {Ekambaranathan, Anirudh and Zhao, Jun and Van Kleek, Max},
title = {How Can We Design Privacy-Friendly Apps for Children? Using a Research through Design Process to Understand Developers' Needs and Challenges},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610066},
doi = {10.1145/3610066},
abstract = {Mobile apps used by children often make use of harmful techniques, such as data tracking and targeted advertising. Previous research has suggested that developers face several systemic challenges in designing apps that prioritise children's best interests. To understand how developers can be better supported, we used a Research through Design (RtD) method to explore what the future of privacy-friendly app development could look like. We performed an elicitation study with 20 children's app developers to understand their needs and requirements. We found a number of specific technical requirements from the participants about how they would like to be supported, such as having actionable transnational design guidelines and easy-to-use development libraries. However, participants were reluctant to adopt these design ideas in their development practices due to perceived financial risks associated with increased privacy in apps. To overcome this critical gap, participants formulated socio-technical requirements that extend to other stakeholders in the mobile industry, including parents and marketplaces. Our findings provide important immediate and long-term design opportunities for the HCI community, and indicate that support for changing app developers' practices must be designed in the context of their relationship with other stakeholders.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {275},
numpages = {29},
keywords = {app developers, children, design workbook, research through design, apps, privacy}
}

@article{10.1145/1462141.1462145,
author = {Economou, Daphne and Gavalas, Damianos and Kenteris, Michael and Tsekouras, George E.},
title = {Cultural Applications for Mobile Devices: Issues and Requirements for Authoring Tools and Development Platforms},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1559-1662},
url = {https://doi.org/10.1145/1462141.1462145},
doi = {10.1145/1462141.1462145},
abstract = {This paper explores requirements that authoring tools and development platforms should satisfy for the development of cultural applications tailored for deployment on Personal Digital Assistants (PDAs) and mobile phones. To effectively determine such requirements the paper reviews the use of mobile technologies in the context of cultural organizations and tourism and examines three 'real world' case studies that focus on the use of PDAs and mobile phones for providing cultural and tourist information, keeping the visitors' interest and attention, as well as promoting various cultural organizations and tourist facilities. This approach allows the extraction of a set of PDA and mobile phone application requirements, the implementation of which is based on the apparatus offered by authoring tools and development platforms. The paper reviews and evaluates the design and development facilities provided by state-of-the-art multimedia application development tools for PDAs and mobile phones: Macromedia Flash Lite, Navipocket, Java 2 Micro Edition and Microsoft .Net platform for the Mobile Web. The paper concludes with a set of recommendations related to the way authoring tools and development platforms should be exploited in order to gratify application and designer needs for developing cultural and tourist applications},
journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
month = {jul},
pages = {18–33},
numpages = {16}
}

@inproceedings{10.1145/2248418.2248420,
author = {Aslam, Faisal and Baig, Ghufran and Qureshi, Mubashir Adnan and Uzmi, Zartash Afzal and Fennell, Luminous and Thiemann, Peter and Schindelhauer, Christian and Haussmann, Elmar},
title = {Rethinking Java Call Stack Design for Tiny Embedded Devices},
year = {2012},
isbn = {9781450312127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2248418.2248420},
doi = {10.1145/2248418.2248420},
abstract = {The ability of tiny embedded devices to run large feature-rich programs is typically constrained by the amount of memory installed on such devices. Furthermore, the useful operation of these devices in wireless sensor applications is limited by their battery life. This paper presents a call stack redesign targeted at an efficient use of RAM storage and CPU cycles by a Java program running on a wireless sensor mote. Without compromising the application programs, our call stack redesign saves 30% of RAM, on average, evaluated over a large number of benchmarks. On the same set of bench-marks, our design also avoids frequent RAM allocations and deallocations, resulting in average 80% fewer memory operations and 23% faster program execution. These may be critical improvements for tiny embedded devices that are equipped with small amount of RAM and limited battery life. However, our call stack redesign is equally effective for any complex multi-threaded object oriented program developed for desktop computers. We describe the redesign, measure its performance and report the resulting savings in RAM and execution time for a wide variety of programs.},
booktitle = {Proceedings of the 13th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, Tools and Theory for Embedded Systems},
pages = {1–10},
numpages = {10},
keywords = {JVM, call stack, wireless sensor networks, memory management, TakaTuka, Java Virtual Machine},
location = {Beijing, China},
series = {LCTES '12}
}

@inproceedings{10.5555/1765175.1765204,
author = {Hussmann, Heinrich and Demuth, Birgit and Finger, Frank},
title = {Modular Architecture for a Toolset Supporting OCL},
year = {2000},
isbn = {354041133X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The practical application of the Object Constraint Language, which is part of the UML specification since version 1.1, depends crucially on the existence of adequate tool support. This paper discusses general design issues for OCL tools. It is argued that the nature of OCL will lead to a large variety of tools, applied in combination with a variety of different UML tools. Therefore, a flexible modular architecture for a UML/OCL toolset is proposed. The paper reports on the first results of an ongoing project which aims at the provision of such an OCL toolset for the public domain.},
booktitle = {Proceedings of the 3rd International Conference on The Unified Modeling Language: Advancing the Standard},
pages = {278–293},
numpages = {16},
location = {York, UK},
series = {UML'00}
}

@inproceedings{10.1145/3426422.3426980,
author = {Morton, John Magnus and Kaszyk, Kuba and Li, Lu and Sun, Jiawen and Dubach, Christophe and Steuwer, Michel and Cole, Murray and O'Boyle, Michael F. P.},
title = {DelayRepay: Delayed Execution for Kernel Fusion in Python},
year = {2020},
isbn = {9781450381758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426422.3426980},
doi = {10.1145/3426422.3426980},
abstract = {Python is a popular, dynamic language for data science and scientific computing. To ensure efficiency, significant numerical libraries are implemented in static native languages. However, performance suffers when switching between native and non-native code, especially if data has to be converted between native arrays and Python data structures. As GPU accelerators are increasingly used, this problem becomes particularly acute. Data and control has to be repeatedly transferred between the accelerator and the host. In this paper, we present DelayRepay, a delayed execution framework for numeric Python programs. It avoids excessive switching and data transfer by using lazy evaluation and kernel fusion. Using DelayRepay, operations on NumPy arrays are executed lazily, allowing multiple calls to accelerator kernels to be fused together dynamically. DelayRepay is available as a drop-in replacement for existing Python libraries. This approach enables significant performance improvement over the state-of-the-art and is invisible to the application programmer. We show that our approach provides a maximum 377\texttimes{} speedup over NumPy - a 409% increase over the state of the art.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {43–56},
numpages = {14},
keywords = {GPU, delayed evaluation, code fusion, dynamic compilation},
location = {Virtual, USA},
series = {DLS 2020}
}

@article{10.1145/1644001.1644004,
author = {Rouson, Damian W. I. and Adalsteinsson, Helgi and Xia, Jim},
title = {Design Patterns for Multiphysics Modeling in Fortran 2003 and C++},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/1644001.1644004},
doi = {10.1145/1644001.1644004},
abstract = {We present three new object-oriented software design patterns in Fortran 2003 and C++. These patterns integrate coupled differential equations, facilitating the flexible swapping of physical and numerical software abstractions at compile-time and runtime. The Semi-Discrete pattern supports the time advancement of a dynamical system encapsulated in a single abstract data type (ADT). The Puppeteer pattern combines ADTs into a multiphysics package, mediates interabstraction communications, and enables implicit marching even when nonlinear terms couple separate ADTs with private data. The Surrogate pattern emulates C++ forward references in Fortran 2003. After code demonstrations using the Lorenz equations, we provide architectural descriptions of our use of the new patterns in extending the Rouson et al. [2008a] Navier-Stokes solver to simulate multiphysics phenomena. We also describe the relationships between the new patterns and two previously developed architectural elements: the Strategy pattern of Gamma et al. [1995] and the template emulation technique of Akin [2003]. This report demonstrates how these patterns manage complexity by providing logical separation between individual physics models and the control logic that bridges between them. Additionally, it shows how language features such as operator overloading and automated memory management enable a clear mathematical notation for model bridging and system evolution.},
journal = {ACM Trans. Math. Softw.},
month = {jan},
articleno = {3},
numpages = {30},
keywords = {Lorenz equations, Design patterns, multiphysics modeling}
}

@inproceedings{10.1145/3173162.3177159,
author = {Wen, Shasha and Liu, Xu and Byrne, John and Chabbi, Milind},
title = {Watching for Software Inefficiencies with Witch},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3177159},
doi = {10.1145/3173162.3177159},
abstract = {Inefficiencies abound in complex, layered software. A variety of inefficiencies show up as wasteful memory operations. Many existing tools instrument every load and store instruction to monitor memory, which significantly slows execution and consumes enormously extra memory. Our lightweight framework, Witch, samples consecutive accesses to the same memory location by exploiting two ubiquitous hardware features: the performance monitoring units (PMU) and debug registers. Witch performs no instrumentation. Hence, witchcraft---tools built atop Witch---can detect a variety of software inefficiencies while introducing negligible slowdown and insignificant memory consumption and yet maintaining accuracy comparable to exhaustive instrumentation tools. Witch allowed us to scale our analysis to a large number of code bases. Guided by witchcraft, we detected several performance problems in important code bases; eliminating these inefficiencies resulted in significant speedups.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {332–347},
numpages = {16},
keywords = {debug registers, software inefficiency detection, profiling, PMU, sampling},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@article{10.1145/3506713,
author = {Bobda, Christophe and Mbongue, Joel Mandebi and Chow, Paul and Ewais, Mohammad and Tarafdar, Naif and Vega, Juan Camilo and Eguro, Ken and Koch, Dirk and Handagala, Suranga and Leeser, Miriam and Herbordt, Martin and Shahzad, Hafsah and Hofste, Peter and Ringlein, Burkhard and Szefer, Jakub and Sanaullah, Ahmed and Tessier, Russell},
title = {The Future of FPGA Acceleration in Datacenters and the Cloud},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7406},
url = {https://doi.org/10.1145/3506713},
doi = {10.1145/3506713},
abstract = {In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {feb},
articleno = {34},
numpages = {42},
keywords = {virtualization, FPGA, security, datacenter, Cloud}
}

@inproceedings{10.1145/3275219.3275226,
author = {Yan, Jiafei and Sun, Hailong and Wang, Xu and Liu, Xudong and Song, Xiaotao},
title = {Profiling Developer Expertise across Software Communities with Heterogeneous Information Network Analysis},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275226},
doi = {10.1145/3275219.3275226},
abstract = {Knowing developer expertise is critical for achieving effective task allocation. However, it is of great challenge to accurately profile the expertise of developers over the Internet as their activities often disperse across different online communities. In this regard, the existing works either merely concern a single community, or simply sum up the expertise in individual communities. The former suffers from low accuracy due to incomplete data, while the latter impractically assumes that developer expertise is completely independent and irrelavant across communities. To overcome those limitations, we propose a new approach to profile developer expertise across software communities through heterogeneous information network (HIN) analysis. A HIN is first built by analyzing the developer activities in various communities, where nodes represent objects like developers and skills, and edges represent the relations among objects. Second, as random walk with restart (RWR) is known for its ability to capture the global structure of the whole network, we adopt RWR over the HIN to estimate the proximity of developer nodes and skill nodes, which essentially reflects developer expertise. Based on the data of 72,645 common users of GitHub and Stack Overflow, we conducted an empirical study and evaluated developer expertise using proposed approach. To evaluate the effect of our approach, we use the obtained expertise to estimate the competency of developers in answering the questions posted in Stack Overflow. The experimental results demonstrate the superiority of our approach over existing methods.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {2},
numpages = {9},
keywords = {random walk with restart, heterogeneous information network, Developer expertise},
location = {Beijing, China},
series = {Internetware '18}
}

@article{10.5555/3205191.3205205,
author = {Zhang, Cheng and Feng, Weiqi and Steffens, Emma and de Landaluce, Alvaro and Kleinman, Scott and LeBlanc, Mark D.},
title = {Lexos 2017: Building Reliable Software in Python},
year = {2018},
issue_date = {June 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {6},
issn = {1937-4771},
abstract = {Refactoring software is challenging, but necessary to ensure software correctness and extensibility. We present a plan that blends automated tools and human reviews when refactoring the back-end of a web-based application. The Lexos software, developed by the NEH-funded Lexomics Project, provides a simple, web-based workflow for text processing, statistical analysis, and visualization of results when exploring digitized texts. The development of Lexos spans six years and includes over fifty undergraduate developers, many who assumed leadership roles in architectural design and systems engineering over three software releases. This paper shares our current refactoring effort on the Python backend to produce Lexos v3.2, an effort that includes a transition from Python v2.7 to Python v3.6. Good software engineering practices guide the effort, including the use of type hinting, a Model-View-Control pattern, PEP 8 code and PEP 257 documentation styles, unit testing, and continuous integration.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {124–134},
numpages = {11}
}

@inproceedings{10.1145/3368089.3409680,
author = {Erlenhov, Linda and Neto, Francisco Gomes de Oliveira and Leitner, Philipp},
title = {An Empirical Study of Bots in Software Development: Characteristics and Challenges from a Practitioner’s Perspective},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409680},
doi = {10.1145/3368089.3409680},
abstract = {Software engineering bots – automated tools that handle tedious tasks – are increasingly used by industrial and open source projects to improve developer productivity. Current research in this area is held back by a lack of consensus of what software engineering bots (DevBots) actually are, what characteristics distinguish them from other tools, and what benefits and challenges are associated with DevBot usage. In this paper we report on a mixed-method empirical study of DevBot usage in industrial practice. We report on findings from interviewing 21 and surveying a total of 111 developers. We identify three different personas among DevBot users (focusing on autonomy, chat interfaces, and “smartness”), each with different definitions of what a DevBot is, why developers use them, and what they struggle with.We conclude that future DevBot research should situate their work within our framework, to clearly identify what type of bot the work targets, and what advantages practitioners can expect. Further, we find that there currently is a lack of general purpose “smart” bots that go beyond simple automation tools or chat interfaces. This is problematic, as we have seen that such bots, if available, can have a transformative effect on the projects that use them.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {445–455},
numpages = {11},
keywords = {Empirical study, Software bot, Software engineering},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3579522,
author = {Razi, Afsaneh and Alsoubai, Ashwaq and Kim, Seunghyun and Ali, Shiza and Stringhini, Gianluca and De Choudhury, Munmun and Wisniewski, Pamela J.},
title = {Sliding into My DMs: Detecting Uncomfortable or Unsafe Sexual Risk Experiences within Instagram Direct Messages Grounded in the Perspective of Youth},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579522},
doi = {10.1145/3579522},
abstract = {We collected Instagram data from 150 adolescents (ages 13-21) that included 15,547 private message conversations of which 326 conversations were flagged as sexually risky by participants. Based on this data, we leveraged a human-centered machine learning approach to create sexual risk detection classifiers for youth social media conversations. Our Convolutional Neural Network (CNN) and Random Forest models outperformed in identifying sexual risks at the conversation-level (AUC=0.88), and CNN outperformed at the message-level (AUC=0.85). We also trained classifiers to detect the severity risk level (i.e., safe, low, medium-high) of a given message with CNN outperforming other models (AUC=0.88). A feature analysis yielded deeper insights into patterns found within sexually safe versus unsafe conversations. We found that contextual features (e.g., age, gender, and relationship type) and Linguistic Inquiry and Word Count (LIWC) contributed the most for accurately detecting sexual conversations that made youth feel uncomfortable or unsafe. Our analysis provides insights into the important factors and contextual features that enhance automated detection of sexual risks within youths' private conversations. As such, we make valuable contributions to the computational risk detection and adolescent online safety literature through our human-centered approach of collecting and ground truth coding private social media conversations of youth for the purpose of risk classification.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {89},
numpages = {29},
keywords = {machine learning, sexual risk detection, deep learning, youth online risks, adolescents online safety}
}

@article{10.1145/1331287.1331293,
author = {Garrett, Matthew},
title = {Powering Down: Smart Power Management is All about Doing More with the Resources We Have.},
year = {2007},
issue_date = {November/December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {7},
issn = {1542-7730},
url = {https://doi.org/10.1145/1331287.1331293},
doi = {10.1145/1331287.1331293},
abstract = {Power management is a topic of interest to everyone. In the beginning there was the desktop computer. It ran at a fixed speed and consumed less power than the monitor it was plugged into. Where computers were portable, their sheer size and weight meant that you were more likely to be limited by physical strength than battery life. It was not a great time for power management. Now consider the present. Laptops have increased in speed by more than 5,000 times. Battery capacity, sadly, has not. With hardware becoming increasingly mobile, however, users are demanding that battery life start matching the way they work. People want to work from cafes. Long-haul flights are now perceived as the ideal opportunity to finish a presentation. Two hours of battery life just isn’t going to cut it; users are looking for upwards of eight hours. What’s drawing that power, and more importantly, how can we manage it better?},
journal = {Queue},
month = {nov},
pages = {16–21},
numpages = {6}
}

@inproceedings{10.1109/MSR.2019.00027,
author = {Zhai, Hongyu and Casalnuovo, Casey and Devanbu, Prem},
title = {Test Coverage in Python Programs},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00027},
doi = {10.1109/MSR.2019.00027},
abstract = {We study code coverage in several popular Python projects: flask, matplotlib, pandas, scikit-learn, and scrapy. Coverage data on these projects is gathered and hosted on the Codecov website, from where this data can be mined. Using this data, and a syntactic parse of the code, we examine the effect of control flow structure, statement type (e.g., if, for) and code age on test coverage. We find that coverage depends on control flow structure, with more deeply nested statements being significantly less likely to be covered. This is a clear effect, which holds up in every project, even when controlling for the age of the line (as determined by git blame). We find that the age of a line per se has a small (but statistically significant) positive effect on coverage. Finally, we find that the kind of statement (try, if, except, raise, etc) has varying effects on coverage, with exception-handling statements being covered much less often. These results suggest that developers in Python projects have difficulty writing test sets that cover deeply-nested and error-handling statements, and might need assistance covering such code.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {116–120},
numpages = {5},
keywords = {mining, code age, test coverage, python},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1145/1067699.1067706,
author = {Papargyris, Anthony and Poulymenakou, Angeliki},
title = {Learning to Fly in Persistent Digital Worlds: The Case of Massively Multiplayer Online Role Playing Games},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {2372-7403},
url = {https://doi.org/10.1145/1067699.1067706},
doi = {10.1145/1067699.1067706},
abstract = {This paper discusses virtual communities' formation and skill development through the process of participation in collective practices in on-line role-playing game environments. The study that follows is mainly based on the participant observation of players acting in two popular Massively Multiplayer Online Role Playing Games. Data have been collected through participation and observation of players and their unfolding practices in forming and maintaining such communities. The objective was to document these practices and to study virtual game environments in order to understand learning opportunities within and among these communities, and to identify design elements of the game that cultivate community building. The primary unit of analysis is the in-game communities known as 'guilds'. Research findings indicate that in such multicultural and anonymous environments, many learning processes are evolving that affect both players' understanding of the game's state, and their social interaction and communication skills.},
journal = {SIGGROUP Bull.},
month = {jan},
pages = {41–49},
numpages = {9},
keywords = {virtual communities, communities of practice, massively multiplayer online role-playing games, legitimate peripheral participation}
}

@inproceedings{10.1145/3293882.3330572,
author = {Kong, Pingfan and Li, Li and Gao, Jun and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
title = {Mining Android Crash Fixes in the Absence of Issue- and Change-Tracking Systems},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330572},
doi = {10.1145/3293882.3330572},
abstract = {Android apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {78–89},
numpages = {12},
keywords = {debugging, crash, testing, mining software repository, Android},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3567955.3567962,
author = {Gouicem, Redha and Sprokholt, Dennis and Ruehl, Jasper and Rocha, Rodrigo C. O. and Spink, Tom and Chakraborty, Soham and Bhatotia, Pramod},
title = {Risotto: A Dynamic Binary Translator for Weak Memory Model Architectures},
year = {2022},
isbn = {9781450399159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567955.3567962},
doi = {10.1145/3567955.3567962},
abstract = {Dynamic Binary Translation (DBT) is a powerful approach to support cross-architecture emulation of unmodified binaries. However, DBT systems face correctness and performance challenges, when emulating concurrent binaries from strong to weak memory consistency architectures. As a matter of fact, we report several translation errors in QEMU, when emulating x86 binaries on Arm hosts. To address these challenges, we propose an end-to-end approach that provides correct and efficient emulation for weak memory model architectures. Our contributions are twofold: we formalize QEMU’s intermediate representation’s memory model, and use it to propose formally verified mapping schemes to bridge the strong-on-weak memory consistency mismatch. Secondly, we implement these verified mappings in Risotto, a QEMU-based DBT system that optimizes memory fence placement while ensuring correctness. Risotto further enhances the emulation performance via cross-architecture dynamic linking of native shared libraries, and fast and correct translation of compare-and-swap operations. We evaluate Risotto using multi-threaded benchmark suites and real-world applications, and show that Risotto improves the emulation performance by 6.7% on average over ”erroneous” QEMU, while ensuring correctness.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {107–122},
numpages = {16},
keywords = {Binary translation, formal verification, memory models},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3519939.3523700,
author = {Fehr, Mathieu and Niu, Jeff and Riddle, River and Amini, Mehdi and Su, Zhendong and Grosser, Tobias},
title = {IRDL: An IR Definition Language for SSA Compilers},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523700},
doi = {10.1145/3519939.3523700},
abstract = {Designing compiler intermediate representations (IRs) is often a manual process that makes exploration and innovation in this space costly. Developers typically use general-purpose programming languages to design IRs. As a result, IR implementations are verbose, manual modifications are expensive, and designing tooling for the inspection or generation of IRs is impractical. While compilers relied historically on a few slowly evolving IRs, domain-specific optimizations and specialized hardware motivate compilers to use and evolve many IRs. We facilitate the implementation of SSA-based IRs by introducing IRDL, a domain-specific language to define IRs. We analyze all 28 domain-specific IRs developed as part of LLVM's MLIR project over the last two years and demonstrate how to express these IRs exclusively in IRDL while only rarely falling back to IRDL's support for generic C++ extensions. By enabling the concise and explicit specification of IRs, we provide foundations for developing effective tooling to automate the compiler construction process.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {199–212},
numpages = {14},
keywords = {MLIR, Compilers, Intermediate Representation},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@proceedings{10.5555/3623290,
title = {ICSE-SEIS '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
abstract = {We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3545258,
title = {Internetware '22: Proceedings of the 13th Asia-Pacific Symposium on Internetware},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hohhot, China}
}

@inproceedings{10.1145/371920.372091,
author = {Beck, Micah and Moore, Terry and Abrahamsson, Leif and Achouiantz, Christophe and Johansson, Patrick},
title = {Enabling Full Service Surrogates Using the Portable Channel Representation},
year = {2001},
isbn = {1581133480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/371920.372091},
doi = {10.1145/371920.372091},
booktitle = {Proceedings of the 10th International Conference on World Wide Web},
pages = {376–385},
numpages = {10},
keywords = {mirroring, dynamic content, web server, surrogate, content distribution, replication, portability},
location = {Hong Kong, Hong Kong},
series = {WWW '01}
}

@inproceedings{10.1145/3075564.3077628,
author = {Bagnato, Alessandra and B\'{\i}r\'{o}, Regina Krisztina and Bonino, Dario and Pastrone, Claudio and Elmenreich, Wilfried and Reiners, Ren\'{e} and Schranz, Melanie and Arnautovic, Edin},
title = {Designing Swarms of Cyber-Physical Systems: The H2020 CPSwarm Project: Invited Paper},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3077628},
doi = {10.1145/3075564.3077628},
abstract = {Cyber-Physical Systems (CPS) find applications in a number of large-scale, safety-critical domains e.g. transportation, smart cities, etc. As a matter of fact, the increasing interactions amongst different CPS are starting to generate unpredictable behaviors and emerging properties, often leading to unforeseen and/or undesired results. Rather than being an unwanted byproduct, these interactions could, however, become an advantage if they were explicitly managed, and accounted, since the early design stages. The CPSwarm project, presented in this paper, aims at tackling these kinds of challenges by easing development and integration of complex herds of heterogeneous CPS. Thanks to CPSwarm, systems designed through a combination of existing and emerging tools, will collaborate on the basis of local policies and exhibit a collective behavior capable of solving complex, real-world, problems. Three real-world use cases will demonstrate the validity of foundational assumptions of the presented approach as well as the viability of the developed tools and methodologies.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {305–312},
numpages = {8},
keywords = {model based design, emergent behaviour, cyber-physical systems, CPS integration, evolutionary algorithms, deployment tools, swarm computing, development tools, cyber-physical system of systems},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{10.1145/2470654.2466124,
author = {Gross, Shad and Pace, Tyler and Bardzell, Jeffrey and Bardzell, Shaowen},
title = {Machinima Production Tools: A Vernacular History of a Creative Medium},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2466124},
doi = {10.1145/2470654.2466124},
abstract = {In recent years, HCI has shown a rising interest in the creative practices associated with massive online communities, including crafters, hackers, DIY, and other expert amateurs. One strategy for researching creativity at this scale is through an analysis of a community's outputs, including its creative works, custom created tools, and emergent practices. In this paper, we offer one such case study, a historical account of World of Warcraft (WoW) machinima (i.e., videos produced inside of video games), which shows how the aesthetic needs and requirements of video making community coevolved with the community-made creativity support tools in use at the time. We view this process as inhabiting different layers and practices of appropriation, and through an analysis of them, we trace the ways that support for emerging stylistic conventions become built into creativity support tools over time.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {971–980},
numpages = {10},
keywords = {medium, machinima, hci, creativity},
location = {Paris, France},
series = {CHI '13}
}

@article{10.1145/3517193,
author = {Dong, Yiwen and Li, Zheyang and Tian, Yongqiang and Sun, Chengnian and Godfrey, Michael W. and Nagappan, Meiyappan},
title = {Bash in the Wild: Language Usage, Code Smells, and Bugs},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3517193},
doi = {10.1145/3517193},
abstract = {The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. In this article, we perform a large-scale empirical study of Bash usage, based on analyses over one million open source Bash scripts found in Github repositories. We identify and discuss which features and utilities of Bash are most often used. Using static analysis, we find that Bash scripts are often error-prone, and the error-proneness has a moderately positive correlation with the size of the scripts. We also find that the most common problem areas concern quoting, resource management, command options, permissions, and error handling. We envision that these findings can be beneficial for learning Bash and future research that aims to improve shell and command-line productivity and reliability.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {8},
numpages = {22},
keywords = {bash, bugs, shell scripts, code smells, Empirical studies, language features}
}

@inproceedings{10.1145/3460120.3485369,
author = {Li, Kai and Wang, Yibo and Tang, Yuzhe},
title = {DETER: Denial of Ethereum Txpool SERvices},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485369},
doi = {10.1145/3460120.3485369},
abstract = {On an Ethereum node, txpool (a.k.a. mempool) is a buffer storing unconfirmed transactions and controls what downstream services can see, such as mining and transaction propagation. This work presents the first security study on Ethereum txpool designs.We discover flawed transaction handling in all known Ethereum clients (e.g., Geth), and by exploiting it, design a series of low-cost denial-of-service attacks named DETER. A DETER attacker can disable a remote Ethereum node's txpool and deny the critical downstream services in mining, transaction propagation, Gas station, etc. By design, DETER attacks incur zero or low Ether cost. The attack can be amplified to cause global disruption to an Ethereum network by targeting centralized network services there (e.g., mining pools and transaction relay services). By evaluating local nodes, we verify the effectiveness and low cost of DETER attacks on all known Ethereum clients and in major testnets. We design non-trivial measurement methods against blackbox mainnet nodes and conduct light probes to confirm that popular mainnet services are exploitable under DETER attacks.We propose mitigation schemes that reduce a DETER attack's success rate down to zero while preserving the miners' revenue.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1645–1667},
numpages = {23},
keywords = {mempool/txpool, ethereum, design flaws, blockchains, unconfirmed transactions},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3426745.3431337,
author = {Kourtellis, Nicolas and Katevas, Kleomenis and Perino, Diego},
title = {FLaaS: Federated Learning as a Service},
year = {2020},
isbn = {9781450381826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426745.3431337},
doi = {10.1145/3426745.3431337},
abstract = {Federated Learning (FL) is emerging as a promising technology to build machine learning models in a decentralized, privacy-preserving fashion. Indeed, FL enables local training on user devices, avoiding user data to be transferred to centralized servers, and can be enhanced with differential privacy mechanisms. Although FL has been recently deployed in real systems, the possibility of collaborative modeling across different 3rd-party applications has not yet been explored. In this paper, we tackle this problem and present Federated Learning as a Service (FLaaS), a system enabling different scenarios of 3rd-party application collaborative model building and addressing the consequent challenges of permission and privacy management, usability, and hierarchical model training. FLaaS can be deployed in different operational environments. As a proof of concept, we implement it on a mobile phone setting and discuss practical implications of results on simulated and real devices with respect to on-device training CPU cost, memory footprint and power consumed per FL model round. Therefore, we demonstrate FLaaS's feasibility in building unique or joint FL models across applications for image object detection in a few hours, across 100 devices.},
booktitle = {Proceedings of the 1st Workshop on Distributed Machine Learning},
pages = {7–13},
numpages = {7},
location = {Barcelona, Spain},
series = {DistributedML'20}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@article{10.1145/1952746.1964843,
author = {McKenzie, Patrick},
title = {Weapons of Mass Assignment: A Ruby on Rails App Highlights Some Serious, yet Easily Avoided, Security Vulnerabilities.},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/1952746.1964843},
doi = {10.1145/1952746.1964843},
abstract = {In May 2010, during a news cycle dominated by users’ widespread disgust with Facebook privacy policies, a team of four students from New York University published a request for $10,000 in donations to build a privacy-aware Facebook alternative. The software, Diaspora, would allow users to host their own social networks and own their own data. The team promised to open-source all the code they wrote, guaranteeing the privacy and security of users’ data by exposing the code to public scrutiny. With the help of front-page coverage from the New York Times, the team ended up raising more than $200,000. They anticipated launching the service to end users in October 2010.},
journal = {Queue},
month = {mar},
pages = {40–48},
numpages = {9}
}

@inproceedings{10.1145/2481268.2481273,
author = {Kumar, Vineet and Hendren, Laurie},
title = {First Steps to Compiling Matlab to X10},
year = {2013},
isbn = {9781450321570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2481268.2481273},
doi = {10.1145/2481268.2481273},
abstract = {Matlab is a popular dynamic array-based language commonly used by students, scientists and engineers, who appreciate the interactive development style, the rich set of array operators, the extensive builtin library, and the fact that they do not have to declare static types. Even though these users like to program in Matlab, their computations are often very compute-intensive and are potentially very good applications for high-performance languages such as X10.To provide a bridge between Matlab and X10, we are developing MiX10, a source-to-source compiler that translates Matlab to X10. This paper provides an overview of the initial design of the MiX10 compiler, presents a template-based specialization approach to compiling the builtin Matlab operators, and provides translation rules for the key sequential Matlab constructs with a focus on those which are challenging to convert to semantically-equivalent X10. An initial core compiler has been implemented, and preliminary results are provided.},
booktitle = {Proceedings of the Third ACM SIGPLAN X10 Workshop},
pages = {2–11},
numpages = {10},
keywords = {X10, Matlab, source-to-source compiler},
location = {Seattle, Washington},
series = {X10 '13}
}

@inproceedings{10.5555/1105634.1105655,
author = {Cordy, James R. and Kark, Anatol W.},
title = {Workshops of CASCON 2005/Ateliers de CASCON 20005},
year = {2005},
publisher = {IBM Press},
booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {283–291},
numpages = {9},
location = {Toranto, Ontario, Canada},
series = {CASCON '05}
}

@article{10.14778/3415478.3415560,
author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and van Hovell, Herman and Ionescu, Adrian and \L{}uszczak, Alicja and undefinedwitakowski, Micha\l{} and Szafra\'{n}ski, Micha\l{} and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei},
title = {Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415560},
doi = {10.14778/3415478.3415560},
abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3411–3424},
numpages = {14}
}

@inproceedings{10.1145/2814270.2814300,
author = {Lopes, Cristina V. and Ossher, Joel},
title = {How Scale Affects Structure in Java Programs},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814300},
doi = {10.1145/2814270.2814300},
abstract = {Many internal software metrics and external quality attributes of Java programs correlate strongly with program size. This knowledge has been used pervasively in quantitative studies of software through practices such as normalization on size metrics. This paper reports size-related super- and sublinear effects that have not been known before. Findings obtained on a very large collection of Java programs -- 30,911 projects hosted at Google Code as of Summer 2011 -- unveils how certain characteristics of programs vary disproportionately with program size, sometimes even non-monotonically. Many of the specific parameters of nonlinear relations are reported. This result gives further insights for the differences of ``programming in the small'' vs. ``programming in the large.'' The reported findings carry important consequences for OO software metrics, and software research in general: metrics that have been known to correlate with size can now be properly normalized so that all the information that is left in them is size-independent.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {675–694},
numpages = {20},
keywords = {Object Oriented Programs, Linear Regression Models, Metrics},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@inproceedings{10.1145/3597926.3598056,
author = {Liu, Han and Chen, Sen and Feng, Ruitao and Liu, Chengwei and Li, Kaixuan and Xu, Zhengzi and Nie, Liming and Liu, Yang and Chen, Yixiang},
title = {A Comprehensive Study on Quality Assurance Tools for Java},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598056},
doi = {10.1145/3597926.3598056},
abstract = {Quality assurance (QA) tools are receiving more and more attention and are widely used by developers. Given the wide range of solutions for QA technology, it is still a question of evaluating QA tools. Most existing research is limited in the following ways: (i) They compare tools without considering scanning rules analysis. (ii) They disagree on the effectiveness of tools due to the study methodology and benchmark dataset. (iii) They do not separately analyze the role of the warnings. (iv) There is no large-scale study on the analysis of time performance. To address these problems, in the paper, we systematically select 6 free or open-source tools for a comprehensive study from a list of 148 existing Java QA tools. To carry out a comprehensive study and evaluate tools in multi-level dimensions, we first mapped the scanning rules to the CWE and analyze the coverage and granularity of the scanning rules. Then we conducted an experiment on 5 benchmarks, including 1,425 bugs, to investigate the effectiveness of these tools. Furthermore, we took substantial effort to investigate the effectiveness of warnings by comparing the real labeled bugs with the warnings and investigating their role in bug detection. Finally, we assessed these tools’ time performance on 1,049 projects. The useful findings based on our comprehensive study can help developers improve their tools and provide users with suggestions for selecting QA tools.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {285–297},
numpages = {13},
keywords = {Scanning rules, Bug finding, CWE, Quality assurance tools},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3411504.3421212,
author = {Saarinen, Markku-Juhani O. and Newell, G. Richard and Marshall, Ben},
title = {Building a Modern TRNG: An Entropy Source Interface for RISC-V},
year = {2020},
isbn = {9781450380904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411504.3421212},
doi = {10.1145/3411504.3421212},
abstract = {The currently proposed RISC-V True Random Number Generator (TRNG) architecture breaks with previous ISA TRNG practice by splitting the Entropy Source (ES) component away from cryptographic PRNGs into a separate interface, and in its use of polling. We describe the interface, its use in cryptography, and offer additional discussion, background, and rationale for various aspects of it. This design is informed by lessons learned from earlier mainstream ISAs, recently introduced SP 800-90B and FIPS 140-3 entropy audit requirements, AIS 31 and Common Criteria, current and emerging cryptographic needs such as post-quantum cryptography, and the goal of supporting a wide variety of RISC-V implementations and applications. Many of the architectural choices are a result of quantitative observations about random number generators in secure microcontrollers, the Linux kernel, and cryptographic libraries. We further compare the architecture to some contemporary random number generators and describe a minimalistic TRNG reference implementation that uses the Entropy Source together with RISC-V AES instructions.},
booktitle = {Proceedings of the 4th ACM Workshop on Attacks and Solutions in Hardware Security},
pages = {93–102},
numpages = {10},
keywords = {sp 800-90b, random, entropy source, risc-v, trng, fips 140-3},
location = {Virtual Event, USA},
series = {ASHES'20}
}

@inproceedings{10.1145/1982185.1982308,
author = {Nakajima, Jun and Lin, Qian and Yang, Sheng and Zhu, Min and Gao, Shang and Xia, Mingyuan and Yu, Peijie and Dong, Yaozu and Qi, Zhengwei and Chen, Kai and Guan, Haibing},
title = {Optimizing Virtual Machines Using Hybrid Virtualization},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982308},
doi = {10.1145/1982185.1982308},
abstract = {It is crucial to minimize virtualization overhead for virtual machine deployment. The conventional \texttimes{}86 CPU is incapable of classical trap-and-emulate virtualization, leading that paravirtualization was the optimal virtualization strategy formerly. Since architectural extensions are introduced to support classical virtualization, hardware assisted virtualization becomes a competitive alternative method. Hardware assisted virtualization is superior in CPU and memory virtualization, yet paravirtualization is still valuable in some aspects as it is capable of shortening the disposal path of I/O virtualization. Thus we propose the hybrid virtualization which runs the paravirtualized guest in the hardware assisted virtual machine container to take advantage of both. Experiment results indicate that our hybrid solution outweighs origin paravirtualization by nearly 30% in memory intensive test and 50% in microbenchmarks. Meanwhile, compared with the origin hardware assisted virtual machine, hybrid guest owns over 16% improvement in I/O intensive workloads.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {573–578},
numpages = {6},
keywords = {paravirtualization, hybrid virtualization, hardware assisted virtualization},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3368640.3368655,
author = {Terzi, Sofia and Zacharaki, Angeliki and Nizamis, Alexandros and Votis, Konstantinos and Ioannidis, Dimosthenis and Tzovaras, Dimitrios and Stamelos, Ioannis},
title = {Transforming the Supply-Chain Management and Industry Logistics with Blockchain Smart Contracts},
year = {2019},
isbn = {9781450372923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368640.3368655},
doi = {10.1145/3368640.3368655},
abstract = {Blockchain technology back in 2009 was mainly used for finance use cases due to the cryptocurrency support of the Bitcoin and the Ethereum networks. Nowadays with the emergence of business oriented distributed ledger frameworks we can find blockchain (BC) in almost every aspect of our everyday real-life transactions. Starting with supply chain, BC has been used to support and secure education, e-government, real estate, insurance, healthcare and other business cases. Regarding blockchain in supply chain, examples as the Walmart and IBM partnership shows that smart contracts (SC) and cryptocurrencies can disrupt the way suppliers, shippers, retailers and customers trust each other and interact. This paper describes how we applied BC technology in two real-life supply chain scenarios. The first one is for logging and tracing products, showing how we can use SCs to identify the ingredients of food products and how to uniquely identify the food product throughout its shipment from the factory to the customer who purchase it. Important aspects of this route is the transparency of the process as well as the verification of transport. The second one deals with how authentication works for users holding BC identities. The identity of a user is important in order to secure the network, allowing only permissioned parties to access it and perform actions on the data. As long as the network is private and immutable, authenticating and authorizing users is crucial for enforcing the end-to-end security to increase users' trust and protect the confidentiality of data. While exploring these two scenarios we define the actors, the transactions these actors can perform on the systems and we propose the SCs that have to be implemented for these use cases to take advantage of BC's unique characteristics, as immutability and non-repudiation. In addition, we provide sample template SCs for both scenarios.},
booktitle = {Proceedings of the 23rd Pan-Hellenic Conference on Informatics},
pages = {9–14},
numpages = {6},
keywords = {logging, blockchain 3.0, traceability, smart contracts, decentralized identity, supply chain, Industry 4.0},
location = {Nicosia, Cyprus},
series = {PCI '19}
}

@article{10.1145/1941487.1941503,
author = {McKenzie, Patrick},
title = {Weapons of Mass Assignment},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/1941487.1941503},
doi = {10.1145/1941487.1941503},
abstract = {A Ruby on Rails app highlights some serious, yet easily avoided, security vulnerabilities.},
journal = {Commun. ACM},
month = {may},
pages = {54–59},
numpages = {6}
}

@inproceedings{10.1145/1247660.1247690,
author = {Cheng, Jerry and Wong, Starsky H.Y. and Yang, Hao and Lu, Songwu},
title = {SmartSiren: Virus Detection and Alert for Smartphones},
year = {2007},
isbn = {9781595936141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1247660.1247690},
doi = {10.1145/1247660.1247690},
abstract = {Smartphones have recently become increasingly popular because they provide "all-in-one" convenience by integrating traditional mobile phones with handheld computing devices. However, the flexibility of running third-party softwares also leaves the smartphones open to malicious viruses. In fact, hundreds of smartphone viruses have emerged in the past two years, which can quickly spread through various means such as SMS/MMS, Bluetooth and traditional IP-based applications. Our own implementations of two proof-of-concept viruses on Windows Mobile have confirmed the vulnerability of this popular smartphone platform.In this paper, we present SmartSiren, a collaborative virusdetection and alert system for smartphones. In order to detect viruses, SmartSiren collects the communication activity information from the smartphones, and performs joint analysis to detect both single-device and system-wide abnormal behaviors. We use a proxy-based architecture to offload the processing burden from resource-constrained smartphones and simplify the collaboration among smartphones. When a potential virus is detected, the proxy quarantines the out-break by sending targeted alerts to those immediately threatened smartphones. We have demonstrated the feasibility of SmartSiren through implementations on a Dopod 577w smartphone, and evaluated its effectiveness using simulations driven by 3-week SMS traces from a national cellular carrier. Our results show that SmartSiren can effectively prevent wide-area virus outbreaks with affordable overhead.},
booktitle = {Proceedings of the 5th International Conference on Mobile Systems, Applications and Services},
pages = {258–271},
numpages = {14},
keywords = {virus detection, security, alert, smartphone, privacy},
location = {San Juan, Puerto Rico},
series = {MobiSys '07}
}

@inproceedings{10.1145/2016551.2016563,
author = {Weckemann, Kay and Lim, Hyung-Taek and Herrscher, Daniel},
title = {Practical Experiences on a Communication Middleware for IP-Based in-Car Networks},
year = {2011},
isbn = {9781450305600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016551.2016563},
doi = {10.1145/2016551.2016563},
abstract = {Current in-car communication networks are based on automotive specific technologies like CAN, FlexRay and MOST and use proprietary communication protocols. While CAN and FlexRay come with a signal-based communication paradigm, MOST provides a more sophisticated interface based on "function blocks" to the application programmer. In the next years, we expect IP-based protocols and standard technologies like Ethernet to be deployed for more and more in-car communication tasks. As a result, we need an IP-based communication middleware that provides both signaland function-based interaction paradigms and works for all distributed applications in vehicles. Main challenges are the large variety of embedded devices and operating systems used in a single car in terms of footprint and compute power. Furthermore, it must be possible to migrate existing interface definitions from legacy technologies to the new IP-based solution. In this paper, we propose an IP-based in-car middleware framework based on an open source solution, Apache Etch. We sketch how different, yet interoperable versions of the middleware can be used to construct a scalable system that fits to both small and large devices. Finally, we identify extensions to Etch that are necessary to qualify the solution for the use in the automotive domain.},
booktitle = {Proceedings of the 5th International Conference on Communication System Software and Middleware},
articleno = {12},
numpages = {7},
keywords = {IP-based in-car networks, embedded middleware, industrial adaption of middleware},
location = {Verona, Italy},
series = {COMSWARE '11}
}

@article{10.1145/1952383.1952384,
author = {Wobbrock, Jacob O. and Kane, Shaun K. and Gajos, Krzysztof Z. and Harada, Susumu and Froehlich, Jon},
title = {Ability-Based Design: Concept, Principles and Examples},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/1952383.1952384},
doi = {10.1145/1952383.1952384},
abstract = {Current approaches to accessible computing share a common goal of making technology accessible to users with disabilities. Perhaps because of this goal, they may also share a tendency to centralize disability rather than ability. We present a refinement to these approaches called ability-based design that consists of focusing on ability throughout the design process in an effort to create systems that leverage the full range of human potential. Just as user-centered design shifted the focus of interactive system design from systems to users, ability-based design attempts to shift the focus of accessible design from disability to ability. Although prior approaches to accessible computing may consider users’ abilities to some extent, ability-based design makes ability its central focus. We offer seven ability-based design principles and describe the projects that inspired their formulation. We also present a research agenda for ability-based design.},
journal = {ACM Trans. Access. Comput.},
month = {apr},
articleno = {9},
numpages = {27},
keywords = {design for all, assistive technology, Ability-based design, universal usability, user interfaces for all, inclusive design, computer access, universal design, adaptive user interfaces}
}

@article{10.1145/2622630,
author = {Bland, Mike},
title = {Finding More than One Worm in the Apple},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2622630},
doi = {10.1145/2622630},
abstract = {If you see something, say something.},
journal = {Commun. ACM},
month = {jul},
pages = {58–64},
numpages = {7}
}

@inproceedings{10.1145/2683467.2683478,
author = {Edwards, Benjamin and Locasto, Michael and Epstein, Jeremy},
title = {Panel Summary: The Future of Software Regulation},
year = {2014},
isbn = {9781450330626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683467.2683478},
doi = {10.1145/2683467.2683478},
abstract = {A panel at the New Security Paradigms Workshop (2014) discussed the topic of regulation and licensing of software developers and information security professionals. This included topics of the current state of certification, future possibilities, and challenges associated with new forms of regulation. This paper presents a brief background on the subject, three opinions presented by the panelists, and finally a summary of the discussion which occurred at the workshop, including input from both the panelists and the workshop attendees.},
booktitle = {Proceedings of the 2014 New Security Paradigms Workshop},
pages = {117–126},
numpages = {10},
keywords = {offensive security, public policy, credit score, ethics, licensing, regulation, software development},
location = {Victoria, British Columbia, Canada},
series = {NSPW '14}
}

@inproceedings{10.1145/3477086.3480835,
author = {Lunar, Mohammad M. R. and Sun, Jianxin and Wensowitch, John and Fay, Michael and Tulay, Halit Bugra and Karanam, Venkat Sai Suman Lamba and Qiu, Brian and Nadig, Deepak and Attebury, Garhan and Yu, Hongfeng and Camp, Joseph and Koksal, Can Emre and Pompili, Dario and Ramamurthy, Byrav and Hashemi, Morteza and Ekici, Eylem and Vuran, Mehmet C.},
title = {OneLNK: One Link to Rule Them All: Web-Based Wireless Experimentation for Multi-Vendor Remotely Accessible Indoor/Outdoor Testbeds},
year = {2021},
isbn = {9781450387033},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477086.3480835},
doi = {10.1145/3477086.3480835},
abstract = {As evolving wireless network architectures become more diverse, complex, and interdependent, and equipment costs prohibit broad access to such networks, remotely accessible experimental testbeds are gaining interest in recent years in wireless communication and networking research. This interest has exacerbated in 2020 and became a vital need during the current global pandemic. However, providing end-users of various educational backgrounds access to radio devices from a heterogeneous set of vendors is challenging. This paper introduces OneLNK, a remotely accessible testbed consisting of radio devices from three different vendors and developed using open source cloud-native technologies. End-users can access the functionalities of OneLNK from a single webpage without any local installations. Using the web URL, users can operate radio devices, set experiment parameters, observe results in real-time, and save generated experiment data for all radio devices. The interactive web UI and its working mechanism for supporting radio equipment are covered with specific experiment capabilities. A diverse set of radio equipment (mmWave, sub-GHz SDR, and sub-6GHz SDR) are facilitated to explain these capabilities. Moreover, measurements of path loss, Received Signal Strength (RSS), and Signal-to-Noise Ratio (SNR) using devices from three different vendors operating on a vast spectrum (568 MHz, 5.8 GHz, and 60 GHz) are reported. The majority of the remotely accessible OneLNK platform was developed remotely during the pandemic by a team of experts from five U.S. states.},
booktitle = {Proceedings of the 15th ACM Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; CHaracterization},
pages = {85–92},
numpages = {8},
keywords = {Web Technologies, VM, Remote Wireless, USRP, IRIS, Testbed},
location = {New Orleans, LA, USA},
series = {WiNTECH '21}
}

@inproceedings{10.1145/986655.986661,
author = {DuVarney, Daniel C. and Venkatakrishnan, V. N. and Bhatkar, Sandeep},
title = {SELF: A Transparent Security Extension for ELF Binaries},
year = {2003},
isbn = {1581138806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/986655.986661},
doi = {10.1145/986655.986661},
abstract = {The ability to analyze and modify binaries is often very useful from a security viewpoint. Security operations one would like to perform on binaries include the ability to extract models of program behavior and insert inline reference monitors. Unfortunately, the existing manner in which binary code is packaged prevents even the simplest of analyses, such as distinguishing code from data, from succeeding 100 percent of the time. In this paper, we propose SELF, a security-enhanced ELF (Executable and Linking Format), which is simply ELF with an extra section added. The extra section contains information about (among other things) the address, size, and alignment requirements of each code and static data item in the program. This information is somewhat similar to traditional debugging information, but contains additional information specifically needed for binary analysis that debugging information lacks. It is also smaller, compatible with optimization, and less likely to facilitate reverse engineering, which we believe makes it practical for use with commercial software products. SELF approach has three key benefits. First, the information for the extra section is easy for compilers to provide, so little work is required on behalf of compiler vendors. Second, the extra section is ignored by default, so SELF binaries will run perfectly on all systems, including ones not interested in leveraging the extra information. Third, the extra section provides sufficient information to perform many security-related operations on the binary code. We believe SELF to be a practical approach, allowing many security analyses to be performed while not requiring major changes to the existing compiler infrastructure. An application example of the utility of SELF to perform address obfuscation (in which the addresses of all code and data items are randomized to defeat memory-error exploits) is presented.},
booktitle = {Proceedings of the 2003 Workshop on New Security Paradigms},
pages = {29–38},
numpages = {10},
location = {Ascona, Switzerland},
series = {NSPW '03}
}

@inproceedings{10.1145/3624062.3624178,
author = {Herten, Andreas},
title = {Many Cores, Many Models: GPU Programming Model vs. Vendor Compatibility Overview},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624178},
doi = {10.1145/3624062.3624178},
abstract = {In recent history, GPUs became a key driver of compute performance in HPC. With the installation of the Frontier supercomputer, they became the enablers of the Exascale era; further largest-scale installations are in progress (Aurora, El Capitan, JUPITER). But the early-day dominance by NVIDIA and their CUDA programming model has changed: The current HPC GPU landscape features three vendors (AMD, Intel, NVIDIA), each with native and derived programming models. The choices are ample, but not all models are supported on all platforms, especially if support for Fortran is needed; in addition, some restrictions might apply. It is hard for scientific programmers to navigate this abundance of choices and limits. This paper gives a guide by matching the GPU platforms with supported programming models, presented in a concise table and further elaborated in detailed comments. An assessment is made regarding the level of support of a model on a platform.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1019–1026},
numpages = {8},
keywords = {SYCL, Programming Models, Intel, HPC, NVIDIA, HIP, GPU, GPGPU, CUDA, AMD},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@article{10.1145/1516016.1516018,
author = {Scott, Katie Minardo},
title = {FEATUREIs Usability Obsolete?},
year = {2009},
issue_date = {May + June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1072-5520},
url = {https://doi.org/10.1145/1516016.1516018},
doi = {10.1145/1516016.1516018},
journal = {Interactions},
month = {may},
pages = {6–11},
numpages = {6}
}

@inproceedings{10.1145/3372297.3417232,
author = {Pashchenko, Ivan and Vu, Duc-Ly and Massacci, Fabio},
title = {A Qualitative Study of Dependency Management and Its Security Implications},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417232},
doi = {10.1145/3372297.3417232},
abstract = {Several large scale studies on the Maven, NPM, and Android ecosystems point out that many developers do not often update their vulnerable software libraries thus exposing the user of their code to security risks. The purpose of this study is to qualitatively investigate the choices and the interplay of functional and security concerns on the developers' overall decision-making strategies for selecting, managing, and updating software dependencies.We run 25 semi-structured interviews with developers of both large and small-medium enterprises located in nine countries. All interviews were transcribed, coded, and analyzed according to applied thematic analysis. They highlight the trade-offs that developers are facing and that security researchers must understand to provide effective support to mitigate vulnerabilities (for example bundling security fixes with functional changes might hinder adoption due to lack of resources to fix functional breaking changes).We further distill our observations to actionable implications on what algorithms and automated tools should achieve to effectively support (semi-)automatic dependency management.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1513–1531},
numpages = {19},
keywords = {interviews, security, qualitative study, vulnerable dependencies, dependency management},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1145/1089551.1089583,
author = {Wang, Tongsen and Wu, Lei and Lin, Zhangxi},
title = {The Revival of Mozilla in the Browser War against Internet Explorer},
year = {2005},
isbn = {1595931120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1089551.1089583},
doi = {10.1145/1089551.1089583},
abstract = {The competition between Internet Explorer and Netscape has been one of the famous issues in the IT world regarding the phenomenon of market dynamics in the information age. Previous investigation indicates that IE and Netscape mainly competed in browser's functionalities and performances. The vertical integration strategy brought about Microsoft's success in the first round of the browser war. Today, the revival of Mozilla by Firefox shows that the vertical externality is a two-edge sword that can have negative effects on IE when security becomes a more outstanding issue. In addition, the open source nature of Firefox becomes one of its critical success factors. Based on this, we can expect that the monopoly of Microsoft can be downplayed by the adoption of open source strategy in the software market.},
booktitle = {Proceedings of the 7th International Conference on Electronic Commerce},
pages = {159–166},
numpages = {8},
keywords = {open source, vertical integration, competition, externalities, compatibility, browser},
location = {Xi'an, China},
series = {ICEC '05}
}

@inproceedings{10.1145/2507065.2507079,
author = {Potts, Liza and Harrison, Angela},
title = {Interfaces as Rhetorical Constructions: Reddit and 4chan during the Boston Marathon Bombings},
year = {2013},
isbn = {9781450321310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2507065.2507079},
doi = {10.1145/2507065.2507079},
abstract = {In this paper, we describe the rhetorical construction of two community sites and analyze how these sites support the information sharing practices of these communities. By examining activity on web-based discussion boards reddit and 4chan, we show how these spaces are developed and shaped over time by participants making rhetorical moves in order to share content within these ecologies. During the 2013 Boston Marathon bombings, we show how these spaces can be altered, disregarding the more typical practices on these sites. When community members embrace or reject these uses, it is as much a reaction to the content as it is to the cultural misuse of the community. In the case of reddit and 4hcan, this acceptance and rejection is especially true when the makers and maintainers of the system are participants themselves. Through this examination, we conclude that it is important to understand the rhetorical construction of these systems as reflections of the cultures they support.},
booktitle = {Proceedings of the 31st ACM International Conference on Design of Communication},
pages = {143–150},
numpages = {8},
keywords = {terrorism, social web, 4chan, digital rhetoric, reddit, information design, participation, disaster, digital culture, interface design, bulletin boards, user experience},
location = {Greenville, North Carolina, USA},
series = {SIGDOC '13}
}

@article{10.1145/2620660.2620662,
author = {Bland, Mike},
title = {Finding More Than One Worm in the Apple: If You See Something, Say Something.},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/2620660.2620662},
doi = {10.1145/2620660.2620662},
abstract = {In February Apple revealed and fixed an SSL (Secure Sockets Layer) vulnerability that had gone undiscovered since the release of iOS 6.0 in September 2012. It left users vulnerable to man-in-the-middle attacks thanks to a short circuit in the SSL/TLS (Transport Layer Security) handshake algorithm introduced by the duplication of a goto statement. Since the discovery of this very serious bug, many people have written about potential causes. A close inspection of the code, however, reveals not only how a unit test could have been written to catch the bug, but also how to refactor the existing code to make the algorithm testable - as well as more clues to the nature of the error and the environment that produced it.},
journal = {Queue},
month = {may},
pages = {10–21},
numpages = {12}
}

@article{10.1145/1457516.1457519,
author = {Doernhoefer, Mark},
title = {Surfing the Net for Software Engineering Notes},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1457516.1457519},
doi = {10.1145/1457516.1457519},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {7–16},
numpages = {10}
}

@inproceedings{10.5555/1298455.1298489,
author = {Peterson, Larry and Bavier, Andy and Fiuczynski, Marc E. and Muir, Steve},
title = {Experiences Building PlanetLab},
year = {2006},
isbn = {1931971471},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper reports our experiences building PlanetLab over the last four years. It identifies the requirements that shaped PlanetLab, explains the design decisions that resulted from resolving conflicts among these requirements, and reports our experience implementing and supporting the system. Due in large part to the nature of the "PlanetLab experiment," the discussion focuses on synthesis rather than new techniques, balancing system-wide considerations rather than improving performance along a single dimension, and learning from feedback from a live system rather than controlled experiments using synthetic workloads.},
booktitle = {Proceedings of the 7th Symposium on Operating Systems Design and Implementation},
pages = {351–366},
numpages = {16},
location = {Seattle, Washington},
series = {OSDI '06}
}

@techreport{10.5555/2965630,
author = {Timmes, Frank and Turk, Matthew and Ahalt, Stan and Wang, Shaowen and Idaszak, Ray and Brower, Richard and Lenhardt, Chris and Gustafson, Karl},
title = {2016 Software Infrastructure for Sustained Innovation (SI2) PI Workshop},
year = {2016},
publisher = {National Science Foundation},
address = {USA},
abstract = {This report summarizes the organization and execution of the National Science Foundation (NSF) sponsored workshop, "Software Infrastructure for Sustained Innovation (SI2) Principal Investigator (PI) Workshop," and the observations and discussions it generated. The workshop was held February 16-17, 2016 at The Westin Arlington Gateway in Arlington, VA and was professionally facilitated by Knowinnovation. It was attended by 98 participants from SI2 program supported projects, industry, and government laboratories. The workshop served as an open forum for identifying challenges and consolidating common approaches used by software practitioners to organize, manage, and sustain academic research software infrastructures. By facilitating the open exchange of experiences and knowledge, the workshop yielded valuable insights to help inform existing and new software infrastructure projects, and also helped nurture a community essential to addressing upcoming challenges in software infrastructures.}
}

@inproceedings{10.1145/1878450.1878465,
author = {Filho, Fernando Figueira and Olson, Gary M. and de Geus, Paulo L\'{\i}cio},
title = {Kolline: A Task-Oriented System for Collaborative Information Seeking},
year = {2010},
isbn = {9781450304030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878450.1878465},
doi = {10.1145/1878450.1878465},
abstract = {This paper presents results of an exploratory study which observed Linux novice users performing complex technical tasks using Google's search engine. In this study we observed that information triage is a difficult process for unexperienced users unless well structured information is provided which results in better satisfaction and search effectiveness. Providing a well structured information allows users to browse through different pieces of documentation without depending exclusively on the keyword search. Based on these observations, this research prototyped Kolline, a system that aims to facilitate information seeking for unexperienced users by allowing more experienced users to collaborate together. Users in Kolline create a task-oriented navigation structure based on web annotations. In this paper we discuss the potential benefits of this technique on helping unexperienced users to solve complex search tasks and present improvements for future work.},
booktitle = {Proceedings of the 28th ACM International Conference on Design of Communication},
pages = {89–94},
numpages = {6},
keywords = {social search, user study, collaborative information seeking, hypertext, interface prototype},
location = {S\~{a}o Carlos, S\~{a}o Paulo, Brazil},
series = {SIGDOC '10}
}

@article{10.1145/3631967,
author = {Li, Wen and Marino, Austin and Yang, Haoran and Meng, Na and Li, Li and Cai, Haipeng},
title = {How Are Multilingual Systems Constructed: Characterizing Language Use and Selection in Open-Source Multilingual Software},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631967},
doi = {10.1145/3631967},
abstract = {For many years now, modern software is known to be developed in multiple languages (hence termed as multilingual or multi-language software). Yet to this date we still only have very limited knowledge about how multilingual software systems are constructed. For instance, it is not yet really clear how different languages are used, selected together, and why they have been so in multilingual software development. Given the fact that using multiple languages in a single software project has become a norm, understanding language use and selection (i.e, language profile) as a basic element of the multilingual construction in contemporary software engineering is an essential first step. In this paper, we set out to fill this gap with a large-scale characterization study on language use and selection in open-source multilingual software. We start with presenting an updated overview of language use in 7,113 GitHub projects spanning five past years by characterizing overall statistics of language profiles, followed by a deeper look into the functionality relevance/justification of language selection in these projects through association rule mining. We proceed with an evolutionary characterization of 1,000 GitHub projects for each of 10 past years to provide a longitudinal view of how language use and selection have changed over the years, as well as how the association between functionality and language selection has been evolving. Among many other findings, our study revealed a growing trend of using 3 to 5 languages in one multilingual software project and noticeable stableness of top language selections. We found a non-trivial association between language selection and certain functionality domains, which was less stable than that with individual languages over time. In a historical context, we also have observed major shifts in these characteristics of multilingual systems both in contrast to earlier peer studies and along the evolutionary timeline. Our findings offer essential knowledge on the multilingual construction in modern software development. Based on our results, we also provide insights and actionable suggestions for both researchers and developers of multilingual systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {language use, Multilingual software, evolutionary characterization, functionality relevance, association mining, language selection, language profile}
}

@inproceedings{10.1145/3624062.3624587,
author = {Ferlanti, Erik and Allen, William J. and Lima, Ernesto A. B. F. and Wang, Yinzhi and Fonner, John M.},
title = {Perspectives and Experiences Supporting Containers for Research Computing at the Texas Advanced Computing Center},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624587},
doi = {10.1145/3624062.3624587},
abstract = {Containers are becoming essential to support the diversity of scientific computing workloads at academic computing centers. Here, we offer perspectives and experiences from the Texas Advanced Computing Center on: the installation, configuration, and support of select containerization platforms; incorporation of containers into the module system to improve their discoverability and usability; facilitation of advanced use cases including MPI containers, GPU containers, and support for multiple instruction set architectures; and finally instruction on best practices to end users through workshops and university courses. We will briefly discuss case studies that highlight the importance of supporting containers for research computing.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {155–164},
numpages = {10},
keywords = {Singularity, Docker, Containers, High performance computing, Apptainer, System administration},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3173574.3173773,
author = {Suzuki, Ryo and Kato, Jun and Gross, Mark D. and Yeh, Tom},
title = {Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173773},
doi = {10.1145/3173574.3173773},
abstract = {We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {programming by demonstration, swarm user interfaces, tangible programming, direct manipulation},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@article{10.14778/3494124.3494151,
author = {Lee, Doris Jung-Lin and Tang, Dixin and Agarwal, Kunal and Boonmark, Thyne and Chen, Caitlyn and Kang, Jake and Mukhopadhyay, Ujjaini and Song, Jerry and Yong, Micah and Hearst, Marti A. and Parameswaran, Aditya G.},
title = {Lux: Always-on Visualization Recommendations for Exploratory Dataframe Workflows},
year = {2021},
issue_date = {November 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3494124.3494151},
doi = {10.14778/3494124.3494151},
abstract = {Exploratory data science largely happens in computational notebooks with dataframe APIs, such as pandas, that support flexible means to transform, clean, and analyze data. Yet, visually exploring data in dataframes remains tedious, requiring substantial programming effort for visualization and mental effort to determine what analysis to perform next. We propose Lux, an always-on framework for accelerating visual insight discovery in dataframe workflows. When users print a dataframe in their notebooks, Lux recommends visualizations to provide a quick overview of the patterns and trends and suggests promising analysis directions. Lux features a high-level language for generating visualizations on demand to encourage rapid visual experimentation with data. We demonstrate that through the use of a careful design and three system optimizations, Lux adds no more than two seconds of overhead on top of pandas for over 98% of datasets in the UCI repository. We evaluate Lux in terms of usability via interviews with early adopters, finding that Lux helps fulfill the needs of data scientists for visualization support within their dataframe workflows. Lux has already been embraced by data science practitioners, with over 3.1k stars on Github.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {727–738},
numpages = {12}
}

@inproceedings{10.5555/3437539.3437763,
author = {Denis-Courmont, R\'{e}mi and Liljestrand, Hans and Chinea, Carlos and Ekberg, Jan-Erik},
title = {Camouflage: Hardware-Assisted CFI for the ARM Linux Kernel},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Software control-flow integrity (CFI) solutions have been applied to the Linux kernel for memory protection. Due to performance costs, deployed software CFI solutions are coarse grained. In this work, we demonstrate a precise hardware-assisted kernel CFI running on widely-used off-the-shelf processors. Specifically, we use the ARMv8.3 pointer authentication (PAuth) extension and present a design that uses it to achieve strong security guarantees with minimal performance penalties. Furthermore, we show how deployment of such security primitives in the kernel can significantly differ from their user space application.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {224},
numpages = {6},
location = {Virtual Event, USA},
series = {DAC '20}
}

@article{10.1145/3372115,
author = {B\"{o}hme, Rainer and Eckey, Lisa and Moore, Tyler and Narula, Neha and Ruffing, Tim and Zohar, Aviv},
title = {Responsible Vulnerability Disclosure in Cryptocurrencies},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3372115},
doi = {10.1145/3372115},
abstract = {Software weaknesses in cryptocurrencies create unique challenges in responsible revelations.},
journal = {Commun. ACM},
month = {sep},
pages = {62–71},
numpages = {10}
}

@article{10.1145/1300655.1300680,
author = {Zucker, Daniel F. and Bulterman, Dick},
title = {Open Standard and Open Sourced SMIL for Interactivity},
year = {2007},
issue_date = {November + December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1072-5520},
url = {https://doi.org/10.1145/1300655.1300680},
doi = {10.1145/1300655.1300680},
journal = {Interactions},
month = {nov},
pages = {41–46},
numpages = {6}
}

@inproceedings{10.1145/3600160.3605021,
author = {Schneider, Michael and Spiekermann, Daniel and Keller, J\"{o}rg},
title = {Network Covert Channels in Routing Protocols},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3605021},
doi = {10.1145/3600160.3605021},
abstract = {Computer networks play a key role in everyday lives. To guarantee fail-safe operation, routing protocols are used that enable dynamic routing via redundant paths. Because of this, routing protocols like RIP or OSPF play an important role in modern network infrastructures. The widespread use together with the mostly missing traffic monitoring of these protocols provide a possible base to exploit these protocols for network steganographic channels. In this paper, we present a novel storage covert channel based on the OSPF routing protocol. We analyzed the protocol in detail with the help of hiding patterns to identify protocol fields that might be suitable for covert communication. We provide a proof-of-concept implementation of our covert channel inside a simulated network, which demonstrates the possibility of covert communication in a routing protocol. Our evaluation covers detectability and countermeasures, steganographic bandwidth and robustness. Furthermore, we sketch an application scenario where such a covert channel can be deployed.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {42},
numpages = {8},
keywords = {hiding patterns, routing protocols, OSPF, network steganography, storage covert channels},
location = {Benevento, Italy},
series = {ARES '23}
}

@inproceedings{10.1145/949344.949440,
author = {Colyer, Adrian and Clement, Andy and Bodkin, Ron and Hugunin, Jim},
title = {Using AspectJ for Component Integration in Middleware},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949440},
doi = {10.1145/949344.949440},
abstract = {This report discusses experiences applying AspectJ [1] to modularize crosscutting concerns in a middleware product line at IBM®. Aspect oriented programming techniques were used to cleanly separate platform specific facilities for aspects such as error handling, performance monitoring and logging from base components, permitting those components to be reused in multiple environments. The initiative also guided the design of the AspectJ Development Tools (AJDT) for Eclipse, and influenced the technical direction of the AspectJ implementation.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {339–344},
numpages = {6},
keywords = {AspectJ, aspect-orientation, middleware},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.1145/1940761.1940785,
author = {Gurzick, David and White, Kevin F. and Lutters, Wayne G. and Landry, Brian M. and Dombrowski, Caroline and Kim, Jeffery Y.},
title = {Designing the Future of Collaborative Workplace Systems: Lessons Learned from a Comparison with Alternate Reality Games},
year = {2011},
isbn = {9781450301213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1940761.1940785},
doi = {10.1145/1940761.1940785},
abstract = {Alternate reality games (ARGs) represent a unique form of group collaboration. A careful comparison of ARGs to more traditional collaborative systems reveals areas for innovation in tools to support ad-hoc teaming. This comparison specifically focuses on processes of group formation, task management, information discovery and collective storytelling. Opportunities for innovation are highlighted, as are future research questions.},
booktitle = {Proceedings of the 2011 IConference},
pages = {174–180},
numpages = {7},
keywords = {groupwork, groupware, alternate reality games, storytelling, CSCW},
location = {Seattle, Washington, USA},
series = {iConference '11}
}

@inproceedings{10.1145/2371574.2371603,
author = {Frohlich, David and Robinson, Simon and Eglinton, Kristen and Jones, Matt and Vartiainen, Elina},
title = {Creative Cameraphone Use in Rural Developing Regions},
year = {2012},
isbn = {9781450311052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371574.2371603},
doi = {10.1145/2371574.2371603},
abstract = {In this paper we consider the current and future use of cameraphones in the context of rural South Africa, where many people do not have access to the latest models and ICT infrastructure is poor. We report a new study of cameraphone use in this setting, and the design and testing of a novel application for creating rich multimedia narratives and materials. We argue for better creative media applications on mobile platforms in this region, and greater attention to their local use.},
booktitle = {Proceedings of the 14th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {181–190},
numpages = {10},
keywords = {cameraphone, storytelling, mobile, development},
location = {San Francisco, California, USA},
series = {MobileHCI '12}
}

@inproceedings{10.1145/3030207.3030213,
author = {Leitner, Philipp and Bezemer, Cor-Paul},
title = {An Exploratory Study of the State of Practice of Performance Testing in Java-Based Open Source Projects},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030213},
doi = {10.1145/3030207.3030213},
abstract = {The usage of open source (OS) software is wide-spread across many industries. While the functional quality of OS projects is considered to be similar to closed-source software, much is unknown about the quality in terms of performance. One challenge for OS developers is that, unlike for functional testing, there is a lack of accepted best practices for performance testing. To reveal the state of practice of performance testing in OS projects, we conduct an exploratory study on 111 Java-based OS projects from GitHub. We study the performance tests of these projects from five perspectives: (1) developers, (2) size, (3) test organization, (4) types of performance tests and (5) used tooling. We show that writing performance tests is not a popular task in OS projects: performance tests form only a small portion of the test suite, are rarely updated, and are usually maintained by a small group of core project developers. Further, even though many projects are aware that they need performance tests, developers appear to struggle implementing them. We argue that future performance testing frameworks should provider better support for low-friction testing, for instance via non-parameterized methods or performance test generation, as well as focus on a tight integration with standard continuous integration tooling.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {373–384},
numpages = {12},
keywords = {performance testing, performance engineering, open source, empirical software engineering, mining software repositories},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/1641587.1641590,
author = {Mansmann, Florian and Fischer, Fabian and Keim, Daniel A. and North, Stephen C.},
title = {Visual Support for Analyzing Network Traffic and Intrusion Detection Events Using TreeMap and Graph Representations},
year = {2009},
isbn = {9781605585727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1641587.1641590},
doi = {10.1145/1641587.1641590},
abstract = {Network security depends heavily on automated Intrusion Detection Systems (IDS) to sense malicious activity. Unfortunately, IDS often deliver both too much raw information, and an incomplete local picture, impeding accurate assessment of emerging threats. We propose a system to support analysis of IDS logs, that visually pivots large sets of Net-Flows. In particular, two visual representations of the flow data are compared: a TreeMap visualization of local network hosts, which are linked through hierarchical edge bundles with the external hosts, and a graph representation using a force-directed layout to visualize the structure of the host communication patterns. Three case studies demonstrate the capabilities of our tool to 1) analyze service usage in a managed network, 2) detect a distributed attack, and 3) investigate hosts in our network that communicate with suspect external IPs.},
booktitle = {Proceedings of the Symposium on Computer Human Interaction for the Management of Information Technology},
articleno = {3},
numpages = {10},
location = {Baltimore, Maryland},
series = {CHiMiT '09}
}

@inproceedings{10.1145/2786805.2786860,
author = {Jensen, Simon Holm and Sridharan, Manu and Sen, Koushik and Chandra, Satish},
title = {MemInsight: Platform-Independent Memory Debugging for JavaScript},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786860},
doi = {10.1145/2786805.2786860},
abstract = {JavaScript programs often suffer from memory issues that can either hurt performance or eventually cause memory exhaustion. While existing snapshot-based profiling tools can be helpful, the information provided is limited to the coarse granularity at which snapshots can be taken. We present MemInsight, a tool that provides detailed, time-varying analysis of the memory behavior of JavaScript applications, including web applications. MemInsight is platform independent and runs on unmodified JavaScript engines. It employs tuned source-code instrumentation to generate a trace of memory allocations and accesses, and it leverages modern browser features to track precise information for DOM (document object model) objects. It also computes exact object lifetimes without any garbage collector assistance, and exposes this information in an easily-consumable manner for further analysis. We describe several client analyses built into MemInsight, including detection of possible memory leaks and opportunities for stack allocation and object inlining. An experimental evaluation showed that with no modifications to the runtime, MemInsight was able to expose memory issues in several real-world applications.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {345–356},
numpages = {12},
keywords = {leak detection, Memory profiling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3290605.3300690,
author = {Feger, Sebastian S. and Dallmeier-Tiessen, S\"{u}nje and Wo\'{z}niak, Pawe\l{} W. and Schmidt, Albrecht},
title = {Gamification in Science: A Study of Requirements in the Context of Reproducible Research},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300690},
doi = {10.1145/3290605.3300690},
abstract = {The need for data preservation and reproducible research is widely recognized in the scientific community. Yet, researchers often struggle to find the motivation to contribute to data repositories and to use tools that foster reproducibility. In this paper, we explore possible uses of gamification to support reproducible practices in High Energy Physics. To understand how gamification can be effective in research tools, we participated in a workshop and performed interviews with data analysts. We then designed two interactive prototypes of a research preservation service that use contrasting gamification strategies. The evaluation of the prototypes showed that gamification needs to address core scientific challenges, in particular the fair reflection of quality and individual contribution. Through thematic analysis, we identified four themes which describe perceptions and requirements of gamification in research: Contribution, Metrics, Applications and Scientific practice. Based on these, we discuss design implications for gamification in science.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {research reproducibility, science, game design elements, gamification},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3018896.3025168,
author = {Cloete, A. H. and Booysen, M. J. and Sandell, R. C. and van der Merwe, A. B.},
title = {Smart Electric Water Heaters: A System Architecture Proposal for Scalable IoT},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3025168},
doi = {10.1145/3018896.3025168},
abstract = {Energy and water are scarce resources in South Africa, and the management thereof is receiving increased attention. One rare place where energy and water coincide is in water heaters, which consume over 30% of household energy in South Africa and collectively pass over 500 billion litres of water annually. These resource-hungry devices are not well managed and tend to be physically inaccessible and difficult to understand for users. The advent of wireless and cloud computing technology brought expectations of simple management of a multitude of devices and the so-called Internet of Things. A few solutions have been proposed for remote water heater management, but these are not scalable, and have not been benchmarked for scalability above tens of units. This paper proposes an end-to-end architecture to monitor and control EWHs based on MQTT, and specific emphasis is placed on the technologies suitable for large-scale cross-domain interoperability. The performance of the system with 10,000 connections and the results of a pilot deployment are presented.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {119},
numpages = {7},
keywords = {internet of things, electric water heaters, smart cities},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00024,
author = {Alves, Isaque and Rocha, Carla},
title = {Qualifying Software Engineers Undergraduates in DevOps - Challenges of Introducing Technical and Non-Technical Concepts in a Project-Oriented Course},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00024},
doi = {10.1109/ICSE-SEET52601.2021.00024},
abstract = {The constant changes in the software industry, practices, and methodologies impose challenges to teaching and learning current software engineering concepts and skills. DevOps is particularly challenging because it covers technical concepts, such as pipeline automation, and non-technical ones, such as team roles and project management. The present study investigates a course setup to introduce these concepts to software engineering undergraduates. We designed the course by employing coding to associate DevOps concepts to Agile, Lean, and Open source practices and tools. We present the main aspects of this project-oriented DevOps course, with 240 students enrolled it since its first offering in 2016. We conducted an empirical study, with both a quantitative and qualitative analysis, to evaluate this project-oriented course setup. We collected the data from the projects repository and students' perceptions from a questionnaire. We mined 148 repositories (corresponding to 72 projects) and obtained 86 valid responses to the questionnaire. We also mapped the concepts which are more challenging to students learn from experience. The results evidence that first-hand experience facilitates the comprehension of DevOps concepts and enriches classes discussions. we present a set of lessons learned, which may help professors better design and conduct project-oriented courses to cover DevOps concepts.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {144–153},
numpages = {10},
keywords = {OSS, open source, education, Agile software development, emerging domains of software, FOSS, DevOps, empirical software engineering, tools and environments},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.5555/998675.999443,
author = {Froehlich, Jon and Dourish, Paul},
title = {Unifying Artifacts and Activities in a Visual Tool for Distributed Software Development Teams},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In large projects, software developers struggle with two sources of complexity the complexity of the code itself, and the complexity of of the process of producing it. Both of these concerns have been subjected to considerable research investigation, and tools and techniques have been developed to help manage them. However, these solutions have generally been developed independently, making it difficult to deal with problems that inherently span both dimensions.We describe Augur, a visualization tool that supports distributed software development processes. Augur creates visual representations of both software artifacts and software development activities, and, crucially, allows developers to explore the relationship between them. Augur is designed not for managers, but for the developers participating in the software development process.We discuss some of the early results of informal evaluation with open source software developers. Our experiences to date suggest that combining views of artifacts and activities is both meaningful and valuable to software developers.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {387–396},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.5555/2023718.2023737,
author = {Bissyand\'{e}, Tegawend\'{e} F. and R\'{e}veill\`{e}re, Laurent and Bromberg, Y\'{e}rom-David and Lawall, Julia L. and Muller, Gilles},
title = {Bridging the Gap between Legacy Services and Web Services},
year = {2010},
isbn = {9783642169540},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web Services is an increasingly used instantiation of Service-Oriented Architectures (SOA) that relies on standard Internet protocols to produce services that are highly interoperable. Other types of services, relying on legacy application layer protocols, however, cannot be composed directly. A promising solution is to implement wrappers to translate between the application layer protocols and the WS protocol. Doing so manually, however, requires a high level of expertise, in the relevant application layer protocols, in low-level network and system programming, and in the Web Service paradigm itself.In this paper, we introduce a generative language based approach for constructing wrappers to facilitate the migration of legacy service functionalities to Web Services. To this end, we have designed the Janus domain-specific language, which provides developers with a high-level way to describe the operations that are required to encapsulate legacy service functionalities. We have successfully used Janus to develop a number of wrappers, including wrappers for IMAP and SMTP servers, for a RTSP-compliant media server and for UPnP service discovery. Preliminary experiments show that Janus-based WS wrappers have performance comparable to manually written wrappers.},
booktitle = {Proceedings of the ACM/IFIP/USENIX 11th International Conference on Middleware},
pages = {273–292},
numpages = {20},
location = {Bangalore, India},
series = {Middleware '10}
}

@inproceedings{10.1145/3430524.3446075,
author = {Harper, Andy and Aflatoony, Leila},
title = {Always On: Unpacking the Challenges of Living with Insulin Pumps, to Design Novel Solutions},
year = {2021},
isbn = {9781450382137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430524.3446075},
doi = {10.1145/3430524.3446075},
abstract = {Insulin pumps are effective tools for the precise control of glucose levels, for type 1 diabetes (T1D) patients. Unfortunately, many design and usability challenges still exist with these technologies. We investigated current shortcomings and limitations through survey (N=105), interview (N=7), and participatory workshop (N=3) data collection methods. Our findings revealed issues with current technolog y including wear-ability and accessibility in public, operation while performing demanding tasks, interruptions during social activities, continuity of maintenance, and interface operations. Using the data from our investigative work, we produced design criteria to develop a novel wrist-worn interface and separate pump design for a closed loop system. We then evaluated the design through remote usability testing sessions (N=7) with insulin pump users. Our study aspires to inform the future design of novel insulin pumps that enable people with T1D to maintain better control of their glucose through consistent interactions with these tools, during their everyday activities.},
booktitle = {Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {99},
numpages = {13},
keywords = {Diabetes Management, Insulin Pump, Interface Design, Continuous Glucose Monitor},
location = {Salzburg, Austria},
series = {TEI '21}
}

@inproceedings{10.1145/1397718.1397720,
author = {Naous, Jad and Gibb, Glen and Bolouki, Sara and McKeown, Nick},
title = {NetFPGA: Reusable Router Architecture for Experimental Research},
year = {2008},
isbn = {9781605581811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1397718.1397720},
doi = {10.1145/1397718.1397720},
abstract = {Our goal is to enable fast prototyping of networking hardware (e.g. modified Ethernet switches and IP routers) for teaching and research. To this end, we built and made available the NetFPGA platform. Starting from open-source reference designs, students and researchers create their designs in Verilog, and then download them to the NetFPGA board where they can process packets at line-rate for 4-ports of 1GE. The board is becoming widely used for teaching and research, and so it has become important to make it easy to re-use modules and designs. We have created a standard interface between modules, making it easier to plug modules together in pipelines, and to create new re-usable designs. In this paper we describe our modular design, and how we have used it to build several systems, including our IP router reference design and some extensions to it.},
booktitle = {Proceedings of the ACM Workshop on Programmable Routers for Extensible Services of Tomorrow},
pages = {1–7},
numpages = {7},
keywords = {reuse, modular design, netfpga},
location = {Seattle, WA, USA},
series = {PRESTO '08}
}

@inproceedings{10.1145/2095050.2095072,
author = {Torres, Weslley and Pinto, Gustavo and Fernandes, Benito and Oliveira, Jo\~{a}o Paulo and Ximenes, Filipe Alencar and Castor, Fernando},
title = {Are Java Programmers Transitioning to Multicore? A Large Scale Study of Java FLOSS},
year = {2011},
isbn = {9781450311830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095050.2095072},
doi = {10.1145/2095050.2095072},
abstract = {We would like to know if Java developers are retrofitting applications to become concurrent and, to get better performance on multicore machines. Also, we would like to know what concurrent programming constructs they currently use. Evidence of how programmers write concurrent programs can help other programmers to be more efficient when using the available constructs. Moreover, this evidence can assist researchers in devising new mechanisms and improving existing ones. For this purpose, we have conducted a study targeting a large-scale Java open source repository, SourceForge. We have analyzed a number of FLOSS projects along two dimensions: spatial and temporal. For the spatial dimension, we studied the latest versions of more than 2000 projects. Our goal is to understand which constructs developers of concurrent systems employ and how frequently they use them. For the temporal dimension we took a closer look at various versions of six projects and analyzed how the use of concurrency constructs has evolved over time. In addition, we tried to establish if uses of concurrency control constructs were aimed at leveraging multicore processors. We have downloaded more than two thousand Java projects including their various versions, in addition to individual analysing about six well known open-source projects.},
booktitle = {Proceedings of the Compilation of the Co-Located Workshops on DSM'11, TMC'11, AGERE! 2011, AOOPES'11, NEAT'11, &amp; VMIL'11},
pages = {123–128},
numpages = {6},
keywords = {multicore, concurrent, parallel, open-source, java},
location = {Portland, Oregon, USA},
series = {SPLASH '11 Workshops}
}

@inproceedings{10.1109/ICSE.2017.38,
author = {Li, Menghao and Wang, Wei and Wang, Pei and Wang, Shuai and Wu, Dinghao and Liu, Jian and Xue, Rui and Huo, Wei},
title = {LibD: Scalable and Precise Third-Party Library Detection in Android Markets},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.38},
doi = {10.1109/ICSE.2017.38},
abstract = {With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis.According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {335–346},
numpages = {12},
keywords = {third-party library, android, software mining},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3583119,
author = {Kaczorowski, Maya and Momot, Falcon and Neville-Neil, George and McCubbin, Chris},
title = {OSS Supply-Chain Security: What Will It Take?},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3583119},
doi = {10.1145/3583119},
abstract = {A discussion with Maya Kaczorowski, Falcon Momot, George Neville-Neil, and Chris McCubbin},
journal = {Commun. ACM},
month = {mar},
pages = {48–54},
numpages = {7}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3594806,
title = {PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@inproceedings{10.1145/347642.347798,
author = {Fischer, Gerhard and Scharff, Eric},
title = {Meta-Design: Design for Designers},
year = {2000},
isbn = {1581132190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/347642.347798},
doi = {10.1145/347642.347798},
abstract = {One fundamental challenge for the design of the interactive systems of the future is to invent and design environments and cultures in which humans can express themselves and engage in personally meaningful activities. Unfortunately, a large number of new media are designed from a perspective of viewing and treating humans primarily as consumers. The possibility for humans to be and act as designers  (in cases in which they desire to do so) should be accessible not only to a small group of high-tech scribes, but rather to all interested individuals and groups. Meta-design characterizes activities, processes, and objectives to create new media and environments that allow users to act as designers and be creative.In this paper we discuss problems addressed by our research on meta-design, provide a conceptual framework for meta-design, and illustrate our developments in the context of a particular system, the Envisionment and Discovery Collaboratory.},
booktitle = {Proceedings of the 3rd Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques},
pages = {396–405},
numpages = {10},
keywords = {open evolvable systems, consumer and designer mindsets, reseeding model, designing “out of the box”, evolutionary growth, seeding, domain-oriented design environments, impact of new media on design, underdesigned systems, open source},
location = {New York City, New York, USA},
series = {DIS '00}
}

@inproceedings{10.1145/3210259.3210262,
author = {Erb, Benjamin and Mei\ss{}ner, Dominik and Kargl, Frank and Steer, Benjamin A. and Cuadrado, Felix and Margan, Domagoj and Pietzuch, Peter},
title = {Graphtides: A Framework for Evaluating Stream-Based Graph Processing Platforms},
year = {2018},
isbn = {9781450356954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210259.3210262},
doi = {10.1145/3210259.3210262},
abstract = {Stream-based graph systems continuously ingest graph-changing events via an established input stream, performing the required computation on the corresponding graph. While there are various benchmarking and evaluation approaches for traditional, batch-oriented graph processing systems, there are no common procedures for evaluating stream-based graph systems. We, therefore, present GraphTides, a generic framework which includes the definition of an appropriate system model, an exploration of the parameter space, suitable workloads, and computations required for evaluating such systems. Furthermore, we propose a methodology and provide an architecture for running experimental evaluations. With our framework, we hope to systematically support system development, performance measurements, engineering, and comparisons of stream-based graph systems.},
booktitle = {Proceedings of the 1st ACM SIGMOD Joint International Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {3},
numpages = {10},
keywords = {measurements, graph processing, graph analytics, temporal graphs, evolving graphs, stream-based graphs, evaluation},
location = {Houston, Texas},
series = {GRADES-NDA '18}
}

@inproceedings{10.1145/2509136.2509516,
author = {Herhut, Stephan and Hudson, Richard L. and Shpeisman, Tatiana and Sreeram, Jaswanth},
title = {River Trail: A Path to Parallelism in JavaScript},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509516},
doi = {10.1145/2509136.2509516},
abstract = {JavaScript is the most popular language on the web and is a crucial component of HTML5 applications and services that run on consumer platforms ranging from desktops to phones. However, despite ample amount of hardware parallelism available to web applications on such platforms, JavaScript web applications remain predominantly sequential. Common parallel programming solutions accepted by other programming languages failed to transfer themselves to JavaScript due to differences in programming models, the additional requirements of the web and different developer expectations.In this paper we present River Trail - a parallel programming model and API for JavaScript that provides safe, portable, programmer-friendly, deterministic parallelism to JavaScript applications. River Trail allows web applications to effectively utilize multiple cores, vector instructions, and GPUs on client platforms while allowing the web developer to remain within the environment of JavaScript. We describe the implementation of the River Trail compiler and runtime and present experimental results that show the impact of River Trail on performance and scalability for a variety of realistic HTML5 applications. Our experiments show that River Trail has a dramatic positive impact on overall performance and responsiveness of computationally intense JavaScript based applications achieving up to 33.6 times speedup for kernels and up to 11.8 times speedup for realistic web applications compared to sequential JavaScript. Moreover, River Trail enables new interactive web usages that are simply not even possible with standard sequential JavaScript.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {729–744},
numpages = {16},
keywords = {javascript, parallelism},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@inproceedings{10.1145/3531348.3532177,
author = {Saito, Taro L. and Takezoe, Naoki and Okada, Yukihiro and Shimamoto, Takako and Yu, Dongmin and Chandrashekharachar, Suprith and Sasaki, Kai and Okumiya, Shohei and Wang, Yan and Kurihara, Takashi and Kobayashi, Ryu and Suzuki, Keisuke and Yang, Zhenghong and Onizuka, Makoto},
title = {Journey of Migrating Millions of Queries on The Cloud},
year = {2022},
isbn = {9781450393539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531348.3532177},
doi = {10.1145/3531348.3532177},
abstract = {Treasure Data is processing millions of distributed SQL queries every day on the cloud. Upgrading the query engine service at this scale is challenging because we need to migrate all of the production queries of the customers to a new version while preserving the correctness and performance of the data processing pipelines. To ensure the quality of the query engines, we utilize our query logs to build customer-specific benchmarks and replay these queries with real customer data in a secure pre-production environment. To simulate millions of queries, we need effective minimization of test query sets and better reporting of the simulation results to proactively find incompatible changes and performance regression of the new version. This paper describes the overall design of our system and shares various challenges in maintaining the quality of the query engine service on the cloud.},
booktitle = {Proceedings of the 2022 Workshop on 9th International Workshop of Testing Database Systems},
pages = {10–16},
numpages = {7},
keywords = {Database testing, Hive, Cloud database, Trino, Treasure Data, Query simulation},
location = {Philadelphia, PA, USA},
series = {DBTest '22}
}

@inproceedings{10.1109/SC.2010.22,
author = {Giampapa, Mark and Gooding, Thomas and Inglett, Todd and Wisniewski, Robert W.},
title = {Experiences with a Lightweight Supercomputer Kernel: Lessons Learned from Blue Gene's CNK},
year = {2010},
isbn = {9781424475599},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SC.2010.22},
doi = {10.1109/SC.2010.22},
abstract = {The Petascale era has recently been ushered in and many researchers have already turned their attention to the challenges of exascale computing. To achieve petascale computing two broad approaches for kernels were taken, a lightweight approach embodied by IBM Blue Gene's CNK, and a more fullweight approach embodied by Cray's CNL. There are strengths and weaknesses to each approach. Examining the current generation can provide insight as to what mechanisms may be needed for the exascale generation. The contributions of this paper are the experiences we had with CNK on Blue Gene/P. We demonstrate it is possible to implement a small lightweight kernel that scales well but still provides a Linux environment and functionality desired by HPC programmers. Such an approach provides the values of reproducibility, low noise, high and stable performance, reliability, and ease of effectively exploiting unique hardware features. We describe the strengths and weaknesses of this approach.},
booktitle = {Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {1–10},
numpages = {10},
series = {SC '10}
}

@inproceedings{10.1145/3325480.3325502,
author = {Manshaei, Roozbeh and Mayat, Uzair and Tarun, Aneesh and DeLong, Sean and Chiang, David and Digregorio, Justin and Khayyer, Shahin and Gupta, Apurva and Kyan, Matthew and Mazalek, Ali},
title = {Tangible Tensors: An Interactive System for Grasping Trends in Biological Systems Modeling},
year = {2019},
isbn = {9781450359177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325480.3325502},
doi = {10.1145/3325480.3325502},
abstract = {Advances in biology and computational power have led to the availability of large biological data sets, yet these advances raise new design challenges. Designers must build effective tools that cater to the needs of biologists and data scientists in order to visually explore and manipulate data for modeling and analysis. We present the Tangible Tensors system, a new tensor-based visualization and tangible manipulation tool that serves to improve functionality over previous data analytics approaches. We designed a platform that supports iterative exploration of the solution space and better interpretation tools for biologists. User study results indicate that our system is easy to learn and use, and useful for data modeling and analysis tasks.},
booktitle = {Proceedings of the 2019 Conference on Creativity and Cognition},
pages = {41–52},
numpages = {12},
keywords = {tensor, interactive tabletops, active tangibles, tangible interaction, exploration, visualization, biological networks},
location = {San Diego, CA, USA},
series = {C&amp;C '19}
}

@inproceedings{10.1145/1297027.1297048,
author = {Leff, Avraham and Rayfield, James T.},
title = {Webrb: Evaluating a Visual Domain-Specific Language for Building Relational Web-Applications},
year = {2007},
isbn = {9781595937865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297027.1297048},
doi = {10.1145/1297027.1297048},
abstract = {Many web-applications can be characterized as "relational". In this paper we introduce and evaluate WebRB, a visualdomain-specific language for building such applications. WebRB addresses the limitations of the conventional "imperative-embedding" approach typically used to build relational web-applications. We describe the WebRB language, present extended examples of its use, and discuss the WebRB visual editor, libraries, and runtime. We then evaluate WebRB by comparing it to alternative approaches, and demonstrate its effectiveness in building relational web-applications.},
booktitle = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications},
pages = {281–300},
numpages = {20},
keywords = {visual programming languages, webrb, web-application development, relational web-applications, web relational blocks, data-flow languages},
location = {Montreal, Quebec, Canada},
series = {OOPSLA '07}
}

@inproceedings{10.1145/2335755.2335845,
author = {Alameda, Jay and Spear, Wyatt and Overbey, Jeffrey L. and Huck, Kevin and Watson, Gregory R. and Tibbitts, Beth},
title = {The Eclipse Parallel Tools Platform: Toward an Integrated Development Environment for XSEDE Resources},
year = {2012},
isbn = {9781450316026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335755.2335845},
doi = {10.1145/2335755.2335845},
abstract = {Eclipse [1] is a widely used, open source integrated development environment that includes support for C, C++, Fortran, and Python. The Parallel Tools Platform (PTP) [2] extends Eclipse to support development on high performance computers. PTP allows the user to run Eclipse on her laptop, while the code is compiled, run, debugged, and profiled on a remote HPC system. PTP provides development assistance for MPI, OpenMP, and UPC; it allows users to submit jobs to the remote batch system and monitor the job queue. It also provides a visual parallel debugger.The XSEDE community comprises a large part of PTP's user base, and we are actively working to make PTP a productive, easy-to-use development environment for the full breadth of XSEDE resources. In this paper, we will describe capabilities we have recently added to PTP to better support XSEDE resources. These capabilities include submission and monitoring of jobs on systems running Sun/Oracle Grid Engine, support for GSI authentication and MyProxy logon, support for environment modules, and integration with compilers from Cray and PGI. We will describe ongoing work and directions for future collaboration, including OpenACC support and parallel debugger integration.},
booktitle = {Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the EXtreme to the Campus and Beyond},
articleno = {48},
numpages = {8},
keywords = {integrated development environments, XSEDE, high performance computing, IDEs, PTP, Eclipse, programming environments, parallel tools platform},
location = {Chicago, Illinois, USA},
series = {XSEDE '12}
}

@inproceedings{10.1145/3558535.3559782,
author = {St\"{u}tz, Rainer and Stockinger, Johann and Moreno-Sanchez, Pedro and Haslhofer, Bernhard and Maffei, Matteo},
title = {Adoption and Actual Privacy of Decentralized CoinJoin Implementations in Bitcoin},
year = {2023},
isbn = {9781450398619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558535.3559782},
doi = {10.1145/3558535.3559782},
abstract = {We present a first measurement study on the adoption and actual privacy of two popular decentralized CoinJoin implementations, Wasabi and Samourai, in the broader Bitcoin ecosystem. By applying highly accurate (¿ 99%) algorithms we can effectively detect 30,251 Wasabi and 223,597 Samourai transactions within the block range 530,500 to 725,348 (2018-07-05 to 2022-02-28). We also found a steady adoption of these services with a total value of mixed coins of ca. 4.74 B USD and average monthly mixing amounts of ca. 172.93 M USD) for Wasabi and ca. 41.72 M USD for Samourai. Furthermore, we could trace ca. 322 M USD directly received by cryptoasset exchanges and ca. 1.16 B USD indirectly received via two hops. Our analysis further shows that the traceability of addresses during the pre-mixing and post-mixing narrows down the anonymity set provided by these coin mixing services. It also shows that the selection of addresses for the CoinJoin transaction can harm anonymity. Overall, this is the first paper to provide a comprehensive picture of the adoption and privacy of distributed CoinJoin transactions. Understanding this picture is particularly interesting in the light of ongoing regulatory efforts that will, on the one hand, affect compliance measures implemented in cryptocurrency exchanges and, on the other hand, the privacy of end-users.},
booktitle = {Proceedings of the 4th ACM Conference on Advances in Financial Technologies},
pages = {254–267},
numpages = {14},
keywords = {mixing, cryptoassets, CoinJoin},
location = {Cambridge, MA, USA},
series = {AFT '22}
}

@inproceedings{10.1145/3593434.3593441,
author = {Matthies, Christoph and Heinrich, Robert and Wohlrab, Rebekka},
title = {Investigating Software Engineering Artifacts in DevOps Through the Lens of Boundary Objects},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593441},
doi = {10.1145/3593434.3593441},
abstract = {Software engineering artifacts are central to DevOps, enabling the collaboration of teams involved with integrating the development and operations domains. However, collaboration around DevOps artifacts has yet to receive detailed research attention. We apply the sociological concept of Boundary Objects to describe and evaluate the specific software engineering artifacts that enable a cross-disciplinary understanding. Using this focus, we investigate how different DevOps stakeholders can collaborate efficiently using common artifacts. We performed a multiple case study and conducted twelve semi-structured interviews with DevOps practitioners in nine companies. We elicited participants’ collaboration practices, focusing on the coordination of stakeholders and the use of engineering artifacts as a means of translation. This paper presents a consolidated overview of four categories of DevOps Boundary Objects and eleven stakeholder groups relevant to DevOps. To help practitioners assess cross-disciplinary knowledge management strategies, we detail how DevOps Boundary Objects contribute to four areas of DevOps knowledge and propose derived dimensions to evaluate their use.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Software Engineering Artifacts, Boundary Objects, Agile Software Development, Knowledge Management, DevOps},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1145/2592798.2592801,
author = {Anderson, Jonathan and Watson, Robert N. M. and Chisnall, David and Gudka, Khilan and Marinos, Ilias and Davis, Brooks},
title = {TESLA: Temporally Enhanced System Logic Assertions},
year = {2014},
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2592798.2592801},
doi = {10.1145/2592798.2592801},
abstract = {Large, complex, rapidly evolving pieces of software such as operating systems are notoriously difficult to prove correct. Developers instead describe expected behaviour through assertions and check actual behaviour through testing. However, many dynamic safety properties cannot be validated this way as they are temporal: they depend on events in the past or future and are not easily expressed in assertions.TESLA is a description, analysis, and validation tool that allows systems programmers to describe expected temporal behaviour in low-level languages such as C. Temporal assertions can span the interfaces between libraries and even languages. TESLA exposes run-time behaviour using program instrumentation, illuminating coverage of complex state machines and detecting violations of specifications.We apply TESLA to complex software, including an OpenSSL security API, the FreeBSD Mandatory Access Control framework, and GNUstep's rendering engine. With performance allowing "always-on" availability, we demonstrate that existing systems can benefit from richer dynamic analysis without being re-written for amenability to a complete formal analysis.},
booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
articleno = {19},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings{10.1145/3313831.3376142,
author = {Gorski, Peter Leo and Acar, Yasemin and Lo Iacono, Luigi and Fahl, Sascha},
title = {Listen to Developers! A Participatory Design Study on Security Warnings for Cryptographic APIs},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376142},
doi = {10.1145/3313831.3376142},
abstract = {The positive effect of security information communicated to developers through API warnings has been established. However, current prototypical designs are based on security warnings for end-users. To improve security feedback for developers, we conducted a participatory design study with 25 professional software developers in focus groups. We identify which security information is considered helpful in avoiding insecure cryptographic API use during development. Concerning console messages, participants suggested five core elements, namely message classification, title message, code location, link to detailed external resources, and color. Design guidelines for end-user warnings are only partially suitable in this context. Participants emphasized the importance of tailoring the detail and content of security information to the context. Console warnings call for concise communication; further information needs to be linked externally. Therefore, security feedback should transcend tools and should be adjustable by software developers across development tools, considering the work context and developer needs.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {focus groups, security warning design, participatory design, software development, cryptographic apis, developer console},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/2933242.2933248,
author = {de Freitas, Adrian A. and Nebeling, Michael and Ranithangam, Akshaye Shreenithi Kirupa Karthikeyan and Yang, Junrui and Dey, Anind K.},
title = {Bluewave: Enabling Opportunistic Context Sharing via Bluetooth Device Names},
year = {2016},
isbn = {9781450343220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933242.2933248},
doi = {10.1145/2933242.2933248},
abstract = {Context-aware applications oftentimes require devices to share a user's context in order to provide them with relevant information and services. However, current context-sharing techniques require significant amounts of setup before they can be used, making them cumbersome when devices need to share information once or spontaneously. To address this problem, we present Bluewave, a Bluetooth-based technique that allows devices to opportunistically share context when they are nearby. With Bluewave, devices upload context to a trusted server, and extend their Bluetooth name with a URL and a set of temporary credentials. Other devices can obtain this information via Bluetooth discovery, and use it to request and receive context without having to pair. Bluewave provides a simple but effective way for users to share context through their mobile devices, supporting applications where the environment needs to collect information about the user. Our system's design has been guided by user feedback, and integrates privacy controls to let users manage how their context is being shared. In this paper, we describe Bluewave's architecture, and show how it can be used to create a wide range of "just in time" services. We argue that Bluewave's low battery consumption, combined with its speed and compatibility with existing devices, significantly reduces the cost of sharing context, and provides a practical way to create, deploy, and prototype a new generation of context-aware applications.},
booktitle = {Proceedings of the 8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {38–49},
numpages = {12},
keywords = {mobile interaction, ubiquitous computing},
location = {Brussels, Belgium},
series = {EICS '16}
}

@article{10.1145/3544559,
author = {Bik, Aart and Koanantakool, Penporn and Shpeisman, Tatiana and Vasilache, Nicolas and Zheng, Bixia and Kjolstad, Fredrik},
title = {Compiler Support for Sparse Tensor Computations in MLIR},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3544559},
doi = {10.1145/3544559},
abstract = {Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This article discusses integrating this idea into MLIR.},
journal = {ACM Trans. Archit. Code Optim.},
month = {sep},
articleno = {50},
numpages = {25},
keywords = {machine learning, Compilers, tensor algebra, sparse data structures}
}

@article{10.1145/1131322.1131333,
author = {Silva, Dilma Da and Krieger, Orran and Wisniewski, Robert W. and Waterland, Amos and Tam, David and Baumann, Andrew},
title = {K42: An Infrastructure for Operating System Research},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/1131322.1131333},
doi = {10.1145/1131322.1131333},
abstract = {K42 is an open-source scalable research operating system well suited to support systems research. The primary goals of K42's design that support such research include flexibility to allow a multitude of policies and implementations to be supported simultaneously, extensibility to allow new policies and implementations to be readily added, and scalability to enable good performance for both small and large applications on both small and large multiprocessor systems. The goals are accomplished via key features including an object-oriented structure that allows specialized resource management implementations and policies on a per-resource, per-application basis, implementation in user-level servers of much of the system functionality, and a sophisticated set of underlying services that provides a programming model for developing system software in a scalable and modular fashion.These characteristics make K42 an attractive framework for prototyping new operating system ideas. In addition, K42 has a sophisticated performance monitoring infrastructure allowing a thorough understanding of new ideas to be gained. The above framework combined with a consistent emphasis on scalability makes K42 well suited for high-end computing initiatives. In this paper, we describe the structure of K42 which contributes to the advantageous prototyping environment, and demonstrate how to utilize it by describing ongoing research efforts.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {34–42},
numpages = {9}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@article{10.1145/1416563.1416567,
author = {Xie, Qing and Memon, Atif M},
title = {Using a Pilot Study to Derive a GUI Model for Automated Testing},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1416563.1416567},
doi = {10.1145/1416563.1416567},
abstract = {Graphical user interfaces (GUIs) are one of the most commonly used parts of today's software. Despite their ubiquity, testing GUIs for functional correctness remains an understudied area. A typical GUI gives many degrees of freedom to an end-user, leading to an enormous input event interaction space that needs to be tested. GUI test designers generate and execute test cases (modeled as sequences of user events) to traverse its parts; targeting a subspace in order to maximize fault detection is a nontrivial task. In this vein, in previous work, we used informal GUI code examination and personal intuition to develop an event-interaction graph (EIG). In this article we empirically derive the EIG model via a pilot study, and the resulting EIG validates our intuition used in previous work; the empirical derivation process also allows for model evolution as our understanding of GUI faults improves. Results of the pilot study show that events interact in complex ways; a GUI's response to an event may vary depending on the context established by preceding events and their execution order. The EIG model helps testers to understand the nature of interactions between GUI events when executed in test cases and why certain events detect faults, so that they can better traverse the event space. New test adequacy criteria are defined for the EIG; new algorithms use these criteria and EIG to systematically generate test cases that are shown to be effective on four fielded open-source applications.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {7},
numpages = {35},
keywords = {test minimization, model-based testing, test suite management, Graphical user interfaces}
}

@proceedings{10.1145/3592307,
title = {ICECC '23: Proceedings of the 2023 6th International Conference on Electronics, Communications and Control Engineering},
year = {2023},
isbn = {9798400700002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fukuoka, Japan}
}

@article{10.1145/3368087,
author = {Riganelli, Oliviero and Micucci, Daniela and Mariani, Leonardo},
title = {Controlling Interactions with Libraries in Android Apps Through Runtime Enforcement},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3368087},
doi = {10.1145/3368087},
abstract = {Android applications are executed on smartphones equipped with a variety of resources that must be properly accessed and controlled, otherwise the correctness of the executions and the stability of the entire environment might be negatively affected. For example, apps must properly acquire, use, and release microphones, cameras, and other multimedia devices, otherwise the behavior of the apps that use the same resources might be compromised.Unfortunately, several apps do not use resources correctly, for instance, due to faults and inaccurate design decisions. By interacting with these apps, users may experience unexpected behaviors, which in turn may cause instability and sporadic failures, especially when resources are accessed.In this article, we present an approach that lets users protect their environment from the apps that use resources improperly by enforcing the correct usage protocol. This is achieved by using software enforcers that can observe executions and change them when necessary. For instance, enforcers can detect that a resource has been acquired but not released and automatically perform the release operation, thus giving the possibility to use that same resource to the other apps.The main idea is that software libraries, in particular, the ones controlling access to resources, can be augmented with enforcers that can be activated and deactivated on demand by users to protect their environment from unwanted app behaviors. We call the software libraries augmented with one or more enforcers proactive libraries, because the activation of the enforcer decorates the library with proactive behaviors that can guarantee the correctness of the execution despite the invocation of the operations implemented by the library. For example, enforcers can detect that a resource has not been released on time and proactively release it.Our experimental results with 27 possible misuses of resources in real Android apps reveal that proactive libraries are able to effectively correct library misuses with negligible runtime overheads.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {dec},
articleno = {8},
numpages = {29},
keywords = {resource usage, policy enforcement, Proactive library, self-healing, runtime enforcement, resource leak, Android}
}

@inproceedings{10.1145/2543882.2543884,
author = {Utting, Ian and Tew, Allison Elliott and McCracken, Mike and Thomas, Lynda and Bouvier, Dennis and Frye, Roger and Paterson, James and Caspersen, Michael and Kolikant, Yifat Ben-David and Sorva, Juha and Wilusz, Tadeusz},
title = {A Fresh Look at Novice Programmers' Performance and Their Teachers' Expectations},
year = {2013},
isbn = {9781450326650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2543882.2543884},
doi = {10.1145/2543882.2543884},
abstract = {This paper describes the results of an ITiCSE working group convened in 2013 to review and revisit the influential ITiCSE 2001 McCracken working group that reported [18] on novice programmers' ability to solve a specified programming problem. Like that study, the one described here asked students to implement a simple program. Unlike the original study, students' in this study were given significant scaffolding for their efforts, including a test harness. Their knowledge of programming concepts was also assessed via a standard language-neutral survey.One of the significant findings of the original working group was that students were less successful at the programming task than their teachers expected, so in this study teachers' expectations were explicitly gathered and matched with students' performance. This study found a significant correlation between students' performance in the practical task and the survey, and a significant effect on performance in the practical task attributable to the use of the test harness. The study also found a much better correlation between teachers' expectations of their students' performance than in the 2001 working group.},
booktitle = {Proceedings of the ITiCSE Working Group Reports Conference on Innovation and Technology in Computer Science Education-Working Group Reports},
pages = {15–32},
numpages = {18},
keywords = {replication, programming, assessment, CS1},
location = {Canterbury, England, United Kingdom},
series = {ITiCSE -WGR '13}
}

@inproceedings{10.1145/3540250.3558956,
author = {Leesatapornwongsa, Tanakorn and Ren, Xiang and Nath, Suman},
title = {FlakeRepro: Automated and Efficient Reproduction of Concurrency-Related Flaky Tests},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558956},
doi = {10.1145/3540250.3558956},
abstract = {Flaky tests, which can non-deterministically pass or fail on the same code, impose significant burden on developers by providing misleading signals during regression testing. Microsoft developers consider flaky tests as one of the top two reasons for slowing down software development. In order to debug the root-cause of a flaky behavior, a developer often needs to first reliably reproduce a failed execution. Unfortunately, this is non-trivial. For example, most of the flakiness in unit tests are caused by concurrency, and reproducing their failures requires specific thread interleaving. To address this challenge, we introduce FlakeRepro that helps developers reproduce a failed execution of a flaky test caused by concurrency. FlakeRepro combines static and dynamic analysis to quickly identify an interleaving that makes a flaky test fail with the same original error message. FlakeRepro is efficient: it can reproduce a failed execution after exploring few tens of interleavings. FlakeRepro integrates well with existing systems: it automatically instruments test binaries that can run on existing and unmodified test pipelines. We have implemented FlakeRepro for .NET and used it in Microsoft. In an experiment with 22 Microsoft projects, FlakeRepro could reproduce 26 of total 31 concurrent-related flaky tests, after exploring only &lt;7 interleavings and within 6 minutes on average.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1509–1520},
numpages = {12},
keywords = {debugging and fault localization, software testing, flaky tests, program analysis},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3570923,
author = {Kaczorowski, Maya and Momot, Falcon and Neville-Neil, George V. and McCubbin, Chris},
title = {OSS Supply-Chain Security: What Will It Take?},
year = {2022},
issue_date = {September/October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {1542-7730},
url = {https://doi.org/10.1145/3570923},
doi = {10.1145/3570923},
abstract = {While enterprise security teams naturally tend to turn their focus primarily to direct attacks on their own infrastructure, cybercrime exploits now are increasingly aimed at easier targets upstream. This has led to a perfect storm, since virtually all significant codebase repositories at this point include at least some amount of open-source software. But opportunities also abound there for the authors of malware. The broader cybercrime world, meanwhile, has noted that open-source supply chains are generally easy to penetrate. What's being done at this point to address the apparent risks?},
journal = {Queue},
month = {nov},
pages = {86–102},
numpages = {17}
}

@inproceedings{10.1145/2988336.2988348,
author = {Schermann, Gerald and Sch\"{o}ni, Dominik and Leitner, Philipp and Gall, Harald C.},
title = {Bifrost: Supporting Continuous Deployment with Automated Enactment of Multi-Phase Live Testing Strategies},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988348},
doi = {10.1145/2988336.2988348},
abstract = {Live testing is used in the context of continuous delivery and deployment to test changes or new features in the production environment. This includes canary releases, dark launches, A/B tests, and gradual rollouts. Oftentimes, multiple of these live testing practices need to be combined (e.g., running an A/B test after a dark launch). Manually administering such multi-phase live testing strategies is a daunting task for developers or release engineers. In this paper, we introduce a formal model for multi-phase live testing, and present Bifrost as a Node.js based prototype implementation that allows developers to define and automatically enact complex live testing strategies. We extensively evaluate the runtime behavior of Bifrost in three rollout scenarios of a microservice-based case study application, and conclude that the performance overhead of our prototype is at or below 8 ms for most scenarios. Further, we show that more than 100 parallel strategies can be enacted even on cheap public cloud instances.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {12},
numpages = {14},
keywords = {Canary Releases, Release Engineering, Continuous Deployment, A/B Testing, Microservices},
location = {Trento, Italy},
series = {Middleware '16}
}

@inproceedings{10.1145/3517208.3523751,
author = {Berlakovich, Felix and Neugschwandtner, Matthias and Barany, Gerg\"{o}},
title = {Look Ma, No Constants: Practical Constant Blinding in GraalVM},
year = {2022},
isbn = {9781450392556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517208.3523751},
doi = {10.1145/3517208.3523751},
abstract = {With the advent of JIT compilers, code-injection attacks have seen a revival in the form of JIT spraying. JIT spraying enables an attacker to inject gadgets into executable memory, effectively sidestepping W⊕X and ASLR.In response to JIT spraying, constant blinding has emerged as a conceptually straightforward and performance friendly defense. Unfortunately, increasingly sophisticated attacks have pinpointed the shortcomings of existing constant blinding implementations.In this paper we present our constant blinding implementation in the GraalVM compiler, enabling constant blinding across a wide range of languages. Our implementation takes insights from the last decade of research on the security of constant blinding into account. We discuss important design decisions and trade-offs as well as the practical implementation issues encountered when implementing constant blinding for GraalVM. We evaluate the performance impact of our implementation with different configurations and demonstrate its effectiveness by fuzzing for unblinded constants.},
booktitle = {Proceedings of the 15th European Workshop on Systems Security},
pages = {36–42},
numpages = {7},
keywords = {JIT compilation, JIT spraying, constant blinding, language runtimes},
location = {Rennes, France},
series = {EuroSec '22}
}

@inproceedings{10.1145/2618168.2618182,
author = {Lapa, Joaquim and Bernardino, Jorge and Figueiredo, Ana},
title = {A Comparative Analysis of Open Source Business Intelligence Platforms},
year = {2014},
isbn = {9781450327138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618168.2618182},
doi = {10.1145/2618168.2618182},
abstract = {Business Intelligence (BI) platforms are applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, improve competitiveness, and fostering customer relationships. However, we notice that BI is not yet a reality in Small and Medium Enterprises (SMEs) and it's even unknown in many of these organizations. The implementation of BI in organizations depends on their specificities. It is essential that the information available on platforms and their functionality comes clearly to organizations. Only a correct choice, in response to business needs, will allow powerful data that results in gains and business success. For this purpose, we have developed a comparative analysis of the existing capabilities in the various BI tools, which is intended to assist the selection of the BI platform best suited to each organization and/or business area. In this paper, we study and compare seven of the most used open source BI platforms: Actuate, JasperSoft, OpenI, Palo, Pentaho, SpagoBI and Vanilla.},
booktitle = {Proceedings of the International Conference on Information Systems and Design of Communication},
pages = {86–92},
numpages = {7},
keywords = {business intelligence, BI platforms, open source BI tools},
location = {Lisbon, Portugal},
series = {ISDOC '14}
}

@inproceedings{10.1145/3557989.3566157,
author = {Wang, Boyu and Hess, Vincent and Crooks, Andrew},
title = {Mesa-Geo: A GIS Extension for the Mesa Agent-Based Modeling Framework in Python},
year = {2022},
isbn = {9781450395373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557989.3566157},
doi = {10.1145/3557989.3566157},
abstract = {Mesa is an open-source agent-based modeling (ABM) framework implemented in the Python programming language, allowing users to build and visualize agent-based models. It has been used in a diverse range of application areas over the years ranging from biology to workforce dynamics. However, there has been no direct support for integrating geographical data from geographical information systems (GIS) into models created with Mesa. Users have had to rely on their own implementations to meet such needs. In this paper we present Mesa-Geo, a GIS extension for Mesa, which allows users to import, manipulate, visualise and export geographical data for ABM. We introduce the main components and functionalities of Mesa-Geo, followed by example applications utilizing geographical data which demonstrates Mesa-Geo's core functionalities and features common to agent-based models. Finally, we conclude with a discussion and outlook on future directions for Mesa-Geo.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
pages = {1–10},
numpages = {10},
keywords = {complex systems, Python, geographic information systems (GIS), agent-based modeling (ABM)},
location = {Seattle, Washington},
series = {GeoSim '22}
}

@inproceedings{10.5555/2422356.2422384,
author = {Henry, Joseph and Shum, Hubert P. H. and Komura, Taku},
title = {Environment-Aware Real-Time Crowd Control},
year = {2012},
isbn = {9783905674378},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {Real-time crowd control has become an important research topic due to the recent advancement in console game quality and hardware processing capability. The degrees of freedom of a crowd is much higher than that provided by a standard user input device. As a result most crowd control systems require the user to design the crowd movements through multiple passes, such as first specifying the crowd's start and goal points, then providing the agent trajectories with streamlines. Such a multi-pass control would spoil the responsiveness and excitement of real-time games. In this paper, we propose a new, single-pass algorithm to control crowds using a deformable mesh. When controlling crowds, we observe that most of the low level details are related to passive interactions between the crowd and the environment, such as obstacle avoidance and diverging/merging at cross points. Therefore, we simplify the crowd control problem by representing the crowd with a deformable mesh that passively reacts to the environment. As a result, the user can focus on high level control that is more important for context delivery. Our algorithm provides an efficient crowd control framework while maintaining the quality of the simulation, which is useful for real-time applications such as strategy games.},
booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {193–200},
numpages = {8},
location = {Lausanne, Switzerland},
series = {SCA '12}
}

@inproceedings{10.1145/2983323.2983647,
author = {Lee, Haejoon and Kang, Minseo and Youn, Sun-Bum and Lee, Jae-Gil and Kwon, YongChul},
title = {An Experimental Comparison of Iterative MapReduce Frameworks},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983647},
doi = {10.1145/2983323.2983647},
abstract = {MapReduce has become a dominant framework in big data analysis, and thus there have been significant efforts to implement various data analysis algorithms in MapReduce. Many data analysis algorithms are inherently iterative, repeating the same set of tasks until a convergence. To efficiently support iterative algorithms at scale, a few variants of Hadoop and new platforms have been proposed and actively developed in both academia and industry. Representative systems include HaLoop, iMapReduce, Twister, and Spark. In this paper, we experimentally compare Hadoop and the aforementioned systems using various workloads and metrics. The five systems are compared through four iterative algorithms---PageRank, recursive query, k-means, and logistic regression---on 50 Amazon EC2 machines (200 cores in total). We thoroughly explore the effectiveness of their new caching, communication, and scheduling mechanisms in support of iterative computation. Our evaluation also shows the performance depending on data skewness and memory residency. Overall, we believe that our evaluation and interpretation will be useful for designing a new framework or improving the existing ones.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {2089–2094},
numpages = {6},
keywords = {iterative algorithms, spark, hadoop, mapreduce, benchmark},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/1836049.1836054,
author = {Stewart, John A. and Dumoulin, Sarah and No\"{e}l, Sylvie},
title = {A Decade of NML Networked Graphics},
year = {2010},
isbn = {9781450302098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1836049.1836054},
doi = {10.1145/1836049.1836054},
abstract = {This paper is a summation of a decade of support for X3D, human-computer Interaction, and networked graphics that occurred at the Networked Media Laboratory, Communications Research Centre, Canada.},
booktitle = {Proceedings of the 15th International Conference on Web 3D Technology},
pages = {27–34},
numpages = {8},
keywords = {FreeWRL, virtual reality, peer to peer, shared virtual worlds, multicast, CRC, X3D, NML},
location = {Los Angeles, California},
series = {Web3D '10}
}

@proceedings{10.1145/3594739,
title = {UbiComp/ISWC '23 Adjunct: Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cancun, Quintana Roo, Mexico}
}

@inproceedings{10.1145/1160633.1160922,
author = {Modi, Pragnesh Jay and Mancoridis, Spiros and Mongan, William M. and Regli, William and Mayk, Israel},
title = {Towards a Reference Model for Agent-Based Systems},
year = {2006},
isbn = {1595933034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1160633.1160922},
doi = {10.1145/1160633.1160922},
abstract = {The current state of the art in agent technology sees that several implementations of agent frameworks exist. However, there is little agreement on the terms and concepts used to describe such systems, which is a significant barrier towards adoption of these technologies by industry, military and commercial entities. A clear definition of terms and concepts at an appropriate level of abstraction is needed to facilitate discussion, evaluation and adoption of these emerging agent technologies. In this paper, we argue that a reference model for agent-based systems can fill this need. We discuss what a reference model is, why one is needed for agent-based systems, and our proposed methodology for creating such a reference model. While the complete model is a work in progress, we present a preliminary version to motivate further discussion from the agents community at large. It is our hope that ultimately a wider community of practice will assume responsibility for the standardization similar to the way that the well-known seven-layer Open Systems Interconnection (OSI) reference model was a driving force underlying communications standards.},
booktitle = {Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {1475–1482},
numpages = {8},
keywords = {reference model, intelligent agents},
location = {Hakodate, Japan},
series = {AAMAS '06}
}

@inproceedings{10.1145/3524842.3528440,
author = {Ciniselli, Matteo and Pascarella, Luca and Bavota, Gabriele},
title = {To What Extent Do Deep Learning-Based Code Recommenders Generate Predictions by Cloning Code from the Training Set?},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528440},
doi = {10.1145/3524842.3528440},
abstract = {Deep Learning (DL) models have been widely used to support code completion. These models, once properly trained, can take as input an incomplete code component (e.g., an incomplete function) and predict the missing tokens to finalize it. GitHub Copilot is an example of code recommender built by training a DL model on millions of open source repositories: The source code of these repositories acts as training data, allowing the model to learn "how to program". The usage of such a code is usually regulated by Free and Open Source Software (FOSS) licenses, that establish under which conditions the licensed code can be redistributed or modified. As of Today, it is unclear whether the code generated by DL models trained on open source code should be considered as "new" or as "derivative" work, with possible implications on license infringements. In this work, we run a large-scale study investigating the extent to which DL models tend to clone code from their training set when recommending code completions. Such an exploratory study can help in assessing the magnitude of the potential licensing issues mentioned before: If these models tend to generate new code that is unseen in the training set, then licensing issues are unlikely to occur. Otherwise, a revision of these licenses urges to regulate how the code generated by these models should be treated when used, for example, in a commercial setting. Highlights from our results show that ~10% to ~0.1% of the predictions generated by a state-of-the-art DL-based code completion tool are Type-1 clones of instances in the training set, depending on the size of the predicted code. Long predictions are unlikely to be cloned.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {167–178},
numpages = {12},
keywords = {code clones, deep learning, code completion},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@proceedings{10.1145/3528535,
title = {Middleware '22: Proceedings of the 23rd ACM/IFIP International Middleware Conference},
year = {2022},
isbn = {9781450393409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Quebec, QC, Canada}
}

@article{10.1109/TNET.2006.886289,
author = {Baughman, Nathaniel E. and Liberatore, Marc and Levine, Brian Neil},
title = {Cheat-Proof Playout for Centralized and Peer-to-Peer Gaming},
year = {2007},
issue_date = {February 2007},
publisher = {IEEE Press},
volume = {15},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2006.886289},
doi = {10.1109/TNET.2006.886289},
abstract = {We explore exploits possible for cheating in real-time, multiplayer games for both client-server and serverless architectures. We offer the first formalization of cheating in online games and propose an initial set of strong solutions. We propose a protocol that has provable anti-cheating guarantees, is provably safe and live, but suffers a performance penalty. We then develop an extended version of this protocol, called asynchronous synchronization, which avoids the penalty, is serverless, offers provable anti-cheating guarantees, is robust in the presence of packet loss, and provides for significantly increased communication performance. This technique is applicable to common game features as well as clustering and cell-based techniques for massively multiplayer games. Specifically, we provide a zero-knowledge proof protocol so that players are within a specific range of each other, and otherwise have no notion of their distance. Our performance claims are backed by analysis using a simulation based on real game traces.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {1–13},
numpages = {13},
keywords = {peer-to-peer networking, multimedia communication, gaming, security}
}

@inproceedings{10.1145/3287324.3287508,
author = {Davis, Janet and Rebelsky, Samuel A.},
title = {Developing Soft and Technical Skills Through Multi-Semester, Remotely Mentored, Community-Service Projects},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287508},
doi = {10.1145/3287324.3287508},
abstract = {For the past four years, we have taught a reimagined software design course with typical and atypical components. Projects form the core of the course: Students work in teams of 4-6 people to develop non-mission-critical software for local non-profits, building their engagement with the community and helping them understand the broader impact of their work. These projects require multiple semesters to complete. Since students typically enroll for the course for one semester, this model gives students the novel experience of legacy software. We also provide each team with an alumni mentor who helps them navigate not only technical problems but also the challenges of working with a real-world, non-technical client. These aspects of the course also develop our students' soft skills. They learn to work with a team, to communicate with non-technical clients, to work with remote collaborators (or mentors), and to think ahead to those who will take on the project in the next semester. As we tell our students, these skills are often as crucial as their technical skills. In this paper, we report on the design of the course and describe some of the challenges associated with this model (e.g., projects that inadvertently reveal information, clients who switch management or expectations, and projects that become obsolete) and provide suggestions for those who might want to adopt a similar approach.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {29–35},
numpages = {7},
keywords = {soft skills, software engineering, alumni engagement, ruby on rails, community service, software design},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@article{10.1145/3572901,
author = {Lubbers, Mart and Koopman, Pieter and Ramsingh, Adrian and Singer, Jeremy and Trinder, Phil},
title = {Could Tierless Languages Reduce IoT Development Grief?},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3572901},
doi = {10.1145/3572901},
abstract = {Internet of Things (IoT) software is notoriously complex, conventionally comprising multiple tiers. Traditionally an IoT developer must use multiple programming languages and ensure that the components interoperate correctly. A novel alternative is to use a single tierless language with a compiler that generates the code for each component and ensures their correct interoperation.We report a systematic comparative evaluation of two tierless language technologies for IoT stacks: one for resource-rich sensor nodes (Clean with iTask) and one for resource-constrained sensor nodes (Clean with iTask and mTask). The evaluation is based on four implementations of a typical smart campus application: two tierless and two Python-based tiered.(1) We show that tierless languages have the potential to significantly reduce the development effort for IoT systems, requiring 70% less code than the tiered implementations. Careful analysis attributes this code reduction to reduced interoperation (e.g., two embedded domain-specific languages and one paradigm versus seven languages and two paradigms), automatically generated distributed communication, and powerful IoT programming abstractions. (2) We show that tierless languages have the potential to significantly improve the reliability of IoT systems, describing how Clean iTask/mTask maintains type safety, provides higher-order failure management, and simplifies maintainability. (3) We report the first comparison of a tierless IoT codebase for resource-rich sensor nodes with one for resource-constrained sensor nodes. The comparison shows that they have similar code size (within 7%), and functional structure. (4) We present the first comparison of two tierless IoT languages, one for resource-rich sensor nodes and the other for resource-constrained sensor nodes.},
journal = {ACM Trans. Internet Things},
month = {feb},
articleno = {6},
numpages = {35},
keywords = {Tierless languages, IoT stacks}
}

@inproceedings{10.1145/3510003.3510112,
author = {Schumi, Richard and Sun, Jun},
title = {ExAIS: Executable AI Semantics},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510112},
doi = {10.1145/3510003.3510112},
abstract = {Neural networks can be regarded as a new programming paradigm, i.e., instead of building ever-more complex programs through (often informal) logical reasoning in the programmers' mind, complex 'AI' systems are built by optimising generic neural network models with big data. In this new paradigm, AI frameworks such as TensorFlow and PyTorch play a key role, which is as essential as the compiler for traditional programs. It is known that the lack of a proper semantics for programming languages (such as C), i.e., a correctness specification for compilers, has contributed to many problematic program behaviours and security issues. While it is in general hard to have a correctness specification for compilers due to the high complexity of programming languages and their rapid evolution, we have a unique opportunity to do it right this time for neural networks (which have a limited set of functions, and most of them have stable semantics). In this work, we report our effort on providing a correctness specification of neural network frameworks such as TensorFlow. We specify the semantics of almost all TensorFlow layers in the logical programming language Prolog. We demonstrate the usefulness of the semantics through two applications. One is a fuzzing engine for TensorFlow, which features a strong oracle and a systematic way of generating valid neural networks. The other is a model validation approach which enables consistent bug reporting for TensorFlow models.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {859–870},
numpages = {12},
keywords = {deep learning models, AI model generation, AI libraries, AI frameworks, test case generation, specification, semantics, model validation},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3320269.3384725,
author = {B\"{u}rstinghaus-Steinbach, Kevin and Krau\ss{}, Christoph and Niederhagen, Ruben and Schneider, Michael},
title = {Post-Quantum TLS on Embedded Systems: Integrating and Evaluating Kyber and SPHINCS+ with Mbed TLS},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384725},
doi = {10.1145/3320269.3384725},
abstract = {We present our integration of post-quantum cryptography (PQC), more specifically of the post-quantum KEM scheme Kyber for key establishment and the post-quantum signature scheme SPHINCS+, into the embedded TLS library mbed TLS. We measure the performance of these post-quantum primitives on four different embedded platforms with three different ARM processors and an Xtensa LX6 processor. Furthermore, we compare the performance of our experimental PQC cipher suite to a classical TLS variant using elliptic curve cryptography (ECC). Post-quantum key establishment and signature schemes have been either integrated into TLS or ported to embedded devices before. However, to the best of our knowledge, we are the first to combine TLS, post-quantum schemes, and embedded systems and to measure and evaluate the performance of post-quantum TLS on embedded platforms. Our results show that post-quantum key establishment with Kyber performs well in TLS on embedded devices compared to ECC variants. The use of SPHINCS+ signatures comes with certain challenges in terms of signature size and signing time, which mainly affects the use of embedded systems as PQC-TLS server but does not necessarily prevent embedded systems to act as PQC-TLS clients.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {841–852},
numpages = {12},
keywords = {embedded systems, kyber, mbed tls, pqc, tls, sphincs+},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/3274783.3274839,
author = {Kim, Hyung-Sin and Andersen, Michael P. and Chen, Kaifei and Kumar, Sam and Zhao, William J. and Ma, Kevin and Culler, David E.},
title = {System Architecture Directions for Post-SoC/32-Bit Networked Sensors},
year = {2018},
isbn = {9781450359528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274783.3274839},
doi = {10.1145/3274783.3274839},
abstract = {The emergence of low-power 32-bit Systems-on-Chip (SoCs), which integrate a 32-bit MCU, radio, and flash, presents an opportunity to re-examine design points and trade-offs at all levels of the system architecture of networked sensors. To this end, we develop a post-SoC/32-bit design point called Hamilton, showing that using integrated components enables a ~$7 core and shifts hardware modularity to design time. We study the interaction between hardware and embedded operating systems, identifying that (1) post-SoC motes provide lower idle current (5.9 μA) than traditional 16-bit motes, (2) 32-bit MCUs are a major energy consumer (e.g., tick increases idle current &gt;50 times), comparable to radios, and (3) thread-based concurrency is viable, requiring only 8.3 μs of context switch time. We design a system architecture, based on a tickless multithreading operating system, with cooperative/adaptive clocking, advanced sensor abstraction, and preemptive packet processing. Its efficient MCU control improves concurrency with ~30% less energy consumption. Together, these developments set the system architecture for networked sensors in a new direction.},
booktitle = {Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems},
pages = {264–277},
numpages = {14},
keywords = {System-on-Chip, RIOT, Operating System, OpenThread, Multithreading, Mote, System Architecture, Wireless Sensor Network},
location = {Shenzhen, China},
series = {SenSys '18}
}

@inproceedings{10.1145/3213846.3213865,
author = {Lyu, Yingjun and Li, Ding and Halfond, William G. J.},
title = {Remove RATs from Your Code: Automated Optimization of Resource Inefficient Database Writes for Mobile Applications},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213865},
doi = {10.1145/3213846.3213865},
abstract = {Developers strive to build feature-filled apps that are responsive and consume as few resources as possible. Most of these apps make use of local databases to store and access data locally. Prior work has found that local database services have become one of the major drivers of a mobile device's resource consumption. In this paper we propose an approach to reduce the energy consumption and improve runtime performance of database operations in Android apps by optimizing inefficient database writes. Our approach automatically detects database writes that happen within loops and that will trigger inefficient autocommit behaviors. Our approach then uses additional analyses to identify those that are optimizable and rewrites the code so that it is more efficient. We evaluated our approach on a set of marketplace Android apps and found it could reduce the energy and runtime of events containing the inefficient database writes by 25% to 90% and needed, on average, thirty-six seconds to analyze and transform each app.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {310–321},
numpages = {12},
keywords = {mobile applications, Performance optimization, database},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.5555/776816.776831,
author = {Chan, Keith and Liang, Zhi Cong Leo and Michail, Amir},
title = {Design Recovery of Interactive Graphical Applications},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Nowadays, the majority of productivity applications are interactive and graphical in nature. In this paper, we explore the possibility of taking advantage of these two characteristics in a design recovery tool. Specifically, the fact that an application is interactive means that we can identify distinct execution bursts corresponding closely to "actions" performed by the user. The fact that the application is graphical means that we can describe those actions visually from a fragment of the application display itself. Combining these two ideas, we obtain an explicit mapping from high-level actions performed by a user (similar to use case scenarios/specification fragments) to their low-level implementation. This mapping can be used for design recovery of interactive graphical applications. We demonstrate our approach using LyX, a scientific word processor.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {114–124},
numpages = {11},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/1390630.1390653,
author = {Edwards, Alex and Tucker, Sean and Worms, S\'{e}bastien and Vaidya, Rahul and Demsky, Brian},
title = {AFID: An Automated Fault Identification Tool},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390653},
doi = {10.1145/1390630.1390653},
abstract = {We present the Automatic Fault IDentification Tool (AFID). AFID automatically constructs repositories of real software faults by monitoring the software development process. AFID records both a fault revealing test case and a faulty version of the source code for any crashing faults that the developer discovers and a fault correcting source code change for any crashing faults that the developer corrects. The test cases are a significant contribution, because they enable new research that explores the dynamic behaviors of the software faults.AFID uses a ptrace-based monitoring mechanism to monitor both the compilation and execution of the application. The ptrace-based technique makes it straightforward for AFID to support a wide range of programming languages and compilers. Our benchmark results indicate that the monitoring overhead will be acceptable for most developers. We performed a short case study to evaluate how effectively the AFID tool records software faults. In our case study, AFID recorded 12 software faults from the 8 participants.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {179–188},
numpages = {10},
keywords = {fault collection},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@article{10.1145/3597206,
author = {Huang, Qing and Liao, Dianshu and Xing, Zhenchang and Zuo, Zhengkang and Wang, Changjing and Xia, Xin},
title = {Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597206},
doi = {10.1145/3597206},
abstract = {Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {147},
numpages = {37},
keywords = {knowledge deduction, code knowledge graph, crowd-scale coding practice checking, Smart contract, code recommendation}
}

@book{10.1145/2886107,
author = {Zaharia, Matei},
title = {An Architecture for Fast and General Data Processing on Large Clusters},
year = {2016},
isbn = {9781970001570},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {11},
abstract = {Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines.At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too.This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing.We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective.This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.}
}

@article{10.1145/3512945,
author = {Wurzel Gon\c{c}alves, Pavl\'{\i}na and \c{C}alikli, G\"{u}l and Bacchelli, Alberto},
title = {Interpersonal Conflicts During Code Review: Developers' Experience and Practices},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512945},
doi = {10.1145/3512945},
abstract = {Code review consists of manual inspection, discussion, and judgment of source code by developers other than the code's author. Due to discussions around competing ideas and group decision-making processes, interpersonal conflicts during code reviews are expected. This study systematically investigates how developers perceive code review conflicts and addresses interpersonal conflicts during code reviews as a theoretical construct. Through the thematic analysis of interviews conducted with 22 developers, we confirm that conflicts during code reviews are commonplace, anticipated and seen as normal by developers. Even though conflicts do happen and carry a negative impact for the review, conflicts-if resolved constructively-can also create value and bring improvement. Moreover, the analysis provided insights on how strongly conflicts during code review and its context (i.e., code, developer, team, organization) are intertwined. Finally, there are aspects specific to code review conflicts that call for the research and application of customized conflict resolution and management techniques, some of which are discussed in this paper. Preprint: https://arxiv.org/abs/2201.05425 Data and material: https://doi.org/10.5281/zenodo.5848794},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {98},
numpages = {33},
keywords = {human factors, conflict management, code review, interpersonal conflicts}
}

@inproceedings{10.1145/3453483.3454030,
author = {Lopes, Nuno P. and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},
title = {Alive2: Bounded Translation Validation for LLVM},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454030},
doi = {10.1145/3453483.3454030},
abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {65–79},
numpages = {15},
keywords = {Automatic Software Verification, Translation Validation, IR Semantics, Compilers},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@proceedings{10.1145/3614321,
title = {ICEGOV '23: Proceedings of the 16th International Conference on Theory and Practice of Electronic Governance},
year = {2023},
isbn = {9798400707421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Belo Horizonte</city>, <country>Brazil</country>, </conf-loc>}
}

@inproceedings{10.1145/3338906.3338935,
author = {Koyuncu, Anil and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and Le Traon, Yves},
title = {IFixR: Bug Report Driven Program Repair},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338935},
doi = {10.1145/3338906.3338935},
abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {314–325},
numpages = {12},
keywords = {Information retrieval, automatic patch generation, fault localization},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3377811.3380392,
author = {Alshayban, Abdulaziz and Ahmed, Iftekhar and Malek, Sam},
title = {Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380392},
doi = {10.1145/3377811.3380392},
abstract = {Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1323–1334},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten Years of Product Line Engineering at Danfoss: Lessons Learned and Way Ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {product line adoption, industrial experiences, product line evaluation},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.5555/1924943.1924944,
author = {Boyd-Wickizer, Silas and Clements, Austin T. and Mao, Yandong and Pesterev, Aleksey and Kaashoek, M. Frans and Morris, Robert and Zeldovich, Nickolai},
title = {An Analysis of Linux Scalability to Many Cores},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper analyzes the scalability of seven system applications (Exim, memcached, Apache, PostgreSQL, gmake, Psearchy, and MapReduce) running on Linux on a 48- core computer. Except for gmake, all applications trigger scalability bottlenecks inside a recent Linux kernel. Using mostly standard parallel programming techniques-- this paper introduces one new technique, sloppy counters-- these bottlenecks can be removed from the kernel or avoided by changing the applications slightly. Modifying the kernel required in total 3002 lines of code changes. A speculative conclusion from this analysis is that there is no scalability reason to give up on traditional operating system organizations just yet.},
booktitle = {Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation},
pages = {1–16},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {OSDI'10}
}

@article{10.1145/2856821,
author = {Capraro, Maximilian and Riehle, Dirk},
title = {Inner Source Definition, Benefits, and Challenges},
year = {2016},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2856821},
doi = {10.1145/2856821},
abstract = {Inner Source (IS) is the use of open source software development practices and the establishment of an open source-like culture within organizations. The organization may still develop proprietary software but internally opens up its development. A steady stream of scientific literature and practitioner reports indicates the interest in this research area. However, the research area lacks a systematic assessment of known research work: No model exists that defines IS thoroughly. Various case studies provide insights into IS programs in the context of specific organizations but only few publications apply a broader perspective. To resolve this, we performed an extensive literature survey and analyzed 43 IS related publications plus additional background literature. Using qualitative data analysis methods, we developed a model of the elements that constitute IS. We present a classification framework for IS programs and projects and apply it to lay out a map of known IS endeavors. Further, we present qualitative models summarizing the benefits and challenges of IS adoption. The survey provides the first broad review of IS literature and systematic arrangement of IS research results.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {67},
numpages = {36},
keywords = {software development productivity, software development efficiency, open collaboration, taxonomy, Inner source, software engineering, internal open source, software development methods}
}

@inproceedings{10.1145/2441776.2441828,
author = {Howison, James and Herbsleb, James D.},
title = {Incentives and Integration in Scientific Software Production},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441828},
doi = {10.1145/2441776.2441828},
abstract = {Science policy makers are looking for approaches to increase the extent of collaboration in the production of scientific software, looking to open collaborations in open source software for inspiration. We examine the software ecosystem surrounding BLAST, a key bioinformatics tool, identifying outside improvements and interviewing their authors. We find that academic credit is a powerful motivator for the production and revealing of improvements. Yet surprisingly, we also find that improvements motivated by academic credit are less likely to be integrated than those with other motivations, including financial gain. We argue that this is because integration makes it harder to see who has contributed what and thereby undermines the ability of reputation to function as a reward for collaboration. We consider how open source avoids these issues and conclude with policy approaches to promoting wider collaboration by addressing incentives for integration.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {459–470},
numpages = {12},
keywords = {collaboration, software development, incentive systems, science policy},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@inproceedings{10.1145/3183519.3183524,
author = {Wang, Pei and Wu, Dinghao and Chen, Zhaofeng and Wei, Tao},
title = {Protecting Million-User IOS Apps with Obfuscation: Motivations, Pitfalls, and Experience},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183524},
doi = {10.1145/3183519.3183524},
abstract = {In recent years, mobile apps have become the infrastructure of many popular Internet services. It is now fairly common that a mobile app serves a large number of users across the globe. Different from web-based services whose important program logic is mostly placed on remote servers, many mobile apps require complicated client-side code to perform tasks that are critical to the businesses. The code of mobile apps can be easily accessed by any party after the software is installed on a rooted or jailbroken device. By examining the code, skilled reverse engineers can learn various knowledge about the design and implementation of an app. Real-world cases have shown that the disclosed critical information allows malicious parties to abuse or exploit the app-provided services for unrightful profits, leading to significant financial losses for app vendors.One of the most viable mitigations against malicious reverse engineering is to obfuscate the software before release. Despite that security by obscurity is typically considered to be an unsound protection methodology, software obfuscation can indeed increase the cost of reverse engineering, thus delivering practical merits for protecting mobile apps.In this paper, we share our experience of applying obfuscation to multiple commercial iOS apps, each of which has millions of users. We discuss the necessity of adopting obfuscation for protecting modern mobile business, the challenges of software obfuscation on the iOS platform, and our efforts in overcoming these obstacles. Our report can benefit many stakeholders in the iOS ecosystem, including developers, security service providers, and Apple as the administrator of the ecosystem.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {235–244},
numpages = {10},
keywords = {obfuscation, software protection, mobile, iOS, reverse engineering},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@proceedings{10.1145/3587281,
title = {W4A '23: Proceedings of the 20th International Web for All Conference},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@inproceedings{10.1145/2002259.2002266,
author = {Obweger, Hannes and Schiefer, Josef and Suntinger, Martin and Kepplinger, Peter and Rozsnyai, Szabolcs},
title = {User-Oriented Rule Management for Event-Based Applications},
year = {2011},
isbn = {9781450304238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2002259.2002266},
doi = {10.1145/2002259.2002266},
abstract = {Event-pattern rules are the foundation of Complex Event Processing (CEP) applications. Yet, despite the vast potential CEP offers for agile business applications, its practical relevance rises and falls with the manageability within the organizational framework conditions of an enterprise. In this paper we present a novel rule-management framework for the event-based system UC4 Decision. It caters to the needs of power users as well as business users: Power users model infrastructural rules based on visual decision graphs and a dedicated expression language. Business users compose rule logic in a simplified web interface from abstracted, configurable building blocks.},
booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-Based System},
pages = {39–48},
numpages = {10},
keywords = {rule management, complex event processing, event-pattern rules},
location = {New York, New York, USA},
series = {DEBS '11}
}

@inproceedings{10.1145/2531602.2531659,
author = {Vasilescu, Bogdan and Serebrenik, Alexander and Devanbu, Prem and Filkov, Vladimir},
title = {How Social Q&amp;A Sites Are Changing Knowledge Sharing in Open Source Software Communities},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2531602.2531659},
doi = {10.1145/2531602.2531659},
abstract = {Historically, mailing lists have been the preferred means for coordinating development and user support activities. With the emergence and popularity growth of social Q&amp;A sites such as the StackExchange network (e.g., StackOverflow), this is beginning to change. Such sites offer different socio-technical incentives to their participants than mailing lists do, e.g., rich web environments to store and manage content collaboratively, or a place to showcase their knowledge and expertise more vividly to peers or potential recruiters. A key difference between StackExchange and mailing lists is gamification, i.e., StackExchange participants compete to obtain reputation points and badges. In this paper, we use a case study of R (a widely-used tool for data analysis) to investigate how mailing list participation has evolved since the launch of StackExchange. Our main contribution is the assembly of a joint data set from the two sources, in which participants in both the texttt{r-help} mailing list and StackExchange are identifiable. This permits their activities to be linked across the two resources and also over time. With this data set we found that user support activities show a strong shift away from texttt{r-help}. In particular, mailing list experts are migrating to StackExchange, where their behaviour is different. First, participants active both on texttt{r-help} and on StackExchange are more active than those who focus exclusively on only one of the two. Second, they provide faster answers on StackExchange than on texttt{r-help}, suggesting they are motivated by the emph{gamified} environment. To our knowledge, our study is the first to directly chart the changes in behaviour of specific contributors as they migrate into gamified environments, and has important implications for knowledge management in software engineering.},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {342–354},
numpages = {13},
keywords = {social q&amp;a, open source, crowdsourced knowledge, gamification., mailing lists},
location = {Baltimore, Maryland, USA},
series = {CSCW '14}
}

@inproceedings{10.1145/2976749.2978422,
author = {Bichsel, Benjamin and Raychev, Veselin and Tsankov, Petar and Vechev, Martin},
title = {Statistical Deobfuscation of Android Applications},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978422},
doi = {10.1145/2976749.2978422},
abstract = {This work presents a new approach for deobfuscating Android APKs based on probabilistic learning of large code bases (termed "Big Code"). The key idea is to learn a probabilistic model over thousands of non-obfuscated Android applications and to use this probabilistic model to deobfuscate new, unseen Android APKs. The concrete focus of the paper is on reversing layout obfuscation, a popular transformation which renames key program elements such as classes, packages, and methods, thus making it difficult to understand what the program does. Concretely, the paper: (i) phrases the layout deobfuscation problem of Android APKs as structured prediction in a probabilistic graphical model, (ii) instantiates this model with a rich set of features and constraints that capture the Android setting, ensuring both semantic equivalence and high prediction accuracy, and (iii) shows how to leverage powerful inference and learning algorithms to achieve overall precision and scalability of the probabilistic predictions.We implemented our approach in a tool called DeGuard and used it to: (i) reverse the layout obfuscation performed by the popular ProGuard system on benign, open-source applications, (ii) predict third-party libraries imported by benign APKs (also obfuscated by ProGuard), and (iii) rename obfuscated program elements of Android malware. The experimental results indicate that DeGuard is practically effective: it recovers 79.1% of the program element names obfuscated with ProGuard, it predicts third-party libraries with accuracy of 91.3%, and it reveals string decoders and classes that handle sensitive data in Android malware.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {343–355},
numpages = {13},
keywords = {program deobfuscation, reverse engineering, malware inspection},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/3448734.3450774,
author = {Yang, Jun and He, Ruoyu and Cui, Baojiang},
title = {A GPU Memory Leakage Code Defect Detection Method Based on the API Calling Feature},
year = {2021},
isbn = {9781450389570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448734.3450774},
doi = {10.1145/3448734.3450774},
abstract = {The General-Purpose Graphics Processing Unit (GPGPU) programming has been widely used in artificial intelligence and deep learning, and the GPGPU programming framework, represented by the CUDA framework launched by NIVIDIA Corporation, can apply the powerful parallel computing power of Graphics Processing unit (GPU) to non-graphics tasks. The gradually open computing power of GPU also brings related security risks, but the industry is still mainly concerned with how to dig into the potential security risks of GPU rather than the protection of known problems. In this paper, we propose an API calling feature (ACF) based method for detecting memory leaks in GPU codes and programmatically implement a prototype method to detect the risk of memory data residue in GPU codes written in CUDA framework. The prototype detection method is implemented using the Pass module development capability provided by the LLVM compiler project, and the method is tested to have good accuracy an effectiveness, which can provide a basis for subsequent GPU codes' security research.},
booktitle = {The 2nd International Conference on Computing and Data Science},
articleno = {44},
numpages = {13},
keywords = {features, General-purpose graphics processing unit programming, detection, code defects},
location = {Stanford, CA, USA},
series = {CONF-CDS 2021}
}

@article{10.1145/3586040,
author = {Fox, Anthony C. J. and Stockwell, Gareth and Xiong, Shale and Becker, Hanno and Mulligan, Dominic P. and Petri, Gustavo and Chong, Nathan},
title = {A Verification Methodology for the Arm® Confidential Computing Architecture: From a Secure Specification to Safe Implementations},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586040},
doi = {10.1145/3586040},
abstract = {We present Arm's efforts in verifying the specification and prototype reference implementation of the Realm Management Monitor (RMM), an essential firmware component of Arm Confidential Computing Architecture (Arm CCA), the recently-announced Confidential Computing technologies incorporated in the Armv9-A architecture. Arm CCA introduced the Realm Management Extension (RME), an architectural extension for Armv9-A, and a technology that will eventually be deployed in hundreds of millions of devices. Given the security-critical nature of the RMM, and its taxing threat model, we use a combination of interactive theorem proving, model checking, and concurrency-aware testing to validate and verify security and safety properties of both the specification and a prototype implementation of the RMM. Crucially, our verification efforts were, and are still being, developed and refined contemporaneously with active development of both specification and implementation, and have been adopted by Arm's product teams.  

We describe our major achievements, realized through the application of formal techniques, as well as challenges that remain for future work. We believe that the work reported in this paper is the most thorough application of formal techniques to the design and implementation of any current commercially-viable Confidential Computing implementation, setting a new high-water mark for work in this area.},
journal = {Proc. ACM Program. Lang.},
month = {apr},
articleno = {88},
numpages = {30},
keywords = {Confidential Computing, Arm Confidential Computing Architecture (Arm CCA), separation kernel, operating system verification, formal methods}
}

@inproceedings{10.1145/1435452.1435456,
author = {Rodr\'{\i}guez, Fernando and Freitag, Felix and Navarro, Leandro},
title = {On the Use of Intelligent Local Resource Management for Improved Virtualized Resource Provision: Challenges, Required Features, and an Approach},
year = {2008},
isbn = {9781605581200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1435452.1435456},
doi = {10.1145/1435452.1435456},
abstract = {This work aims to achieve better management of physical resources by dynamically reallocating and adjusting local resources according to demand. The research contribution should be measured through three aspects: administrative with a delegation mechanism to help in the management of Virtual Machine (VM) based large-scale systems, fine-grain allocation with an approach to improve the use of physical resources, and intelligent management with a configurable self-adapting engine aware of the application-level requirements. We take into account that each application has different behavior and requires different hardware and software. These dynamic features in a cluster of VM-based resource providers present a challenge to assign properly, according to certain optimization criteria, the physical resources to the VMs. We evaluate, as a case study of resources management, the automated management of CPU in multiprocessor machines.},
booktitle = {Proceedings of the 2nd Workshop on System-Level Virtualization for High Performance Computing},
pages = {24–31},
numpages = {8},
keywords = {virtualization, local resource manager},
location = {Glasgow, Scotland},
series = {HPCVirt '08}
}

@inproceedings{10.5555/2740769.2740789,
author = {Wu, Zhaohui and Wu, Jian and Khabsa, Madian and Williams, Kyle and Chen, Hung-Hsuan and Huang, Wenyi and Tuarob, Suppawong and Choudhury, Sagnik Ray and Ororbia, Alexander and Mitra, Prasenjit and Giles, C. Lee},
title = {Towards Building a Scholarly Big Data Platform: Challenges, Lessons and Opportunities},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {We introduce a big data platform that provides various services for harvesting scholarly information and enabling efficient scholarly applications. The core architecture of the platform is built on a secured private cloud, crawls data using a scholarly focused crawler that leverages a dynamic scheduler, processes by utilizing a map reduce based crawl-extraction-ingestion (CEI) workflow, and is stored in distributed repositories and databases. Services such as scholarly data harvesting, information extraction, and user information and log data analytics are integrated into the platform and provided by an OAI and RESTful API. We also introduce a set of scholarly applications built on top of this platform including citation recommendation and collaborator discovery.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {117–126},
numpages = {10},
keywords = {scholarly big data, big data, information extraction},
location = {London, United Kingdom},
series = {JCDL '14}
}

@inproceedings{10.1145/2702123.2702216,
author = {Randall, David P. and Diamant, E. Ilana and Lee, Charlotte P.},
title = {Creating Sustainable Cyberinfrastructures},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702216},
doi = {10.1145/2702123.2702216},
abstract = {In this paper we report the results of a qualitative research study of the GENI cyberinfrastructure: a program of four federated cyberinfrastructures. Drawing on theories of stakeholder positioning, we examine how different GENI stakeholders attempt to enlist new participants in the cyberinfrastructures of GENI, and leverage existing relationships to create sustainable infrastructure. This study contributes to our understanding of how cyberinfrastructures emerge over time through processes of stakeholder alignment, enrollment, and through synergies among stakeholder groups. We explore these issues to better understand how cyberinfrastructures can be designed to sustain over time.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1759–1768},
numpages = {10},
keywords = {enrollment, cyberinfrastructures, actor-network theory, synergizing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@article{10.1145/514183.514185,
author = {Fielding, Roy T. and Taylor, Richard N.},
title = {Principled Design of the Modern Web Architecture},
year = {2002},
issue_date = {May 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/514183.514185},
doi = {10.1145/514183.514185},
abstract = {The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer (REST) architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support.},
journal = {ACM Trans. Internet Technol.},
month = {may},
pages = {115–150},
numpages = {36},
keywords = {Network-based applications, REST, World Wide Web}
}

@inproceedings{10.1145/3447786.3456248,
author = {Kuenzer, Simon and B\u{a}doiu, Vlad-Andrei and Lefeuvre, Hugo and Santhanam, Sharan and Jung, Alexander and Gain, Gaulthier and Soldani, Cyril and Lupu, Costin and Teodorescu, \c{S}tefan and R\u{a}ducanu, Costi and Banu, Cristian and Mathy, Laurent and Deaconescu, R\u{a}zvan and Raiciu, Costin and Huici, Felipe},
title = {Unikraft: Fast, Specialized Unikernels the Easy Way},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456248},
doi = {10.1145/3447786.3456248},
abstract = {Unikernels are famous for providing excellent performance in terms of boot times, throughput and memory consumption, to name a few metrics. However, they are infamous for making it hard and extremely time consuming to extract such performance, and for needing significant engineering effort in order to port applications to them. We introduce Unikraft, a novel micro-library OS that (1) fully modularizes OS primitives so that it is easy to customize the unikernel and include only relevant components and (2) exposes a set of composable, performance-oriented APIs in order to make it easy for developers to obtain high performance.Our evaluation using off-the-shelf applications such as nginx, SQLite, and Redis shows that running them on Unikraft results in a 1.7x-2.7x performance improvement compared to Linux guests. In addition, Unikraft images for these apps are around 1MB, require less than 10MB of RAM to run, and boot in around 1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux Foundation open source project and can be found at www.unikraft.org.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {376–394},
numpages = {19},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{10.1145/3517745.3561463,
author = {Davanian, Ali and Faloutsos, Michalis},
title = {MalNet: A Binary-Centric Network-Level Profiling of IoT Malware},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561463},
doi = {10.1145/3517745.3561463},
abstract = {Where are the IoT C2 servers located? What vulnerabilities does IoT malware try to exploit? What DDoS attacks are launched in practice? In this work, we conduct a large scale study to answer these questions. Specifically, we collect and dynamically analyze 1447 malware binaries on the day that they become publicly known between March 2021 and March 2022 from VirusTotal and MalwareBazaar. By doing this, we are able to observe and profile their behavior at the network level including: (a) C2 communication, (b) proliferation, and (c) issued DDoS attacks. Our comprehensive study provides the following key observations. First, we quantify the elusive behavior of C2 servers: 91% of the time a server does not respond to a second probe four hours after a successful probe. In addition, we find that 15% of the live servers that we find are not known by threat intelligence feeds available on VirusTotal. Second, we find that the IoT malware relies on fairly old vulnerabilities in its proliferation. Our binaries attempt to exploit 12 different vulnerabilities with 9 of them more than 4 years old, while the most recent one was 5 months old. Third, we observe the launch of 42 DDoS attacks that span 8 types of attacks, with two types of attacks targeting gaming servers. The promising results indicate the significant value of using a dynamic analysis approach that includes active measurements and probing towards detecting and containing IoT botnets.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {472–487},
numpages = {16},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{10.1145/3338906.3338917,
author = {Zhang, Chen and Chen, Bihuan and Chen, Linlin and Peng, Xin and Zhao, Wenyun},
title = {A Large-Scale Empirical Study of Compiler Errors in Continuous Integration},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338917},
doi = {10.1145/3338906.3338917},
abstract = {Continuous Integration (CI) is a widely-used software development practice to reduce risks. CI builds often break, and a large amount of efforts are put into troubleshooting broken builds. Despite that compiler errors have been recognized as one of the most frequent types of build failures, little is known about the common types, fix efforts and fix patterns of compiler errors that occur in CI builds of open-source projects. To fill such a gap, we present a large-scale empirical study on 6,854,271 CI builds from 3,799 open-source Java projects hosted on GitHub. Using the build data, we measured the frequency of broken builds caused by compiler errors, investigated the ten most common compiler error types, and reported their fix time. We manually analyzed 325 broken builds to summarize fix patterns of the ten most common compiler error types. Our findings help to characterize and understand compiler errors during CI and provide practical implications to developers, tool builders and researchers.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {176–187},
numpages = {12},
keywords = {Compiler Errors, Continuous Integration, Build Failures},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2594291.2594311,
author = {Maiya, Pallavi and Kanade, Aditya and Majumdar, Rupak},
title = {Race Detection for Android Applications},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594311},
doi = {10.1145/2594291.2594311},
abstract = {Programming environments for smartphones expose a concurrency model that combines multi-threading and asynchronous event-based dispatch. While this enables the development of efficient and feature-rich applications, unforeseen thread interleavings coupled with non-deterministic reorderings of asynchronous tasks can lead to subtle concurrency errors in the applications.In this paper, we formalize the concurrency semantics of the Android programming model. We further define the happens-before relation for Android applications, and develop a dynamic race detection technique based on this relation. Our relation generalizes the so far independently studied happens-before relations for multi-threaded programs and single-threaded event-driven programs. Additionally, our race detection technique uses a model of the Android runtime environment to reduce false positives.We have implemented a tool called DroidRacer. It generates execution traces by systematically testing Android applications and detects data races by computing the happens-before relation on the traces. We analyzed 15 Android applications including popular applications such as Facebook, Twitter and K-9 Mail. Our results indicate that data races are prevalent in Android applications, and that DroidRacer is an effective tool to identify data races.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {316–325},
numpages = {10},
keywords = {Android concurrency semantics, data races, happens-before reasoning},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@inproceedings{10.5555/2399776.2399784,
author = {Badreddin, Omar and Forward, Andrew and Lethbridge, Timothy C.},
title = {Model Oriented Programming: An Empirical Study of Comprehension},
year = {2012},
publisher = {IBM Corp.},
address = {USA},
abstract = {Many tools and approaches support the use of modeling abstractions in textual form. However, there have been few studies about whether textual models are as comprehensible as graphical models. We present an experiment investigating the understandability of three different notations: Systems modeled in UML, and the same systems in both Java and Umple. Umple is a model-oriented programming technology that enhances languages like Java and PHP with textual modeling abstractions. It was designed to bridge the gap between textual and graphical modeling. Our experiment asked participants to answer questions reflecting their level of comprehension. The results reveal that for simple comprehension tasks, a visual model and a textual model are comparable. Java's comprehension levels were lowest of all three notations. Our results align with the intuition that raising the abstraction levels of common object-oriented programming languages enhances comprehensibility.},
booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {73–86},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '12}
}

@inproceedings{10.1145/2391229.2391230,
author = {Conway, Neil and Marczak, William R. and Alvaro, Peter and Hellerstein, Joseph M. and Maier, David},
title = {Logic and Lattices for Distributed Programming},
year = {2012},
isbn = {9781450317610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2391229.2391230},
doi = {10.1145/2391229.2391230},
abstract = {In recent years there has been interest in achieving application-level consistency criteria without the latency and availability costs of strongly consistent storage infrastructure. A standard technique is to adopt a vocabulary of commutative operations; this avoids the risk of inconsistency due to message reordering. Another approach was recently captured by the CALM theorem, which proves that logically monotonic programs are guaranteed to be eventually consistent. In logic languages such as Bloom, CALM analysis can automatically verify that programs achieve consistency without coordination.In this paper we present BloomL, an extension to Bloom that takes inspiration from both of these traditions. BloomL generalizes Bloom to support lattices and extends the power of CALM analysis to whole programs containing arbitrary lattices. We show how the Bloom interpreter can be generalized to support efficient evaluation of lattice-based code using well-known strategies from logic programming. Finally, we use BloomL to develop several practical distributed programs, including a key-value store similar to Amazon Dynamo, and show how BloomL encourages the safe composition of small, easy-to-analyze lattices into larger programs.},
booktitle = {Proceedings of the Third ACM Symposium on Cloud Computing},
articleno = {1},
numpages = {14},
keywords = {distributed programming, Bloom, lattice, eventual consistency},
location = {San Jose, California},
series = {SoCC '12}
}

@inproceedings{10.1145/2342356.2342360,
author = {Escriva, Robert and Wong, Bernard and Sirer, Emin G\"{u}n},
title = {HyperDex: A Distributed, Searchable Key-Value Store},
year = {2012},
isbn = {9781450314190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342356.2342360},
doi = {10.1145/2342356.2342360},
abstract = {Distributed key-value stores are now a standard component of high-performance web services and cloud computing applications. While key-value stores offer significant performance and scalability advantages compared to traditional databases, they achieve these properties through a restricted API that limits object retrieval---an object can only be retrieved by the (primary and only) key under which it was inserted. This paper presents HyperDex, a novel distributed key-value store that provides a unique search primitive that enables queries on secondary attributes. The key insight behind HyperDex is the concept of hyperspace hashing in which objects with multiple attributes are mapped into a multidimensional hyperspace. This mapping leads to efficient implementations not only for retrieval by primary key, but also for partially-specified secondary attribute searches and range queries. A novel chaining protocol enables the system to achieve strong consistency, maintain availability and guarantee fault tolerance. An evaluation of the full system shows that HyperDex is 12-13x faster than Cassandra and MongoDB for finding partially specified objects. Additionally, HyperDex achieves 2-4x higher throughput for get/put operations.},
booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {25–36},
numpages = {12},
keywords = {nosql, key-value store, strong consistency, fault-tolerance, performance},
location = {Helsinki, Finland},
series = {SIGCOMM '12}
}

@article{10.1145/2538030,
author = {Isemann, Daniel and Ahmad, Khurshid},
title = {Ontological Access to Images of Fine Art},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/2538030},
doi = {10.1145/2538030},
abstract = {Information retrieval in a knowledge rich domain poses challenges that are different from other domains. The domain of fine arts and cultural heritage is an exemplar of such a domain. The many facets of, and complex interrelations between, works of fine art are not easily addressed by conventional keyword-based approaches or even by structured cataloguing systems. Information retrieval challenges in this domain include: the conversion of existing legacy data into knowledge representations that emulate the semantics of the domain's relationships; and easy access to a robust knowledge representation for users unfamiliar with query languages. Our research addresses aspects of both challenges as they are connected and may benefit from being addressed in conjunction. Based on a study on user preferences in art image search and a review of existing structured resources for cataloguing art and heritage information, we have developed two prototypes: Ontology Populator and Artfinder. The first prototype, Ontology Populator, is used to automatically enrich data akin to legacy data kept by heritage institutions and transform it into a knowledge base. The second prototype is a graphical query builder, Artfinder, which interacts with the knowledge base. The Artfinder interface, is constructed dynamically from the structure of the underlying knowledge. A task-based evaluation of Artfinder was carried out with 10 expert and 10 layperson evaluators. Participants reviewed the interface favourably and the evaluation also revealed potential for improvement. Artfinder and its “query logic,” perhaps is a semantically richer mode of accessing knowledge repositories, allowing for logically more complex queries than are currently supported outside the realm of dedicated query languages. We believe that domain experts and perhaps informed laypersons will benefit from this retrieval approach.},
journal = {J. Comput. Cult. Herit.},
month = {apr},
articleno = {3},
numpages = {25},
keywords = {description logic modeling, Legacy data conversion, ontology-based user interface}
}

@article{10.14778/3415478.3415546,
author = {Matsunobu, Yoshinori and Dong, Siying and Lee, Herman},
title = {MyRocks: LSM-Tree Database Storage Engine Serving Facebook's Social Graph},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415546},
doi = {10.14778/3415478.3415546},
abstract = {Facebook uses MySQL to manage tens of petabytes of data in its main database named the User Database (UDB). UDB serves social activities such as likes, comments, and shares. In the past, Facebook used InnoDB, a B+Tree based storage engine as the backend. The challenge was to find an index structure using less space and write amplification [1]. LSM-tree [2] has the potential to greatly improve these two bottlenecks. RocksDB, an LSM tree-based key/value store was already widely used in variety of applications but had a very low-level key-value interface. To overcome these limitations, MyRocks, a new MySQL storage engine, was built on top of RocksDB by adding relational capabilities. With MyRocks, using the RocksDB API, significant efficiency gains were achieved while still benefiting from all the MySQL features and tools. The transition was mostly transparent to client applications.Facebook completed the UDB migration from InnoDB to MyRocks in 2017. Since then, ongoing improvements in production operations, and additional enhancements to MySQL, MyRocks, and RocksDB, provided even greater efficiency wins. MyRocks also reduced the instance size by 62.3% for UDB data sets and performed fewer I/O operations than InnoDB. Finally, MyRocks consumed less CPU time for serving the same production traffic workload. These gains enabled us to reduce the number of database servers in UDB to less than half, saving significant resources. In this paper, we describe our journey to build and run an OLTP LSM-tree SQL database at scale. We also discuss the features we implemented to keep pace with UDB workloads, what made migrations easier, and what operational and software development challenges we faced during the two years of running MyRocks in production.Among the new features we introduced in RocksDB were transactional support, bulk loading, and prefix bloom filters, all are available for the benefit of all RocksDB users.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3217–3230},
numpages = {14}
}

@inproceedings{10.1145/3534056.3534936,
author = {Lund, Simon A. F. and Bonnet, Philippe and Jensen, Klaus B. A. and Gonzalez, Javier},
title = {I/O Interface Independence with XNVMe},
year = {2022},
isbn = {9781450393805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534056.3534936},
doi = {10.1145/3534056.3534936},
abstract = {The tight coupling of data-intensive systems and I/O interface has been a problem for years. A database system, relying on an specific I/O backend for direct asynchronous I/Os such as libaio, inherits its limitations in terms of portability, expressiveness and performance. The emergence of high-performance NVMe Solid-State Drives (SSDs), enabling new command sets, compounds this problem. Indeed, efforts to streamline the I/O stack have led to the introduction of new, complex and idiosyncratic I/O interfaces such as SPDK, io_uring or asynchronous ioctls. What is the appropriate I/O interface for a given system? How can applications effectively leverage SSD and end-to-end I/O interface innovations? Is I/O interface lock-in a necessary evil for data-intensive systems and storage services? Our answer to the latter question is no. Our answer to the former questions is xNVMe, a cross-platform user-space library that provides I/O-interface independence to user-space software. In this paper, we present the xNVMe API, we detail its design and we show that xNVMe has a negligible cost atop the most efficient I/O interfaces on Linux, FreeBSD and Windows.},
booktitle = {Proceedings of the 15th ACM International Conference on Systems and Storage},
pages = {108–119},
numpages = {12},
keywords = {NVMe SSD, SPDK, I/O interface, io_uring},
location = {Haifa, Israel},
series = {SYSTOR '22}
}

@article{10.1145/3597208,
author = {Batoun, Mohamed Amine and Yung, Ka Lai and Tian, Yuan and Sayagh, Mohammed},
title = {An Empirical Study on GitHub Pull Requests’ Reactions},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597208},
doi = {10.1145/3597208},
abstract = {The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., “Thumbs-up”, “Hooray”, “Heart”, “Rocket”, and “Laugh”). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the “end-users” of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request’s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests’ reactions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {146},
numpages = {35},
keywords = {pull requests, software collaboration, feedback, GitHub reactions}
}

@inproceedings{10.1145/3485832.3485915,
author = {Marson, Giorgia Azzurra and Andreina, Sebastien and Alluminio, Lorenzo and Munichev, Konstantin and Karame, Ghassan},
title = {Mitosis: Practically Scaling Permissioned Blockchains},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485915},
doi = {10.1145/3485832.3485915},
abstract = {Scalability remains one of the biggest challenges to the adoption of permissioned blockchain technologies for large-scale deployments. Namely, permissioned blockchains typically exhibit low latencies, compared to permissionless deployments—however at the cost of poor scalability. As a remedy, various solutions were proposed to capture “the best of both worlds”, targeting low latency and high scalability simultaneously. Among these, blockchain sharding emerges as the most prominent technique. Most existing sharding proposals exploit features of the permissionless model and are therefore restricted to cryptocurrency applications. A few permissioned sharding proposals exist, however, they either make strong trust assumptions on the number of faulty nodes or rely on trusted hardware or assume a static participation model where all nodes are expected to be available all the time. In practice, nodes may join and leave the system dynamically, which makes it challenging to establish how to shard and when. In this work, we address this problem and present&nbsp;Mitosis, a novel approach to practically improve scalability of permissioned blockchains. Our system allows the dynamic creation of blockchains, as more participants join the system, to meet practical scalability requirements. Crucially, it enables the division of an existing blockchain (and its participants) into two—reminiscent of mitosis, the biological process of cell division. Mitosis inherits the low latency of permissioned blockchains while preserving high throughput via parallel processing. Newly created chains in our system are fully autonomous, can choose their own consensus protocol, and yet they can interact with each other to share information and assets—meeting high levels of interoperability. We analyse the security of Mitosis and evaluate experimentally the performance of our solution when instantiated over Hyperledger Fabric. Our results show that Mitosis can be ported with little modifications and manageable overhead to existing permissioned blockchains, such as Hyperledger Fabric. As far as we are aware, Mitosis emerges as the first workable and practical solution to scale existing permissioned blockchains.},
booktitle = {Annual Computer Security Applications Conference},
pages = {773–783},
numpages = {11},
keywords = {cross-chain communication, permissioned blockchains, dynamic sharding, scalability},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@article{10.1145/3133909,
author = {Mazinanian, Davood and Ketkar, Ameya and Tsantalis, Nikolaos and Dig, Danny},
title = {Understanding the Use of Lambda Expressions in Java},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133909},
doi = {10.1145/3133909},
abstract = {Java 8 retrofitted lambda expressions, a core feature of functional programming, into a mainstream object-oriented language with an imperative paradigm. However, we do not know how Java developers have adapted to the functional style of thinking, and more importantly, what are the reasons motivating Java developers to adopt functional programming. Without such knowledge, researchers miss opportunities to improve the state of the art, tool builders use unrealistic assumptions, language designers fail to improve upon their designs, and developers are unable to explore efficient and effective use of lambdas.  We present the first large-scale, quantitative and qualitative empirical study to shed light on how imperative programmers use lambda expressions as a gateway into functional thinking. Particularly, we statically scrutinize the source code of 241 open-source projects with 19,770 contributors, to study the characteristics of 100,540 lambda expressions. Moreover, we investigate the historical trends and adoption rates of lambdas in the studied projects. To get a complementary perspective, we seek the underlying reasons on why developers introduce lambda expressions, by surveying 97 developers who are introducing lambdas in their projects, using the firehouse interview method.  Among others, our findings revealed an increasing trend in the adoption of lambdas in Java: in 2016, the ratio of lambdas introduced per added line of code increased by 54% compared to 2015. Lambdas were used for various reasons, including but not limited to (i) making existing code more succinct and readable, (ii) avoiding code duplication, and (iii) simulating lazy evaluation of functions. Interestingly, we found out that developers are using Java's built-in functional interfaces inefficiently, i.e., they prefer to use general functional interfaces over the specialized ones, overlooking the performance overheads that might be imposed. Furthermore, developers are not adopting techniques from functional programming, e.g., currying. Finally, we present the implications of our findings for researchers, tool builders, language designers, and developers.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {85},
numpages = {31},
keywords = {Multi-paradigm Programming, Empirical Studies, Java 8, The Firehouse Interview Method, Functional Programming, Lambda Expressions}
}

@inproceedings{10.1145/3491102.3517589,
author = {Salminen, Joni and Wenyun Guan, Kathleen and Jung, Soon-Gyo and Jansen, Bernard},
title = {Use Cases for Design Personas: A Systematic Review and New Frontiers},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517589},
doi = {10.1145/3491102.3517589},
abstract = {Personas represent the needs of users in diverse populations and impact design by endearing empathy and improving communication. While personas have been lauded for their benefits, we could locate no prior review of persona use cases in design, prompting the question: how are personas actually used to achieve these benefits? To address this question, we review 95 articles containing persona application across multiple domains, and identify software development, healthcare, and higher education as the top domains that employ personas. We then present a three-stage design hierarchy of persona usage to describe how personas are used in design tasks. Finally, we assess the increasing trend of persona initiatives aimed towards social good rather than solely commercial interests. Our findings establish a roadmap of best practices for how practitioners can innovatively employ personas to increase the value of designs and highlight avenues of using personas for socially impactful purposes.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {543},
numpages = {21},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3538641.3561486,
author = {Lelovic, Luka and Mathews, Michael and Elsayed, Amr and Cerny, Tomas and Frajtak, Karel and Tisnovsky, Pavel and Taibi, Davide},
title = {Architectural Languages in the Microservice Era: A Systematic Mapping Study},
year = {2022},
isbn = {9781450393980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538641.3561486},
doi = {10.1145/3538641.3561486},
abstract = {In modern software systems, Microservice Architecture (MSA) has gained popularity over monolithic design by providing the ability for flexible and independently upgradable services. Although there are considerable benefits that MSA provides, as new microservices are introduced into these MSA-based systems, they can become increasingly complex and hard to understand. Architectural languages are a potential solution to this problem because they can provide a comprehensive overview of system's architecture as it changes. In this paper, the authors conduct a systematic mapping study to identify the architectural languages discussed in academia. In particular, the authors observe the architectural languages that have the capability of representing MSA-based systems. Through the use of a detailed query in 4 reliable indexers, a collection of 402 papers were filtered down to a small set of 19 relevant papers. This filtration was done based on a research paper inclusion criteria and a language inclusion criteria. With these papers, a total of 12 architectural languages were investigated for the representation of MSA-based systems.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {39–46},
numpages = {8},
keywords = {service composition, domain-specific language, microservices architecture, architectural language},
location = {Virtual Event, Japan},
series = {RACS '22}
}

@article{10.1145/2377677.2377681,
author = {Escriva, Robert and Wong, Bernard and Sirer, Emin G\"{u}n},
title = {HyperDex: A Distributed, Searchable Key-Value Store},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2377677.2377681},
doi = {10.1145/2377677.2377681},
abstract = {Distributed key-value stores are now a standard component of high-performance web services and cloud computing applications. While key-value stores offer significant performance and scalability advantages compared to traditional databases, they achieve these properties through a restricted API that limits object retrieval---an object can only be retrieved by the (primary and only) key under which it was inserted. This paper presents HyperDex, a novel distributed key-value store that provides a unique search primitive that enables queries on secondary attributes. The key insight behind HyperDex is the concept of hyperspace hashing in which objects with multiple attributes are mapped into a multidimensional hyperspace. This mapping leads to efficient implementations not only for retrieval by primary key, but also for partially-specified secondary attribute searches and range queries. A novel chaining protocol enables the system to achieve strong consistency, maintain availability and guarantee fault tolerance. An evaluation of the full system shows that HyperDex is 12-13x faster than Cassandra and MongoDB for finding partially specified objects. Additionally, HyperDex achieves 2-4x higher throughput for get/put operations.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {25–36},
numpages = {12},
keywords = {fault-tolerance, performance, nosql, key-value store, strong consistency}
}

@article{10.1145/1486508.1486516,
author = {Grammenos, Dimitris and Savidis, Anthony and Stephanidis, Constantine},
title = {Designing Universally Accessible Games},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/1486508.1486516},
doi = {10.1145/1486508.1486516},
abstract = {Today, computer games are one of the major sources of entertainment. Computer games are usually far more demanding than typical interactive applications in terms of motor and sensory skills needed for interaction control, due to special-purpose input devices, complicated interaction techniques, and the primary emphasis on visual control and attention. This renders computer games inaccessible to a large percentage of people with disabilities. This article introduces the concept of universally accessible games, that is, games proactively designed to optimally fit and adapt to individual gamer characteristics and to be concurrently played among people with diverse abilities, without requiring particular adjustments or modifications. The concept is elaborated and tested through four case studies: a web-based chess game (UA-Chess), an action game (Access Invaders), a universally inaccessible game (Game Over!) used as an interactive educational tool, and an improved version of Access Invaders (Terrestrial Invaders). For all cases, key design and evaluation findings are discussed, reporting consolidated know-how and experience. Finally, the research challenge of creating multiplayer universally accessible games is further elaborated, proposing the novel concept of Parallel Game Universes as a potential solution.},
journal = {Comput. Entertain.},
month = {feb},
articleno = {8},
numpages = {29},
keywords = {Design for all, unified design, accessible games, universal access}
}

@inproceedings{10.1145/3492321.3519565,
author = {Guo, Liwei and Lin, Felix Xiaozhu},
title = {Minimum Viable Device Drivers for ARM Trustzone},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3519565},
doi = {10.1145/3492321.3519565},
abstract = {While TrustZone can isolate IO hardware, it lacks drivers for modern IO devices. Rather than porting drivers, we propose a novel approach to deriving minimum viable drivers: developers exercise a full driver and record the driver/device interactions; the processed recordings, dubbed driverlets, are replayed in the TEE at run time to access IO devices. Driverlets address two key challenges: correctness and expressiveness, for which they build on a key construct called interaction template. The interaction template ensures faithful reproduction of recorded IO jobs (albeit on new IO data); it accepts dynamic input values; it tolerates nondeterministic device behaviors. We demonstrate driverlets on a series of sophisticated devices, making them accessible to Trust-Zone for the first time to our knowledge. Our experiments show that driverlets are secure, easy to build, and incur acceptable overhead (1.4\texttimes{}-2.7\texttimes{} compared to native drivers). Driverlets fill a critical gap in the TrustZone TEE, realizing its long-promised vision of secure IO.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {300–316},
numpages = {17},
keywords = {operating systems, arm TrustZone, security, device drivers},
location = {Rennes, France},
series = {EuroSys '22}
}

@inproceedings{10.1145/1168054.1168064,
author = {Cabri, Giacomo and Leonardi, Letizia and Quitadamo, Raffaele},
title = {Enabling Java Mobile Computing on the IBM Jikes Research Virtual Machine},
year = {2006},
isbn = {3939352055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168054.1168064},
doi = {10.1145/1168054.1168064},
abstract = {Today's complex applications must face the distribution of data and code among different network nodes. Java is a wide-spread language that allows developers to build complex software, even distributed, but it cannot handle the migration of computations (i.e. threads), due to intrinsic limitations of many traditional JVMs. After analyzing the approaches in literature, this paper presents our research work on the IBM Jikes Research Virtual Machine: exploiting some of its innovative VM techniques, we implemented an extension of its scheduler that allows applications to easily capture the state of a running thread and makes it possible to restore it elsewhere (i.e. on a different hardware or software architecture, but still with a version of JikesRVM installed). Our thread serialization mechanism provides support for both proactive and reactive migration of single- and multi-threaded Java applications. With respect to previous approaches, we implemented the mobility framework without recompiling a previous JVM source code, but simply extending its functionalities with a full Java package.},
booktitle = {Proceedings of the 4th International Symposium on Principles and Practice of Programming in Java},
pages = {62–71},
numpages = {10},
keywords = {code mobility, distributed applications, thread persistence, Java virtual machine},
location = {Mannheim, Germany},
series = {PPPJ '06}
}

@article{10.1145/2400682.2400683,
author = {Coppens, Bart and De Sutter, Bjorn and Maebe, Jonas},
title = {Feedback-Driven Binary Code Diversification},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2400682.2400683},
doi = {10.1145/2400682.2400683},
abstract = {As described in many blog posts and in the scientific literature, exploits for software vulnerabilities are often engineered on the basis of patches. For example, “Microsoft Patch Tuesday” is often followed by “Exploit Wednesday” during which yet unpatched systems become vulnerable to patch-based exploits. Part of the patch engineering includes the identification of the vulnerable binary code by means of reverse-engineering tools and diffing add-ons. In this article we present a feedback-driven compiler tool flow that iteratively transforms code until diffing tools become ineffective enough to close the “Exploit Wednesday” window of opportunity. We demonstrate the tool's effectiveness on a set of real-world patches and against the latest version of BinDiff.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jan},
articleno = {24},
numpages = {26},
keywords = {software diversity, binary diffing, program matching, Compiler transformations, patches}
}

@article{10.1145/2522968.2522977,
author = {Yahyavi, Amir and Kemme, Bettina},
title = {Peer-to-Peer Architectures for Massively Multiplayer Online Games: A Survey},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2522968.2522977},
doi = {10.1145/2522968.2522977},
abstract = {Scalability, fast response time, and low cost are of utmost importance in designing a successful massively multiplayer online game. The underlying architecture plays an important role in meeting these conditions. Peer-to-peer architectures, due to their distributed and collaborative nature, have low infrastructure costs and can achieve high scalability. They can also achieve fast response times by creating direct connections between players. However, these architectures face many challenges. Distributing a game among peers makes maintaining control over the game more complex. Peer-to-peer architectures also tend to be vulnerable to churn and cheating. Moreover, different genres of games have different requirements that should be met by the underlying architecture, rendering the task of designing a general-purpose architecture harder. Many peer-to-peer gaming solutions have been proposed that utilize a range of techniques while using somewhat different and confusing terminologies. This article presents a comprehensive overview of current peer-to-peer solutions for massively multiplayer games using a uniform terminology.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {9},
numpages = {51},
keywords = {replication, network overlays, Peer-to-peer networking, incentives, commercial applications, fault tolerance, massively multiplayer online games, interest management, cheating, consistency control, multicasting}
}

@inproceedings{10.1145/2517208.2517221,
author = {Medeiros, Fl\'{a}vio and Ribeiro, M\'{a}rcio and Gheyi, Rohit},
title = {Investigating Preprocessor-Based Syntax Errors},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517221},
doi = {10.1145/2517208.2517221},
abstract = {The C preprocessor is commonly used to implement variability in program families. Despite the widespread usage, some studies indicate that the C preprocessor makes variability implementation difficult and error-prone. However, we still lack studies to investigate preprocessor-based syntax errors and quantify to what extent they occur in practice. In this paper, we define a technique based on a variability-aware parser to find syntax errors in releases and commits of program families. To investigate these errors, we perform an empirical study where we use our technique in 41 program family releases, and more than 51 thousand commits of 8 program families. We find 7 and 20 syntax errors in releases and commits of program families, respectively. They are related not only to incomplete annotations, but also to complete ones. We submit 8 patches to fix errors that developers have not fixed yet, and they accept 75% of them. Our results reveal that the time developers need to fix the errors varies from days to years in family repositories. We detect errors even in releases of well-known and widely used program families, such as Bash, CVS and Vim. We also classify the syntax errors into 6 different categories. This classification may guide developers to avoid them during development.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {75–84},
numpages = {10},
keywords = {program families, preprocessors, syntax errors},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@inproceedings{10.1145/3426426.3428485,
author = {Laddad, Shadaj and Sen, Koushik},
title = {ScalaPy: Seamless Python Interoperability for Cross-Platform Scala Programs},
year = {2020},
isbn = {9781450381772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426426.3428485},
doi = {10.1145/3426426.3428485},
abstract = {In recent years, Python has become the language of choice for data scientists with its many high-quality scientific libraries and Scala has become the go-to language for big data systems. In this paper, we bridge these languages with ScalaPy, a system for interoperability between Scala and Python. With ScalaPy, developers can use Python libraries in Scala by treating Python values as Scala objects and exposing Scala values to Python. ScalaPy supports both Scala on the JVM and Scala Native, enabling its usage from data experiments in interactive notebook environments to performance-critical production systems. In this paper, we explore the challenges involved with mixing the semantics and implementations of these two disparate languages.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Symposium on Scala},
pages = {2–13},
numpages = {12},
keywords = {Python, Scala, language interoperability},
location = {Virtual, USA},
series = {SCALA 2020}
}

@inproceedings{10.1145/2463676.2465290,
author = {Mishne, Gilad and Dalton, Jeff and Li, Zhenghua and Sharma, Aneesh and Lin, Jimmy},
title = {Fast Data in the Era of Big Data: Twitter's Real-Time Related Query Suggestion Architecture},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465290},
doi = {10.1145/2463676.2465290},
abstract = {We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time "twist": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of "big data". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a "big data" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle "big" as well as "fast" data.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1147–1158},
numpages = {12},
keywords = {hadoop, log analysis, mapreduce},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3616961.3616978,
author = {Oppenlaender, Jonas and Silvennoinen, Johanna and Paananen, Ville and Visuri, Aku},
title = {Perceptions and Realities of Text-to-Image Generation},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616961.3616978},
doi = {10.1145/3616961.3616978},
abstract = {Generative artificial intelligence (AI) is a widely popular technology that will have a profound impact on society and individuals. Less than a decade ago, it was thought that creative work would be among the last to be automated&nbsp;– yet today, we see AI encroaching on many creative domains. In this paper, we present the findings of a survey study on people’s perceptions of text-to-image generation. We touch on participants’ technical understanding of the emerging technology, their fears and concerns, and thoughts about risks and dangers of text-to-image generation to the individual and society. We find that while participants were aware of the risks and dangers associated with the technology, only few participants considered the technology to be a personal risk. The risks for others were more easy to recognize for participants. Artists were particularly seen at risk. Interestingly, participants who had tried the technology rated its future importance lower than those who had not tried it. This result shows that many people are still oblivious of the potential personal risks of generative artificial intelligence and the impending societal changes associated with this technology.},
booktitle = {Proceedings of the 26th International Academic Mindtrek Conference},
pages = {279–288},
numpages = {10},
keywords = {text-to-image generation, generative AI},
location = {Tampere, Finland},
series = {Mindtrek '23}
}

@inproceedings{10.1145/1050491.1050498,
author = {Grimstead, Ian J. and Avis, Nick J. and Walker, David W.},
title = {Visualization across the Pond: How a Wireless PDA Can Collaborate with Million-Polygon Datasets via 9,000km of Cable},
year = {2005},
isbn = {1595930124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1050491.1050498},
doi = {10.1145/1050491.1050498},
abstract = {We present an initial report on using our distributed, collaborative grid enabled visualization environment to link SuperComputing 2004 (Pittsburgh, PA, USA) with the Cardiff School of Computer Science (Cardiff, Wales, UK). A PDA was used to visualize large, shared datasets (in the range of 0.5--4.5 million polygons) stored in Cardiff, interacting with a laptop (rendering the data locally). The system used was the Resource-Aware Visualization Environment (RAVE), deployed as Web Services running in the background on remote machines. This enables us to use a wide range of heterogeneous machines without being concerned with the underlying implementation of RAVE, or the architecture of the machine.},
booktitle = {Proceedings of the Tenth International Conference on 3D Web Technology},
pages = {47–56},
numpages = {10},
keywords = {visualization, services, grid/web, heterogeneous systems},
location = {Bangor, United Kingdom},
series = {Web3D '05}
}

@inproceedings{10.5555/3195638.3195673,
author = {Zhan, Jia and Akgun, Itir and Zhao, Jishen and Davis, Al and Faraboschi, Paolo and Wang, Yuangang and Xie, Yuan},
title = {A Unified Memory Network Architecture for In-Memory Computing in Commodity Servers},
year = {2016},
publisher = {IEEE Press},
abstract = {In-memory computing is emerging as a promising paradigm in commodity servers to accelerate data-intensive processing by striving to keep the entire dataset in DRAM. To address the tremendous pressure on the main memory system, discrete memory modules can be networked together to form a memory pool, enabled by recent trends towards richer memory interfaces (e.g. Hybrid Memory Cubes, or HMCs). Such an inter-memory network provides a scalable fabric to expand memory capacity, but still suffers from long multi-hop latency, limited bandwidth, and high power consumption---problems that will continue to exacerbate as the gap between interconnect and transistor performance grows. Moreover, inside each memory module, an intra-memory network (NoC) is typically employed to connect different memory partitions. Without careful design, the back-pressure inside the memory modules can further propagate to the inter-memory network to cause a performance bottleneck.To address these problems, we propose co-optimization of intra- and inter-memory network. First, we re-organize the intra-memory network structure, and provide a smart I/O interface to reuse the intra-memory NoC as the network switches for inter-memory communication, thus forming a unified memory network. Based on this architecture, we further optimize the inter-memory network for both high performance and lower energy, including a distance-aware selective compression scheme to drastically reduce communication burden, and a light-weight power-gating algorithm to turn off under-utilized links while guaranteeing a connected graph and deadlock-free routing. We develop an event-driven simulator to model our proposed architectures. Experiment results based on both synthetic traffic and real big-data workloads show that our unified memory network architecture can achieve 75.1% average memory access latency reduction and 22.1% total memory energy saving.},
booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
articleno = {29},
numpages = {14},
location = {Taipei, Taiwan},
series = {MICRO-49}
}

@inproceedings{10.1145/3322431.3325107,
author = {Yahyazadeh, Moosa and Podder, Proyash and Hoque, Endadul and Chowdhury, Omar},
title = {Expat: Expectation-Based Policy Analysis and Enforcement for Appified Smart-Home Platforms},
year = {2019},
isbn = {9781450367530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322431.3325107},
doi = {10.1145/3322431.3325107},
abstract = {This paper focuses on developing a security mechanism geared towards appified smart-home platforms. Such platforms often expose programming interfaces for developing automation apps that mechanize different tasks among smart sensors and actuators (e.g., automatically turning on the AC when the room temperature is above 80 F). Due to the lack of effective access control mechanisms, these automation apps can not only have unrestricted access to the user's sensitive information (e.g., the user is not at home) but also violate user expectations by performing undesired actions. As users often obtain these apps from unvetted sources, a malicious app can wreak havoc on a smart-home system by either violating the user's security and privacy, or creating safety hazards (e.g., turning on the oven when no one is at home). To mitigate such threats, we propose Expat which ensures that user expectations are never violated by the installed automation apps at runtime. To achieve this goal, Expat provides a platform-agnostic, formal specification language UEI for capturing user expectations of the installed automation apps' behavior. For effective authoring of these expectations (as policies) in UEI, Expat also allows a user to check the desired properties (e.g., consistency, entailment) of them; which due to their formal semantics can be easily discharged by an SMT solver. Expat then enforces UEI policies in situ with an inline reference monitor which can be realized using the same app programming interface exposed by the underlying platform. We instantiate Expat for one of the representative platforms, OpenHAB, and demonstrate it can effectively mitigate a wide array of threats by enforcing user expectations while incurring only modest performance overhead.},
booktitle = {Proceedings of the 24th ACM Symposium on Access Control Models and Technologies},
pages = {61–72},
numpages = {12},
keywords = {appified smart-home platforms, inline reference monitoring, policy enforcement, iot security},
location = {Toronto ON, Canada},
series = {SACMAT '19}
}

@inproceedings{10.1145/1985793.1985807,
author = {Kumar, Sandeep and Khoo, Siau-Cheng and Roychoudhury, Abhik and Lo, David},
title = {Mining Message Sequence Graphs},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985807},
doi = {10.1145/1985793.1985807},
abstract = {Dynamic specification mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, mining program behavior from execution traces is difficult for concurrent/distributed programs. Specifically, the inherent partial order relationships among events occurring across processes pose a big challenge to specification mining. In this paper, we propose a framework for mining partial orders so as to understand concurrent program behavior. Our miner takes in a set of concurrent program traces, and produces a message sequence graph (MSG) to represent the concurrent program behavior. An MSG represents a graph where the nodes of the graph are partial orders, represented as Message Sequence Charts. Mining an MSG allows us to understand concurrent program behaviors since the nodes of the MSG depict important "phases" or "interaction snippets" involving several concurrently executing processes. To demonstrate the power of this technique, we conducted experiments on mining behaviors of several fairly complex distributed systems. We show that our miner can produce the corresponding MSGs with both high precision and recall.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
keywords = {distributed systems, specification mining},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/3331076.3331089,
author = {Desai, Bipin C.},
title = {Privacy in the Age of Information (and Algorithms)},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331089},
doi = {10.1145/3331076.3331089},
abstract = {This paper raises the privacy issues related to information that is accessible about individuals from their mobile devices and that which is collected when they interact with and use so called "free" services provided on the web. The importance of privacy has been ignored by most legislation and any laws passed have no teeth. The only exception is the privacy protection that is embedded in the EU's General Data Protection Regulation(GDPR). GDPR gives control to individuals over their personal data and requires any organization which collects and controls personal information to have in place appropriate measures both technical and logistic, to implement the data protection principles. In this paper, we propose a technical solution to provide a personal email and web server with complete control of all correspondence and contents. This would liberate users from fake free services and provide privacy and security.},
booktitle = {Proceedings of the 23rd International Database Applications &amp; Engineering Symposium},
articleno = {17},
numpages = {12},
keywords = {online social networks, privacy, Heimdallr, security, free email service, warrantless constant surveillance},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.7146/aahcc.v1i1.21199,
author = {Ames, Morgan G.},
title = {Charismatic Technology},
year = {2015},
publisher = {Aarhus University Press},
address = {Aarhus N},
url = {https://doi.org/10.7146/aahcc.v1i1.21199},
doi = {10.7146/aahcc.v1i1.21199},
abstract = {To explain the uncanny holding power that some technologies seem to have, this paper presents a theory of charisma as attached to technology. It uses the One Laptop per Child project as a case study for exploring the features, benefits, and pitfalls of charisma. It then contextualizes OLPC's charismatic power in the historical arc of other charismatic technologies, highlighting the enduring nature of charisma and the common themes on which the charisma of a century of technological progress rests. In closing, it discusses how scholars and practitioners in human-computer interaction might use the concept of charismatic technology in their own work.},
booktitle = {Proceedings of The Fifth Decennial Aarhus Conference on Critical Alternatives},
pages = {109–120},
numpages = {12},
keywords = {charisma, childhood, education, one laptop per child, ideology, history of technology, religion, science and technology studies, technological determinism, utopianism},
location = {Aarhus, Denmark},
series = {CA '15}
}

@inproceedings{10.1145/1176617.1176631,
author = {Lim, Seung Chan Slim and Lucas, Peter},
title = {JDA: A Step towards Large-Scale Reuse on the Web},
year = {2006},
isbn = {159593491X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176617.1176631},
doi = {10.1145/1176617.1176631},
abstract = {The Web is maturing as a rich application development platform, and efforts are being made to provide richer and more dynamic interactions using JavaScript. JavaScript-based Web applications such as Google Maps have gained extra attention because they can be easily included in HTML for reuse. Unfortunately, various technical hurdles have made it difficult for JavaScript reuse to extend beyond its current state. Furthermore, JavaScript reuse is still out of reach for a large portion of the Web user base unversed in the use of programming languages.In this paper, we dive deeper into our previous work on the JavaScript Dataflow Architecture (JDA). JDA is intended for Web client applications written using HTML and JavaScript. We discuss the ways in which the architecture addresses many of the hurdles that modern Web client applications face in the realm of large-scale reuse and remixing. JDA aims to provide an ecosystem comprised of black box components operating within a JavaScript-based asynchronous message-passing environment. The environment allows you to use simple HTML to assemble Web applications from JavaScript black boxes scattered around the World Wide Web. No programming skill is required in their assembly, and no plug-ins or applets are required for their execution. Furthermore, the architecture extends the black box metaphor beyond the boundaries of JavaScript and allows multiple JavaScript components contained within an HTML file to be reused as a whole.A detailed account of an early prototype is discussed, and research is being done to improve it. JDA suggests that large-scale reuse and arbitrary remixing of Web applications can be realized using currently existing technologies.},
booktitle = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
pages = {586–601},
numpages = {16},
keywords = {information devices architecture, black box reuse, rich web application, JavaScript, composition, dataflow architecture, web engineering},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@inproceedings{10.1109/ICPC.2017.7,
author = {Almeida, Daniel A. and Murphy, Gail C. and Wilson, Greg and Hoye, Mike},
title = {Do Software Developers Understand Open Source Licenses?},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.7},
doi = {10.1109/ICPC.2017.7},
abstract = {Software provided under open source licenses is widely used, from forming high-profile stand-alone applications (e.g., Mozilla Firefox) to being embedded in commercial offerings (e.g., network routers). Despite the high frequency of use of open source licenses, there has been little work about whether software developers understand the open source licenses they use. To our knowledge, only one survey has been conducted, which focused on which licenses developers choose and when they encounter problems with licensing open source software. To help fill the gap of whether or not developers understand the open source licenses they use, we conducted a survey that posed development scenarios involving three popular open source licenses (GNU GPL 3.0, GNU LGPL 3.0 and MPL 2.0) both alone and in combination. The 375 respondents to the survey, who were largely developers, gave answers consistent with those of a legal expert's opinion in 62% of 42 cases. Although developers clearly understood cases involving one license, they struggled when multiple licenses were involved. An analysis of the quantitative and qualitative results of the study indicate a need for tool support to help guide developers in understanding this critical information attached to software components.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {1–11},
numpages = {11},
keywords = {survey, open source, software licenses},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/1930488.1930533,
author = {Hammouda, Imed and Mikkonen, Tommi and Oksanen, Ville and Jaaksi, Ari},
title = {Open Source Legality Patterns: Architectural Design Decisions Motivated by Legal Concerns},
year = {2010},
isbn = {9781450300117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1930488.1930533},
doi = {10.1145/1930488.1930533},
abstract = {Complications emerge when various open source software components, governed by different licenses, are used in the same software system. For various reasons, these licenses introduce different privileges and requirements on the use and distribution of composed code, and are therefore often fundamentally incompatible with each other when combined arbitrarily. Consequently the way the different components can be integrated requires attention at the level of software architecture. In this paper, we introduce open source legality patterns -- architectural design decisions motivated by legal concerns associated with open source licensing issues and licenses themselves. Towards the end of the paper, we also review some related work and discuss why it is important to create common guidelines for designs that mix and match different open source systems and proprietary software, and provide directions for future work.},
booktitle = {Proceedings of the 14th International Academic MindTrek Conference: Envisioning Future Media Environments},
pages = {207–214},
numpages = {8},
keywords = {legal concerns, open source licensing, design patterns},
location = {Tampere, Finland},
series = {MindTrek '10}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.1145/3632954,
author = {Park, Joongun and Kang, Seunghyo and Lee, Sanghyeon and Kim, Taehoon and Park, Jongse and Kwon, Youngjin and Huh, Jaehyuk},
title = {Hardware Hardened Sandbox Enclaves for Trusted Serverless Computing},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3632954},
doi = {10.1145/3632954},
abstract = {In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this paper proposes an extended TEE model called Cloister, designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves which is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this paper shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
keywords = {Serverless computing, Trusted Execution Environment, Security, Hardware}
}

@article{10.1145/2902362,
author = {Hindle, Abram and Barr, Earl T. and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
title = {On the Naturalness of Software},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/2902362},
doi = {10.1145/2902362},
abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension.We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations---and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether (a) code can be usefully modeled by statistical language models and (b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very regular, and, in fact, even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area.},
journal = {Commun. ACM},
month = {apr},
pages = {122–131},
numpages = {10}
}

@inproceedings{10.1145/3379337.3415860,
author = {Lin, Richard and Ramesh, Rohit and Chi, Connie and Jain, Nikhil and Nuqui, Ryan and Dutta, Prabal and Hartmann, Bj\"{o}rn},
title = {Polymorphic Blocks: Unifying High-Level Specification and Low-Level Control for Circuit Board Design},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415860},
doi = {10.1145/3379337.3415860},
abstract = {Mainstream board-level circuit design tools work at the lowest level of design --- schematics and individual components. While novel tools experiment with higher levels of design, abstraction often comes at the expense of the fine-grained control afforded by low-level tools. In this work, we propose a hardware description language (HDL) approach that supports users at multiple levels of abstraction from broad system architecture to subcircuits and component selection. We extend the familiar hierarchical block diagram with polymorphism to include abstract-typed blocks (e.g., generic resistor supertype) and electronics modeling (i.e., currents and voltages). Such an approach brings the advantages of reusability and encapsulation from object-oriented programming, while addressing the unique needs of electronics designers such as physical correctness verification. We discuss the system design, including fundamental abstractions, the block diagram construction HDL, and user interfaces to inspect and fine-tune the design; demonstrate example designs built with our system; and present feedback from intermediate-level engineers who have worked with our system.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {529–540},
numpages = {12},
keywords = {circuit design, printed circuit board (pcb) design, hardware description language (hdl)},
location = {Virtual Event, USA},
series = {UIST '20}
}

@proceedings{10.1145/3569951,
title = {PEARC '23: Practice and Experience in Advanced Research Computing},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Portland, OR, USA}
}

@article{10.1145/3422667,
author = {Ram\'{\i}rez, Crist\'{o}bal and Hern\'{a}ndez, C\'{e}sar Alejandro and Palomar, Oscar and Unsal, Osman and Ram\'{\i}rez, Marco Antonio and Cristal, Adri\'{a}n},
title = {A RISC-V Simulator and Benchmark Suite for Designing and Evaluating Vector Architectures},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3422667},
doi = {10.1145/3422667},
abstract = {Vector architectures lack tools for research. Consider the gem5 simulator, which is possibly the leading platform for computer-system architecture research. Unfortunately, gem5 does not have an available distribution that includes a flexible and customizable vector architecture model. In consequence, researchers have to develop their own simulation platform to test their ideas, which consume much research time. However, once the base simulator platform is developed, another question is the following: Which applications should be tested to perform the experiments? The lack of Vectorized Benchmark Suites is another limitation. To face these problems, this work presents a set of tools for designing and evaluating vector architectures. First, the gem5 simulator was extended to support the execution of RISC-V Vector instructions by adding a parameterizable Vector Architecture model for designers to evaluate different approaches according to the target they pursue. Second, a novel Vectorized Benchmark Suite is presented: a collection composed of seven data-parallel applications from different domains that can be classified according to the modules that are stressed in the vector architecture. Finally, a study of the Vectorized Benchmark Suite executing on the gem5-based Vector Architecture model is highlighted. This suite is the first in its category that covers the different possible usage scenarios that may occur within different vector architecture designs such as embedded systems, mainly focused on short vectors, or High-Performance-Computing (HPC), usually designed for large vectors.},
journal = {ACM Trans. Archit. Code Optim.},
month = {nov},
articleno = {38},
numpages = {30},
keywords = {vectorization, benchmarking, High-performance computer architecture, gem5, vector architectures}
}

@inproceedings{10.1145/2207676.2208620,
author = {Chilana, Parmit K. and Ko, Amy J. and Wobbrock, Jacob O.},
title = {LemonAid: Selection-Based Crowdsourced Contextual Help for Web Applications},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2208620},
doi = {10.1145/2207676.2208620},
abstract = {Web-based technical support such as discussion forums and social networking sites have been successful at ensuring that most technical support questions eventually receive helpful answers. Unfortunately, finding these answers is still quite difficult, since users' textual queries are often incomplete, imprecise, or use different vocabularies to describe the same problem. We present LemonAid, a new approach to help that allows users to find help by instead selecting a label, widget, link, image or other user interface (UI) element that they believe is relevant to their problem. LemonAid uses this selection to retrieve previously asked questions and their corresponding answers. The key insight that makes LemonAid work is that users tend to make similar selections in the interface for similar help needs and different selections for different help needs. Our initial evaluation shows that across a corpus of dozens of tasks and thousands of requests, LemonAid retrieved a result for 90% of help requests based on UI selections and, of those, over half had relevant matches in the top 2 results.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1549–1558},
numpages = {10},
keywords = {crowdsourced help, software support, contextual help},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2568225.2568232,
author = {Huang, Peng and Ma, Xiao and Shen, Dongcai and Zhou, Yuanyuan},
title = {Performance Regression Testing Target Prioritization via Performance Risk Analysis},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568232},
doi = {10.1145/2568225.2568232},
abstract = {As software evolves, problematic changes can significantly degrade software performance, i.e., introducing performance regression. Performance regression testing is an effective way to reveal such issues in early stages. Yet because of its high overhead, this activity is usually performed infrequently. Consequently, when performance regression issue is spotted at a certain point, multiple commits might have been merged since last testing. Developers have to spend extra time and efforts narrowing down which commit caused the problem. Existing efforts try to improve performance regression testing efficiency through test case reduction or prioritization.  In this paper, we propose a new lightweight and white-box approach, performance risk analysis (PRA), to improve performance regression testing efficiency via testing target prioritization. The analysis statically evaluates a given source code commit's risk in introducing performance regression. Performance regression testing can leverage the analysis result to test commits with high risks first while delaying or skipping testing on low-risk commits.  To validate this idea's feasibility, we conduct a study on 100 real-world performance regression issues from three widely used, open-source software. Guided by insights from the study, we design PRA and build a tool, PerfScope. Evaluation on the examined problematic commits shows our tool can successfully alarm 91% of them. Moreover, on 600 randomly picked new commits from six large-scale software, with our tool, developers just need to test only 14-22% of the 600 commits and will still be able to alert 87-95% of the commits with performance regression.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {60–71},
numpages = {12},
keywords = {performance risk analysis, Performance regression, cost modeling},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3098954.3103165,
author = {J\"{a}ger, Lukas and Petri, Richard and Fuchs, Andreas},
title = {Rolling DICE: Lightweight Remote Attestation for COTS IoT Hardware},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3103165},
doi = {10.1145/3098954.3103165},
abstract = {The specification Device Identity Composition Engine (DICE) provides a novel basis for remote attestations specifically suitable in the IoT context. Its purpose is to provide means for remote attestations to devices that are too size-, cost-, energy- or otherwise constrained to have Trusted Platform Module attached.This paper gives a short explanation of DICE and compares different approaches for building up a remote attestation protocol based on it, using symmetric and asymmetric cryptography. Based on this comparison a symmetric attestation protocol is proposed for most resource constrained devices and its implications for attestation servers are discussed. Furthermore a feasibility study is conducted mapping the DICE and the proposed DICE-based attestation approach to commercial off-the-shelf (COTS) hardware -- namely Arduino Uno in this case -- and measurement of the code size, binary size and added computational requirements is provided. The security of the mapping approach is evaluated and its advantages and pitfalls are demonstrated. The goal is to show how DICE-based approaches can be mapped to existing hardware and how a more secure IoT environment can be established on already deployed devices without changes to the hardware.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {95},
numpages = {8},
keywords = {DICE, HMAC, Remote Attestation, IoT, Arduino},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3355369.3355601,
author = {Anderson, Blake and McGrew, David},
title = {TLS Beyond the Browser: Combining End Host and Network Data to Understand Application Behavior},
year = {2019},
isbn = {9781450369480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355369.3355601},
doi = {10.1145/3355369.3355601},
abstract = {The Transport Layer Security (TLS) protocol has evolved in response to different attacks and is increasingly relied on to secure Internet communications. Web browsers have led the adoption of newer and more secure cryptographic algorithms and protocol versions, and thus improved the security of the TLS ecosystem. Other application categories, however, are increasingly using TLS, but too often are relying on obsolete and insecure protocol options.To understand in detail what applications are using TLS, and how they are using it, we developed a novel system for obtaining process information from end hosts and fusing it with network data to produce a TLS fingerprint knowledge base. This data has a rich set of context for each fingerprint, is representative of enterprise TLS deployments, and is automatically updated from ongoing data collection. Our dataset is based on 471 million endpoint-labeled and 8 billion unlabeled TLS sessions obtained from enterprise edge networks in five countries, plus millions of sessions from a malware analysis sandbox. We actively maintain an open source dataset that, at 4,500+ fingerprints and counting, is both the largest and most informative ever published. In this paper, we use the knowledge base to identify trends in enterprise TLS applications beyond the browser: application categories such as storage, communication, system, and email. We identify a rise in the use of TLS by nonbrowser applications and a corresponding decline in the fraction of sessions using version 1.3. Finally, we highlight the shortcomings of na\"{\i}vely applying TLS fingerprinting to detect malware, and we present recent trends in malware's use of TLS such as the adoption of cipher suite randomization.},
booktitle = {Proceedings of the Internet Measurement Conference},
pages = {379–392},
numpages = {14},
location = {Amsterdam, Netherlands},
series = {IMC '19}
}

@inproceedings{10.1145/3604930.3605721,
author = {Arora, Rohan and Devi, Umamaheswari and Eilam, Tamar and Goyal, Aanchal and Narayanaswami, Chandra and Parida, Pritish},
title = {Towards Carbon Footprint Management in Hybrid Multicloud},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605721},
doi = {10.1145/3604930.3605721},
abstract = {Enterprises today aspire to optimize the operating costs and carbon footprint (CFP) of their IT operations jointly without compromising their business imperatives. This has given rise to a hybrid approach in which enterprises retain the dynamic choice to leverage private data centers and one or more public clouds in conjunction. While cloud service providers (CSPs) have long provided APIs for estimating, reconciling, and optimizing operating costs, they have only recently started exposing APIs related to CFP.Indeed, this is a step in the right direction. Nevertheless, our analyses of these APIs reveals many gaps that need to be addressed to facilitate sizing and placement decisions that can factor in carbon. First, there is a lack of standardized, transparent methodology for CFP quantification across different CSPs. Second, the coarse granularity of the CFP data provided today can help with post-facto reporting but is not suitable for proactive fine-grained optimization. Last, enterprises themselves are unable to independently compute the current CFP or estimate potential CFP savings since CSPs do not share the required power usage data.To address these gaps, enterprises have started developing their own carbon assessment methodologies and tools to estimate the CFP of workloads running on public clouds using the available user-facing APIs. These systems hold the promise for an independent and unbiased evaluation and estimation of relative savings between different deployment options by cloud users. We describe and analyze the details of CSP-native carbon-reporting tools and their quantification methodology, and the "outside-of-the-cloud" estimation approaches. Finally, we present opportunities for future research in the direction of trustworthy, fine-grained, public cloud workload CFP estimation, which is a prerequisite for meaningful realization of carbon optimization.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {9},
numpages = {7},
keywords = {data centers, carbon-aware optimization, GHG emissions, cloud, GHG accounting, sustainable computing, carbon footprint},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{10.1145/2641580.2641590,
author = {Nyman, Linus},
title = {Hackers on Forking},
year = {2014},
isbn = {9781450330169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641580.2641590},
doi = {10.1145/2641580.2641590},
abstract = {All open source licenses allow the copying of an existing body of code for use as the basis of a separate development project. This practice is commonly known as forking the code. This paper presents the results of a study in which 11 programmers were interviewed about their opinions on the right to fork and the impact of forking on open source software development.The results show that there is a general consensus among programmers' views regarding both the favourable and unfavourable aspects that stem from the right to fork. Interestingly, while all programmers noted potential downsides to the right to fork, it was seen by all as an integral component of open source software, and a right that must not be infringed regardless of circumstance or outcome.},
booktitle = {Proceedings of The International Symposium on Open Collaboration},
pages = {1–10},
numpages = {10},
keywords = {fork, free software, code forking, Open source, code reuse},
location = {Berlin, Germany},
series = {OpenSym '14}
}

@proceedings{10.1145/3551902,
title = {EuroPLop '22: Proceedings of the 27th European Conference on Pattern Languages of Programs},
year = {2022},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@inproceedings{10.1145/2466627.2466640,
author = {Schofield, Tom and Vines, John and Higham, Tom and Carter, Ed and Atken, Memo and Golding, Amy},
title = {Trigger Shift: Participatory Design of an Augmented Theatrical Performance with Young People},
year = {2013},
isbn = {9781450321501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2466627.2466640},
doi = {10.1145/2466627.2466640},
abstract = {Trigger Shift was a project that involved collaborating with a group of young people to explore the ways commercially available technologies could be appropriated into performance art. The project led to the production of an augmented theatrical performance using the Microsoft Kinect sensor that was presented to live audiences six times over two days. In this paper we describe the bottom-up, 11-month long participatory design process conducted with our young participants. We describe the manner in which the project was introduced to our participants and the techniques used to help them actively make decisions about the design of and role of technology in the final performance. We candidly report on the problems encountered during the design process and how the project team had to be reflexive to the needs of participants and the single predefined end-goal of the project. A number of strengths and weaknesses of bottom-up participatory design with young people are highlighted, and we reflect upon these to provide guidance for future researchers undertaking work in this domain.},
booktitle = {Proceedings of the 9th ACM Conference on Creativity &amp; Cognition},
pages = {203–212},
numpages = {10},
keywords = {performance theatre, Kinect, participatory design},
location = {Sydney, Australia},
series = {C&amp;C '13}
}

@inproceedings{10.1145/3530190.3534807,
author = {Vigil-Hayes, Morgan and Hossain, Md Nazmul and Elliott, Alexander K and Belding, Elizabeth M. and Zegura, Ellen},
title = {LoRaX: Repurposing LoRa as a Low Data Rate Messaging System to Extend Internet Boundaries},
year = {2022},
isbn = {9781450393478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530190.3534807},
doi = {10.1145/3530190.3534807},
abstract = {Globally, 43% of households lack Internet access, primarily in regions where deployment and/or service costs are prohibitive, including in the least developed countries, rural locations, and regions with high concentrations of ethnic minorities and low-income populations. Unfortunately, this lack of Internet access increasingly equates to a lack of access to essential services, such as healthcare, education, and economic opportunities. In an environment of marginal economics, creative and varied approaches to obtaining access have flourished, including Internet kiosks long popular in the Global South, libraries as public access in the Global North, parking lot use of open WiFi access points, and spectrum-based solutions such as TV whitespace links and citizen band radio. In the near future, local 5G and the deployment of satellite constellations promise yet additional options in the price/performance space for access. In this context we are interested in the following research question: How can the presence of multiple networks, with different price, performance, and geographic reach profiles, be best used in concert to improve access to critical services? We propose that a robust answer to this question bears a holistic, cross-layer examination of new communication paradigms, network architecture innovation, and application design. We make this concrete by running to ground a specific case study of two networks, one high performance yet limited in geographic scope and the other low performance yet pervasive. Specifically our LoRaX (LoRa eXtends the Internet) system combines high bandwidth but non-pervasive Internet access with a low data rate, low power, yet ubiquitious network made possible by IoT developments. By focusing on two networks with extreme differences, we explore a design space that offers users new opportunities for participating in Internet-based services–even when high speed Internet connectivity is intermittent. We also reflect on the generality of the environment and our solution approach for future multi-network settings.},
booktitle = {Proceedings of the 5th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {195–213},
numpages = {19},
keywords = {digital divide, challenged networking environment, LoRa, LPWAN, last mile connectivity, low data rate messaging},
location = {Seattle, WA, USA},
series = {COMPASS '22}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00023,
author = {Ham, MyungJoo and Lim, Geunsik},
title = {Making Configurable and Unified Platform, Ready for Broader Future Devices},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00023},
doi = {10.1109/ICSE-SEIP.2019.00023},
abstract = {The wide spread of IoT and edge devices has introduced new challenges for software platforms of consumer electronics, of which traditional targets had been smart phones, wearable devices, and smart TVs. In general, such traditional devices share well-defined common features and requirements, which emerging IoT devices lack of. Besides, IoT and edge devices have much longer tails, which makes it further intractable to define common features and requirements, composing a customized software platform. Such high diversities prohibits having individual build configurations per device type, which multiplies burdens for developers as well as infrastructures. Besides, IoT developers need to customize platforms for new devices on-the-fly, which traditional platform tools are not capable of; such tasks usually require the rare release and build experts along with significant time and effort. In this paper, we have successfully addressed the issues: unifying a software platform (Tizen) and its infrastructure to increase the developmental productivity for varying device types and making Tizen highly configurable so that novice developers may create their own variations on-the-fly easily and quickly. This has enabled a public web service, Craftroom.tizen.org, where IoT developers may acquire their own customized Tizen on-the-fly. This project is integrated into Tizen since 4.0, which is released to the public, and has enabled Tizen team to start IoT platforms and others to prepare a software platform for autonomous driving systems and on-device AI systems with minimal time and effort.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {141–150},
numpages = {10},
keywords = {software platform, release and deployment, build system, software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@inproceedings{10.1145/1629335.1629358,
author = {Porter, Joseph and Karsai, Gabor and Sztipanovits, Janos},
title = {Towards a Time-Triggered Schedule Calculation Tool to Support Model-Based Embedded Software Design},
year = {2009},
isbn = {9781605586274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629335.1629358},
doi = {10.1145/1629335.1629358},
abstract = {Time-triggered architectures (TTA) provide replica determinism in safety-critical distributed embedded software designs. TTA has become a crucial part of many high-confidence embedded paradigms, as it decouples functional concerns from platform timing concerns in system designs. Complex embedded software development workflows for safety-critical applications are increasingly managed by model-based design tools, in order to support automated verification and reconcile conflicts between functional and non-functional concerns in designs. We present a prototype scheduling tool (ESched) which calculates cyclic schedules for time-triggered networks. ESched supports the model-based workflow of the ESMoL modeling language and tool suite. Using ESMoL, designers can rapidly iterate through simulating a control design, capturing platform effects in models, generating a schedule (if feasible), and re-simulating the control design subject to the platform model and the computed schedule. ESched specifications include a number of useful platform parameters, and it supports troubleshooting of infeasible schedules by allowing the user to specify partial platform models to solve.},
booktitle = {Proceedings of the Seventh ACM International Conference on Embedded Software},
pages = {167–176},
numpages = {10},
keywords = {time triggered architecture, model-based design, constraint programming},
location = {Grenoble, France},
series = {EMSOFT '09}
}

@inproceedings{10.1145/337180.337200,
author = {Michail, Amir},
title = {Data Mining Library Reuse Patterns Using Generalized Association Rules},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337200},
doi = {10.1145/337180.337200},
abstract = {In this paper, we show how data mining can be used to discover library reuse patterns in existing applications. Specifically, we consider the problem of discovering library classes and member functions that are typically reused in combination by application classes. This paper improves upon our earlier research using “association rules” [8] by taking into account the inheritance hierarchy using “generalized association rules”. This turns out to be a non-trivial but worthwhile endeavor.By browsing generalized association rules, a developer can discover patterns in library usage in a way that takes into account inheritance relationships. For example, such a rule might tell us that application classes that inherit from a particular library class often instantiate another class or one of its descendents. We illustrate the approach using our tool, CodeWeb, by demonstrating characteristic ways in which applications reuse classes in the KDE application framework.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {167–176},
numpages = {10},
keywords = {reuse patterns, software libraries, data mining},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3461564.3461583,
author = {Braybrooke, Kit and Janes, Stephanie and Sato, Chihiro},
title = {Care-Full Design Sprints, Online? Addressing Gaps in Cultural Access and Inclusion during Covid-19 with Vulnerable Communities in London and Tokyo},
year = {2021},
isbn = {9781450390569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461564.3461583},
doi = {10.1145/3461564.3461583},
abstract = {What does it mean to invite vulnerable communities to the table in times of crisis not just as subjects, but as co-designers, in ways that facilitate nourishing and care-full relations? In this paper, we present the case of an online design sprint involving two groups of diverse participants in London and Tokyo as the Covid-19 pandemic unfolded. This modified design sprint model, which we describe as a ’care-full design stroll’, integrated co-design approaches with ethics of care to offer remote cultural experiences aimed at addressing inequalities of access and inclusion faced by the arts and cultural sectors in Japan and the UK. We analyse data from ethnographic observations, interviews and surveys in both nations to illustrate the challenges and opportunities of facilitating design sprints online. Our findings show how care-full co-design, underpinned by concepts of thinking-with and working-alongside, can be facilitated in online-only and/or limited terrains, in ways that nourish cultural organisations and their publics in times of great uncertainty. We conclude with a set of six design principles which provide practical recommendations for the effective facilitation of future care-full co-design sprints for groups working in a variety of settings.},
booktitle = {Proceedings of the 10th International Conference on Communities &amp; Technologies - Wicked Problems in the Age of Tech},
pages = {25–37},
numpages = {13},
keywords = {design sprints, care, co-design, cultural heritage, covid19, digital inclusion, access},
location = {Seattle, WA, USA},
series = {C&amp;T '21}
}

@article{10.1109/TNET.2022.3150000,
author = {Nadig, Deepak and Ramamurthy, Byrav and Bockelman, Brian},
title = {SNAG: SDN-Managed Network Architecture for GridFTP Transfers Using Application-Awareness},
year = {2022},
issue_date = {Aug. 2022},
publisher = {IEEE Press},
volume = {30},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3150000},
doi = {10.1109/TNET.2022.3150000},
abstract = {Increasingly, academic campus networks support large-scale data transfer workflows for data-intensive science. These data transfers rely on high-performance, scalable, and reliable protocols for moving large amounts of data over a high-bandwidth, high-latency network. GridFTP is a widely used protocol for wide area network (WAN) data movement. However, as the GridFTP protocol does not share connection information with the network-layer, network operators have reduced flexibility, particularly in identifying/managing flows across the network. We address this problem by deploying a production “ &lt;italic&gt;application-aware&lt;/italic&gt; ” software defined network (SDN) for managing GridFTP transfers for data-intensive science workflows. We first propose a novel application-aware architecture called SNAG (SDN-managed Network Architecture for GridFTP transfers). SNAG combines &lt;italic&gt;application-layer and network-layer collaboration&lt;/italic&gt; (termed “application-awareness”) with SDN-enabled network management to classify, monitor and to manage network resources actively. Until now, our SNAG deployment has successfully classified over &lt;italic&gt;1.5 Billion&lt;/italic&gt; GridFTP connections at the Holland Computing Center (HCC), University of Nebraska-Lincoln (UNL). Next, we develop an application-aware SDN system to provide differentiated network services for distributed computing workflows. At HCC, we also demonstrate how our system ensures the quality of service (QoS) for high-throughput workflows such as Compact Muon Solenoid (CMS) and Laser Interferometer Gravitational-Wave Observatory (LIGO). Further, we also demonstrate how application-aware SDN can be exploited to create &lt;italic&gt;policy-driven&lt;/italic&gt; approaches to achieve accurate resource accounting for each workflow. We present strategies for implementing differentiated network services and discuss their capacity improvement benefits. Lastly, we provide some guidelines and recommendations for developing application-aware SDN architectures for general-purpose applications.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {1585–1598},
numpages = {14}
}

@inproceedings{10.1145/3243734.3243759,
author = {Ujcich, Benjamin E. and Jero, Samuel and Edmundson, Anne and Wang, Qi and Skowyra, Richard and Landry, James and Bates, Adam and Sanders, William H. and Nita-Rotaru, Cristina and Okhravi, Hamed},
title = {Cross-App Poisoning in Software-Defined Networking},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243759},
doi = {10.1145/3243734.3243759},
abstract = {Software-defined networking (SDN) continues to grow in popularity because of its programmable and extensible control plane realized through network applications (apps). However, apps introduce significant security challenges that can systemically disrupt network operations, since apps must access or modify data in a shared control plane state. If our understanding of how such data propagate within the control plane is inadequate, apps can co-opt other apps, causing them to poison the control plane's integrity. We present a class of SDN control plane integrity attacks that we call cross-app poisoning (CAP), in which an unprivileged app manipulates the shared control plane state to trick a privileged app into taking actions on its behalf. We demonstrate how role-based access control (RBAC) schemes are insufficient for preventing such attacks because they neither track information flow nor enforce information flow control (IFC). We also present a defense, ProvSDN, that uses data provenance to track information flow and serves as an online reference monitor to prevent CAP attacks. We implement ProvSDN on the ONOS SDN controller and demonstrate that information flow can be tracked with low-latency overheads.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {648–663},
numpages = {16},
keywords = {network operating system, information flow control, data provenance, software-defined networking},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.14778/3611540.3611547,
author = {Saxena, Mohit and Sowell, Benjamin and Alamgir, Daiyan and Bahadur, Nitin and Bisht, Bijay and Chandrachood, Santosh and Keswani, Chitti and Krishnamoorthy, G. and Lee, Austin and Li, Bohou and Mitchell, Zach and Porwal, Vaibhav and Chappidi, Maheedhar Reddy and Ross, Brian and Sekiyama, Noritaka and Zaki, Omer and Zhang, Linchi and Shah, Mehul A.},
title = {The Story of AWS Glue},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611547},
doi = {10.14778/3611540.3611547},
abstract = {AWS Glue is Amazon's serverless data integration cloud service that makes it simple and cost effective to extract, clean, enrich, load, and organize data. Originally launched in August 2017, AWS Glue began as an extract-transform-load (ETL) service designed to relieve developers and data engineers of the undifferentiated heavy lifting needed to load databases, data warehouses, and build data lakes on Amazon S3. Since then, it has evolved to serve a larger audience including ETL specialists and data scientists, and includes a broader suite of data integration capabilities. Today, hundreds of thousands of customers use AWS Glue every month.In this paper, we describe the use cases and challenges cloud customers face in preparing data for analytics and the tenets we chose to drive Glue's design. We chose early on to focus on ease-of-use, scale, and extensibility. At its core, Glue offers serverless Apache Spark and Python engines backed by a purpose-built resource manager for fast startup and auto-scaling. In Spark, it offers a new data structure --- DynamicFrames --- for manipulating messy schema-free semi-structured data such as event logs, a variety of transformations and tooling to simplify data preparation, and a new shuffle plugin to offload to cloud storage. It also includes a Hivemetastore compatible Data Catalog with Glue crawlers to build and manage metadata, e.g. for data lakes on Amazon S3. Finally, Glue Studio is its visual interface for authoring Spark and Python-based ETL jobs. We describe the innovations that differentiate AWS Glue and drive its popularity and how it has evolved over the years.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3557–3569},
numpages = {13}
}

@inproceedings{10.1145/2976749.2978382,
author = {Ruef, Andrew and Hicks, Michael and Parker, James and Levin, Dave and Mazurek, Michelle L. and Mardziel, Piotr},
title = {Build It, Break It, Fix It: Contesting Secure Development},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978382},
doi = {10.1145/2976749.2978382},
abstract = {Typical security contests focus on breaking or mitigating the impact of buggy systems. We present the Build-it, Break-it, Fix-it (BIBIFI) contest, which aims to assess the ability to securely build software, not just break it. In BIBIFI, teams build specified software with the goal of maximizing correctness, performance, and security. The latter is tested when teams attempt to break other teams' submissions. Winners are chosen from among the best builders and the best breakers. BIBIFI was designed to be open-ended-teams can use any language, tool, process, etc. that they like. As such, contest outcomes shed light on factors that correlate with successfully building secure software and breaking insecure software. During 2015, we ran three contests involving a total of 116 teams and two different programming problems. Quantitative analysis from these contests found that the most efficient build-it submissions used C/C++, but submissions coded in other statically-typed languages were less likely to have a security flaw; build-it teams with diverse programming-language knowledge also produced more secure code. Shorter programs correlated with better scores. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {690–703},
numpages = {14},
keywords = {software engineering, security},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/949305.949330,
author = {Chen, G. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Mathiske, B. and Wolczko, M.},
title = {Heap Compression for Memory-Constrained Java Environments},
year = {2003},
isbn = {1581137125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949305.949330},
doi = {10.1145/949305.949330},
abstract = {Java is becoming the main software platform for consumer and embedded devices such as mobile phones, PDAs, TV set-top boxes, and in-vehicle systems. Since many of these systems are memory constrained, it is extremely important to keep the memory footprint of Java applications under control.The goal of this work is to enable the execution of Java applications using a smaller heap footprint than that possible using current embedded JVMs. We propose a set of memory management strategies to reduce heap footprint of embedded Java applications that execute under severe memory constraints. Our first contribution is a new garbage collector, referred to as the Mark-Compact-Compress (MCC) collector, that allows an application to run with a heap smaller than its footprint. An important characteristic of this collector is that it compresses objects when heap compaction is not sufficient for creating space for the current allocation request. In addition to employing compression, we also consider a heap management strategy and associated garbage collector, called MCL (Mark-Compact-Lazy Allocate), based on lazy allocation of object portions. This new collector operates like the conventional Mark-Compact (MC) collector, but takes advantage of the observation that many Java applications create large objects, of which only a small portion is actually used. In addition, we also combine MCC and MCL, and present MCCL (Mark-Compact-Compress-Lazy Al-locate), which outperforms both MCC and MCL.We have implemented these collectors using KVM, and performed extensive experiments using a set of ten embedded Java applications. We have found our new garbage collection strategies to be useful in two main aspects. First, they reduce the minimum heap size necessary to execute an application without out-of-memory exception. Second, our strategies reduce the heap occupancy. That is, at a given time, they reduce the heap memory requirement of the application being executed. We have also conducted experiments with a more aggressive object compression strategy and discussed its main advantages.},
booktitle = {Proceedings of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programing, Systems, Languages, and Applications},
pages = {282–301},
numpages = {20},
keywords = {Java virtual machine, memory compression, heap, garbage collection},
location = {Anaheim, California, USA},
series = {OOPSLA '03}
}

@inproceedings{10.1109/ICSE48619.2023.00095,
author = {Wu, Yulun and Yu, Zeliang and Wen, Ming and Li, Qiang and Zou, Deqing and Jin, Hai},
title = {Understanding the Threats of Upstream Vulnerabilities to Downstream Projects in the Maven Ecosystem},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00095},
doi = {10.1109/ICSE48619.2023.00095},
abstract = {Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales.This study aims to bridge such gaps. In particular, we collect 44,450 instances of 〈CVE, upstream, downstream〉 relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reachability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1046–1058},
numpages = {13},
keywords = {vulnerability, ecosystem security, maven},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@article{10.1145/1255421.1255422,
author = {Henning, Michi},
title = {API: Design Matters: Why Changing APIs Might Become a Criminal Offense.},
year = {2007},
issue_date = {May-June 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {1542-7730},
url = {https://doi.org/10.1145/1255421.1255422},
doi = {10.1145/1255421.1255422},
abstract = {After more than 25 years as a software engineer, I still find myself underestimating the time it will take to complete a particular programming task. Sometimes, the resulting schedule slip is caused by my own shortcomings: as I dig into a problem, I simply discover that it is a lot harder than I initially thought, so the problem takes longer to solve—such is life as a programmer. Just as often I know exactly what I want to achieve and how to achieve it, but it still takes far longer than anticipated. When that happens, it is usually because I am struggling with an API that seems to do its level best to throw rocks in my path and make my life difficult. What I find telling is that, after 25 years of progress in software engineering, this still happens. Worse, recent APIs implemented in modern programming languages make the same mistakes as their two-decade-old counterparts written in C. There seems to be something elusive about API design that, despite many years of progress, we have yet to master.},
journal = {Queue},
month = {may},
pages = {24–36},
numpages = {13}
}

@inproceedings{10.1109/MICRO.2006.40,
author = {Lu, Shan and Zhou, Pin and Liu, Wei and Zhou, Yuanyuan and Torrellas, Josep},
title = {PathExpander: Architectural Support for Increasing the Path Coverage of Dynamic Bug Detection},
year = {2006},
isbn = {0769527329},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MICRO.2006.40},
doi = {10.1109/MICRO.2006.40},
abstract = {Dynamic software bug detection tools are commonly used because they leverage run-time information. However, they suffer from a fundamental limitation, the Path Coverage Problem: they detect bugs only in taken paths but not in non-taken paths. In other words, they require bugs to be exposed in the monitored execution. This paper makes one of the first attempts to address this fundamental problem with a simple hardware extension. First, we propose PathExpander , a novel design that dynamically increases the code path coverage of dynamic bug detection tools with no programmer involvement. As a program executes, PathExpander selectively executes non-taken paths in a sandbox without side effects. This enables dynamic bug detection tools to find bugs that are present in these non-taken paths and would otherwise not be detected. Second, we propose a simple hardware extension to control the huge overhead in its pure software implementation to a moderate level. To further minimize overhead, PathExpander provides an optimization option to execute non-taken paths on idle cores in chip multi-processor architectures that support speculative execution. To evaluate PathExpander, we use three dynamic bug detection methods: dynamic software-only checker (CCured), dynamic hardware-assisted checker (iWatcher) and assertions; and conduct side-by-side comparison with PathExpander's counterpart software implementation. Our experiments with seven buggy programs using general inputs that do not expose the tested bugs show that PathExpander is able to help these tools detect 21 (out of 38) tested bugs that are otherwise missed. This is because PathExpander increases the code coverage of each test case from 40% to 65% on average, based on the branch coverage metric. When applications are tested with multiple inputs, the cumulative coverage also significantly improves by 19%. We also show that PathExpander introduces modest false positives (4 on average) and overhead (less than 9.9%). The 3.4 orders of magnitude lower overhead compared with pure-software implementation further justifies the hardware design in PathExpander.},
booktitle = {Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {38–52},
numpages = {15},
series = {MICRO 39}
}

@inproceedings{10.1145/1960314.1960325,
author = {Dinn, Andrew E.},
title = {Flexible, Dynamic Injection of Structured Advice Using Byteman},
year = {2011},
isbn = {9781450306065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960314.1960325},
doi = {10.1145/1960314.1960325},
abstract = {Byteman is a flexible, dynamic advice injection tool for Java. It uses an Event Condition Action rule script language to structure placement and control execution of injected Java code fragments. The Byteman agent can be loaded at application startup or into a running application and rules can be loaded, unloaded and reloaded while the application continues running.This makes Byteman ideal for use in live deployments, simplifying tracing and diagnosis of bugs and investigation and monitoring of performance issues. Byteman is also very useful during all stages of testing. Tests can inject rules into an application or subsystem during test initialisation or as the test progresses, redirecting or breaking the application code at strategic points and validating application behaviour. This limits the need to write harness or mock code, and minimises the extent to which the test distorts normal execution, ensuring that test conditions are as close to live running as possible.Byteman is being used widely within JBoss to perform unit, integration and system testing. Our customer support team use it to perform tracing, debugging and validation of JVM, JBoss and customer code, both inhouse and in live deployments. We are currently developing an extensible, reconfigurable statistical performance monitoring package for JBoss AS based on Byteman. We expect this to be easily extended by JBoss AS customers to enable monitoring of their own applications.This paper employs real examples of these use cases to demonstrate how Byteman works. It shows how Byteman makes advice injection simpler, quicker and more interactive than alternative tools without sacrificing power or rigour.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development Companion},
pages = {41–50},
numpages = {10},
keywords = {injection, advice, tools, dynamic, testing, java},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@inproceedings{10.1145/3448016.3457559,
author = {Zhou, Jingyu and Xu, Meng and Shraer, Alexander and Namasivayam, Bala and Miller, Alex and Tschannen, Evan and Atherton, Steve and Beamon, Andrew J. and Sears, Rusty and Leach, John and Rosenthal, Dave and Dong, Xin and Wilson, Will and Collins, Ben and Scherer, David and Grieser, Alec and Liu, Young and Moore, Alvin and Muppana, Bhaskar and Su, Xiaoge and Yadav, Vishesh},
title = {FoundationDB: A Distributed Unbundled Transactional Key Value Store},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457559},
doi = {10.1145/3448016.3457559},
abstract = {FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2653–2666},
numpages = {14},
keywords = {simulation testing, optimistic concurrency control, multiversion concurrency control, oltp, unbundled database, strict serializability},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/2786805.2786877,
author = {Dhar, Aritra and Purandare, Rahul and Dhawan, Mohan and Rangaswamy, Suresh},
title = {CLOTHO: Saving Programs from Malformed Strings and Incorrect String-Handling},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786877},
doi = {10.1145/2786805.2786877},
abstract = {Software is susceptible to malformed data originating from untrusted sources. Occasionally the programming logic or constructs used are inappropriate to handle the varied constraints imposed by legal and well-formed data. Consequently, softwares may produce unexpected results or even crash. In this paper, we present CLOTHO, a novel hybrid approach that saves such softwares from crashing when failures originate from malformed strings or inappropriate handling of strings. CLOTHO statically analyses a program to identify statements that are vulnerable to failures related to associated string data. CLOTHO then generates patches that are likely to satisfy constraints on the data, and in case of failures produces program behavior which would be close to the expected. The precision of the patches is improved with the help of a dynamic analysis. We have implemented CLOTHO for the JAVA String API, and our evaluation based on several popular open-source libraries shows that CLOTHO generates patches that are semantically similar to the patches generated by the programmers in the later versions. Additionally, these patches are activated only when a failure is detected, and thus CLOTHO incurs no runtime overhead during normal execution, and negligible overhead in case of failures.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {555–566},
numpages = {12},
keywords = {Automatic Program Repair, Strings, Program Analysis},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3320269.3384736,
author = {Yan, Shengbo and Wu, Chenlu and Li, Hang and Shao, Wei and Jia, Chunfu},
title = {PathAFL: Path-Coverage Assisted Fuzzing},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384736},
doi = {10.1145/3320269.3384736},
abstract = {Fuzzing is an effective method to find software bugs and vulnerabilities. One of the most useful techniques is the coverage-guided fuzzing, whose key element is the tracing code coverage information. Existing coverage-guided fuzzers generally use the the number of basic blocks or edges explored to measure code coverage. Path-coverage can provide more accurate coverage information than basic block and edge coverage. However, the number of paths grows exponentially as the size of a program increases. It is almost impossible to trace all the paths of a real-world application. In this paper, we propose a fuzzing solution named PathAFL, which assists a fuzzer by path identification. It can effectively identify and utilize the important h-path, which is a new path but whose edges have all been touched previously. First, PathAFL only inserts one assembly instruction to AFL's original code to calculate the path hash, and uses a selective instrumentation strategy to reduce the tracing granularity of an execution path. Second, we design a fast filtering algorithm to choose higher weight paths from a large number of h-paths and add them to the seed queue. Third, both the seed selection algorithm and the power schedule are implemented based on the path weight. Finally we implemented PathAFL based on the popular fuzzer AFL and evaluated it on 10 well-fuzzed benchmark programs. In 24 hours, PathAFL explored 38% more paths and 9.3% more edges than AFL. Compared with CollAFL-x, the number is 25% and 5.9% correspondingly. Moreover, PathAFL found the more bugs on the LAVA-M dataset, even four unlisted bugs. The results show that PathAFL outperforms the previous fuzzers in terms of both code coverage and bug discovery. In well-tested programs, PathAFL found 8 new security bugs with 6 CVEs assigned.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {598–609},
numpages = {12},
keywords = {path coverage, vulnerability, instrumentation, fuzzing},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/1755913.1755932,
author = {Chipounov, Vitaly and Candea, George},
title = {Reverse Engineering of Binary Device Drivers with RevNIC},
year = {2010},
isbn = {9781605585772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1755913.1755932},
doi = {10.1145/1755913.1755932},
abstract = {This paper presents a technique that helps automate the reverse engineering of device drivers. It takes a closed-source binary driver, automatically reverse engineers the driver's logic, and synthesizes new device driver code that implements the exact same hardware protocol as the original driver. This code can be targeted at the same or a different OS. No vendor documentation or source code is required. Drivers are often proprietary and available for only one or two operating systems, thus restricting the range of device support on all other OSes. Restricted device support leads to low market viability of new OSes and hampers OS researchers in their efforts to make their ideas available to the 'real world.' Reverse engineering can help automate the porting of drivers, as well as produce replacement drivers with fewer bugs and fewer security vulnerabilities. Our technique is embodied in RevNIC, a tool for reverse engineering network drivers. We use RevNIC to reverse engineer four proprietary Windows drivers and port them to four different OSes, both for PCs and embedded systems. The synthesized network drivers deliver performance nearly identical to that of the original drivers.},
booktitle = {Proceedings of the 5th European Conference on Computer Systems},
pages = {167–180},
numpages = {14},
keywords = {proprietary software, reverse engineering, binary, device drivers, closed-source},
location = {Paris, France},
series = {EuroSys '10}
}

@article{10.1145/1506409.1506424,
author = {Henning, Michi},
title = {API Design Matters},
year = {2009},
issue_date = {May 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/1506409.1506424},
doi = {10.1145/1506409.1506424},
abstract = {Bad application programming interfaces plague software engineering. How do we get things right?},
journal = {Commun. ACM},
month = {may},
pages = {46–56},
numpages = {11}
}

@inproceedings{10.1145/2901318.2901326,
author = {Lozi, Jean-Pierre and Lepers, Baptiste and Funston, Justin and Gaud, Fabien and Qu\'{e}ma, Vivien and Fedorova, Alexandra},
title = {The Linux Scheduler: A Decade of Wasted Cores},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901326},
doi = {10.1145/2901318.2901326},
abstract = {As a central part of resource management, the OS thread scheduler must maintain the following, simple, invariant: make sure that ready threads are scheduled on available cores. As simple as it may seem, we found that this invariant is often broken in Linux. Cores may stay idle for seconds while ready threads are waiting in runqueues. In our experiments, these performance bugs caused many-fold performance degradation for synchronization-heavy scientific applications, 13% higher latency for kernel make, and a 14-23% decrease in TPC-H throughput for a widely used commercial database. The main contribution of this work is the discovery and analysis of these bugs and providing the fixes. Conventional testing techniques and debugging tools are ineffective at confirming or understanding this kind of bugs, because their symptoms are often evasive. To drive our investigation, we built new tools that check for violation of the invariant online and visualize scheduling activity. They are simple, easily portable across kernel versions, and run with a negligible overhead. We believe that making these tools part of the kernel developers' tool belt can help keep this type of bug at bay.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {1},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@article{10.1145/3610220,
author = {Cheng, Kathy and Cuvin, Phil and Olechowski, Alison and Zhou, Shurui},
title = {User Perspectives on Branching in Computer-Aided Design},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610220},
doi = {10.1145/3610220},
abstract = {Branching is a feature of distributed version control systems that facilitates the "divide and conquer" strategy present in complex and collaborative work domains. Branching has revolutionized modern software development and has the potential to similarly transform hardware product development via CAD (computer-aided design). Yet, contrasting with its status in software, branching as a feature of commercial CAD systems is in its infancy, and little research exists to investigate its use in the digital design and development of physical products. To address this knowledge gap, in this paper, we mine and analyze 719 user-generated posts from online CAD forums to qualitatively study designers' intentions for and preliminary use of branching in CAD. Our work contributes a taxonomy of CAD branching use cases, an identification of deficiencies of existing branching capabilities in CAD, and a discussion of the untapped potential of CAD branching to support a new paradigm of collaborative mechanical design. The insights gained from this study may help CAD tool developers address design shortcomings in CAD branching tools and assist CAD practitioners by raising their awareness of CAD branching to improve design efficiency and collaborative workflows in hardware development teams.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {371},
numpages = {30},
keywords = {software configuration management, product data management, hardware design and development, computer-aided design, version control}
}

@article{10.1145/2089094.2089099,
author = {Gretarsson, Brynjar and O’Donovan, John and Bostandjiev, Svetlin and H\"{o}llerer, Tobias and Asuncion, Arthur and Newman, David and Smyth, Padhraic},
title = {TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089099},
doi = {10.1145/2089094.2089099},
abstract = {We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which TopicNets enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful NSF grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a PhD thesis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {feb},
articleno = {23},
numpages = {26},
keywords = {Topic modeling, graph visualization, text visualization}
}

@inproceedings{10.1145/3126908.3126957,
author = {Proctor, W. Cyrus and Storm, Patrick and Hanlon, Matthew R. and Mendoza, Nathaniel},
title = {Securing HPC: Development of a Low Cost, Open Source Multi-Factor Authentication Infrastructure},
year = {2017},
isbn = {9781450351140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126908.3126957},
doi = {10.1145/3126908.3126957},
abstract = {Multi-factor authentication (MFA) is rapidly becoming the de facto standard for access to all computing, whether via web, phone, or direct command-line access. HPC centers and other institutions supporting hundreds or thousands of users face challenging cost, licensing, user support, and infrastructure deployment decisions when considering a transition to MFA at scale.This paper describes our experiences and lessons learned throughout the assessment, planning, and phased deployment of MFA across production systems supporting more than 10,000 accounts. It focuses on the ultimate curation, creation, and integration of a multitude of software components, some developed in-house and built to be compatible within existing HPC environments, and all of which are freely available for open source distribution. We motivate the development of this customized infrastructure by highlighting some of the particular needs of our research community. What follows is an information resource for others when considering their own MFA deployments.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {37},
numpages = {11},
keywords = {multi-factor authentication, open source infrastructure, large-scale deployment},
location = {Denver, Colorado},
series = {SC '17}
}

@inproceedings{10.1145/3084863.3084864,
author = {Mokhov, Serguei A. and Song, Miao and Mudur, Sudhir P. and Grogono, Peter},
title = {Hands-on: Rapid Interactive Application Prototyping for Media Arts and Stage Performance},
year = {2017},
isbn = {9781450350099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084863.3084864},
doi = {10.1145/3084863.3084864},
abstract = {We complement the last two editions of the of the course at SIG-GRAPH Asia to make it more of a hands-on nature. We explore a rapid prototyping of interactive graphical applications using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices such as, Kinect, Wii, iDevice-based controls, and others. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2.},
booktitle = {ACM SIGGRAPH 2017 Studio},
articleno = {3},
numpages = {29},
keywords = {OpenGL, real-time, jitter/MAX, human-computer interfaces, processing, interaction, illimitable space system (ISS), kinect, computer graphics education},
location = {Los Angeles, California},
series = {SIGGRAPH '17}
}

@inproceedings{10.1145/2882903.2882904,
author = {Wang, Li and Zhou, Minqi and Zhang, Zhenjie and Yang, Yin and Zhou, Aoying and Bitton, Dina},
title = {Elastic Pipelining in an In-Memory Database Cluster},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882904},
doi = {10.1145/2882903.2882904},
abstract = {An in-memory database cluster consists of multiple interconnected nodes with a large capacity of RAM and modern multi-core CPUs. As a conventional query processing strategy, pipelining remains a promising solution for in-memory parallel database systems, as it avoids expensive intermediate result materialization and parallelizes the data processing among nodes. However, to fully unleash the power of pipelining in a cluster with multi-core nodes, it is crucial for the query optimizer to generate good query plans with appropriate intra-node parallelism, in order to maximize CPU and network bandwidth utilization. A suboptimal plan, on the contrary, causes load imbalance in the pipelines and consequently degrades the query performance. Parallelism assignment optimization at compile time is nearly impossible, as the workload in each node is affected by numerous factors and is highly dynamic during query evaluation. To tackle this problem, we propose elastic pipelining, which makes it possible to optimize intra-node parallelism assignments in the pipelines based on the actual workload at runtime. It is achieved with the adoption of new elastic iterator model and a fully optimized dynamic scheduler. The elastic iterator model generally upgrades traditional iterator model with new dynamic multi-core execution adjustment capability. And the dynamic scheduler efficiently provisions CPU cores to query execution segments in the pipelines based on the light-weight measurements on the operators. Extensive experiments on real and synthetic (TPC-H) data show that our proposal achieves almost full CPU utilization on typical decision-making analytical queries, outperforming state-of-the-art open-source systems by a huge margin.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1279–1294},
numpages = {16},
keywords = {numa architecture, in-memory database, distributed query processing, multi-core, query processing},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00009,
author = {Tan, Shin Hwei and Hu, Chunfeng and Li, Ziqiang and Zhang, Xiaowen and Zhou, Ying},
title = {GitHub-OSS Fixit},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00009},
doi = {10.1109/ICSE-SEET52601.2021.00009},
abstract = {Many studies have shown the benefits of introducing open-source projects into teaching Software Engineering (SE) courses. However, there are several limitations of existing studies that limit the wide adaptation of open-source projects in a classroom setting, including (1) the selected project is limited to one particular project, (2) most studies only investigated on its effect on teaching a specific SE concept, and (3) students may make mistakes in their contribution which leads to poor quality code. Meanwhile, software companies have successfully launched programs like Google Summer of Code (GSoC) and FindBugs "fixit" to contribute to open-source projects. Inspired by the success of these programs, we propose GitHub-OSS Fixit, a team-based course project where students are taught to contribute to open-source Java projects by fixing bugs reported in GitHub. We described our course outline to teach students SE concepts by encouraging the usages of several automated program analysis tools. We also included the carefully designed instructions that we gave to students for participating in GitHub-OSS Fixit. As all lectures and labs are conducted online, we think that our course design could help in guiding future online SE courses. Overall, our survey results show that students think that GitHub-OSS Fixit could help them to improve many skills and apply the knowledge taught in class. In total, 154 students have submitted 214 pull requests to 24 different Java projects, in which 93 of them have been merged, and 46 have been closed by developers.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {1–10},
numpages = {10},
keywords = {open-source software, program repair, software engineering},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@inproceedings{10.1145/2818000.2818003,
author = {Coker, Zack and Maass, Michael and Ding, Tianyuan and Le Goues, Claire and Sunshine, Joshua},
title = {Evaluating the Flexibility of the Java Sandbox},
year = {2015},
isbn = {9781450336826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818000.2818003},
doi = {10.1145/2818000.2818003},
abstract = {The ubiquitously-installed Java Runtime Environment (JRE) provides a complex, flexible set of mechanisms that support the execution of untrusted code inside a secure sandbox. However, many recent exploits have successfully escaped the sandbox, allowing attackers to infect numerous Java hosts. We hypothesize that the Java security model affords developers more flexibility than they need or use in practice, and thus its complexity compromises security without improving practical functionality. We describe an empirical study of the ways benign open-source Java applications use and interact with the Java security manager. We found that developers regularly misunderstand or misuse Java security mechanisms, that benign programs do not use all of the vast flexibility afforded by the Java security model, and that there are clear differences between the ways benign and exploit programs interact with the security manager. We validate these results by deriving two restrictions on application behavior that restrict (1) security manager modifications and (2) privilege escalation. We demonstrate that enforcing these rules at runtime stop a representative proportion of modern Java 7 exploits without breaking backwards compatibility with benign applications. These practical rules should be enforced in the JRE to fortify the Java sandbox.},
booktitle = {Proceedings of the 31st Annual Computer Security Applications Conference},
pages = {1–10},
numpages = {10},
location = {Los Angeles, CA, USA},
series = {ACSAC '15}
}

@inproceedings{10.1145/2988458.2988460,
author = {Song, Miao and Mokhov, Serguei A. and Mudur, Sudhir P. and Grogono, Peter},
title = {Hands-on: Rapid Interactive Application Prototyping for Media Arts and Stage Production},
year = {2016},
isbn = {9781450345385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988458.2988460},
doi = {10.1145/2988458.2988460},
abstract = {We expand on the last year's successful first edition of the course to make it more of a hands-on nature. We explore a rapid prototyping of interactive graphical applications using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices such as, Kinect, Wii, iDevice-based controls, and others. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2.},
booktitle = {SIGGRAPH ASIA 2016 Courses},
articleno = {19},
numpages = {29},
keywords = {jitter/MAX, human-computer interfaces, computer graphics education, real-time, OpenGL, processing, kinect, interaction},
location = {Macau},
series = {SA '16}
}

@inproceedings{10.1145/3338906.3338909,
author = {Davis, James C. and Michael IV, Louis G. and Coghlan, Christy A. and Servant, Francisco and Lee, Dongyoon},
title = {Why Aren’t Regular Expressions a Lingua Franca? An Empirical Study on the Re-Use and Portability of Regular Expressions},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338909},
doi = {10.1145/3338906.3338909},
abstract = {This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics?  In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language.We experimentally evaluated the riskiness of this practice using a novel regex corpus — 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences.  We report that developers’ belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {443–454},
numpages = {12},
keywords = {portability, empirical software engineering, re-use, mining software repositories, Regular expressions, developer perceptions, ReDoS},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/77276.77282,
author = {Phillips, D. and Vais, P. and Perlman, S. and Lantz, K. and Picco, M.},
title = {The Multi-Media Workstation},
year = {1989},
isbn = {0897913531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/77276.77282},
doi = {10.1145/77276.77282},
abstract = {Good afternoon, ladies and gentlemen. Thank you very much for
taking time out from the parties to join us for one of the
peripheral activities of SIGGRAPH. As you know, the panel that
we're going to be holding this afternoon is entitled the
Multi-Media Workstation. Before I make some introductory remarks, I
am required to make some administrative remarks.The first thing is to remind you that the proceedings of all of
the panels are being audio taped this year for subsequent
transcription and publication. What that means, is that when we
have the audience interaction, please come to the microphones that
are scattered around the floor to make your remarks. Otherwise, I
won't be able to recognize you.The second thing I want to mention to you is that when we're
done at 5:15 we are going to vacate the stage. We're going to
vacate the room so the AV people can lock up. If you want to
continue discussion with us, there's a breakout room that's been
set aside, Salon J, which is down around the corner. So join us
there please, because we'll be scooting out of here right away.Finally, I need to tell you that the --- for those of you who
are involved --- the Pioneers Reception will be held between 6:00
and 9:00 at the Computer Museum this evening, and buses will leave
from the Boylston Street exit of the Convention Center at 5:00,
5:30 and 6:00. Absolutely no video or audio taping allowed at the
Pioneers. You don't want to hear any of those old reminiscences
repeated.Let's get on with the business of the afternoon. Multi-Media
Workstations. A couple of preliminary remarks that I think all of
my colleagues up here will agree with. The things that we're going
to be discussing this afternoon do not represent fundamentally new
technologies. You've been able to buy add-in video cards and audio
devices for personal computers and workstations for some years now.
What we are going to be addressing is a confluence of many
technologies --- hardware and software --- that has finally made it
possible to envision a fully integrated system that will
incorporate all of these multi-media capabilities. So we're giving
you a vision of maybe not what you're seeing at this year's
SIGGRAPH, but certainly a SIGGRAPH or two from now, I can
confidently predict that you're going to be seeing workstations
that incorporate the kinds of capabilities that you'll hear
discussed this afternoon.I should also emphasize that we are not here to give the kind of
a presentation that you might expect from a group of folks --- from
the Media Lab or from Xerox PARC who are going to tell you about
some of the far-out kinds of things that they're working on. I
emphasize again the technologies that are being described this
afternoon are almost here and now, and will soon be available to
you.Now let me make some comments about how in my particular
environment I came to be interested in the concept of a multi-media
workstation. I think each of us will probably have different
stories to tell about why multi-media is important to the kinds of
applications that we're involved with or envision.},
booktitle = {ACM SIGGRAPH 89 Panel Proceedings},
pages = {93–109},
numpages = {17},
location = {Boston, Massachusetts, USA},
series = {SIGGRAPH '89}
}

@inproceedings{10.1145/3132747.3132786,
author = {Levy, Amit and Campbell, Bradford and Ghena, Branden and Giffin, Daniel B. and Pannuto, Pat and Dutta, Prabal and Levis, Philip},
title = {Multiprogramming a 64kB Computer Safely and Efficiently},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132786},
doi = {10.1145/3132747.3132786},
abstract = {Low-power microcontrollers lack some of the hardware features and memory resources that enable multiprogrammable systems. Accordingly, microcontroller-based operating systems have not provided important features like fault isolation, dynamic memory allocation, and flexible concurrency. However, an emerging class of embedded applications are software platforms, rather than single purpose devices, and need these multiprogramming features. Tock, a new operating system for low-power platforms, takes advantage of limited hardware-protection mechanisms as well as the type-safety features of the Rust programming language to provide a multiprogramming environment for microcontrollers. Tock isolates software faults, provides memory protection, and efficiently manages memory for dynamic application workloads written in any language. It achieves this while retaining the dependability requirements of long-running applications.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {234–251},
numpages = {18},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{10.1145/2488222.2488267,
author = {Aniello, Leonardo and Baldoni, Roberto and Querzoni, Leonardo},
title = {Adaptive Online Scheduling in Storm},
year = {2013},
isbn = {9781450317580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488222.2488267},
doi = {10.1145/2488222.2488267},
abstract = {Today we are witnessing a dramatic shift toward a data-driven economy, where the ability to efficiently and timely analyze huge amounts of data marks the difference between industrial success stories and catastrophic failures. In this scenario Storm, an open source distributed realtime computation system, represents a disruptive technology that is quickly gaining the favor of big players like Twitter and Groupon. A Storm application is modeled as a topology, i.e. a graph where nodes are operators and edges represent data flows among such operators. A key aspect in tuning Storm performance lies in the strategy used to deploy a topology, i.e. how Storm schedules the execution of each topology component on the available computing infrastructure.In this paper we propose two advanced generic schedulers for Storm that provide improved performance for a wide range of application topologies. The first scheduler works offline by analyzing the topology structure and adapting the deployment to it; the second scheduler enhance the previous approach by continuously monitoring system performance and rescheduling the deployment at run-time to improve overall performance. Experimental results show that these algorithms can produce schedules that achieve significantly better performances compared to those produced by Storm's default scheduler.},
booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
pages = {207–218},
numpages = {12},
keywords = {distributed event processing, scheduling, storm, cep},
location = {Arlington, Texas, USA},
series = {DEBS '13}
}

@proceedings{10.1145/3214907,
title = {SIGGRAPH '18: ACM SIGGRAPH 2018 Emerging Technologies},
year = {2018},
isbn = {9781450358101},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Changing the Human ExperienceSIGGRAPH 2018 showcases emerging technologies that are exponentially expanding our human experience. The internet of things, the quantitative self, and immersive technologies have matured, and the new data systems that support these innovations make way for more invention and interconnectedness.A variety of new research at SIGGRAPH 2018 will be showcased, including work focused on personal vicissitude; new products and systems to surround us in comfort and function; and new technologies designed to change the way we will approach sports, games and active watching.},
location = {Vancouver, British Columbia, Canada}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3594671,
title = {Programming '23: Companion Proceedings of the 7th International Conference on the Art, Science, and Engineering of Programming},
year = {2023},
isbn = {9798400707551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@article{10.1145/2767005,
author = {Ardagna, Claudio A. and Asal, Rasool and Damiani, Ernesto and Vu, Quang Hieu},
title = {From Security to Assurance in the Cloud: A Survey},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2767005},
doi = {10.1145/2767005},
abstract = {The cloud computing paradigm has become a mainstream solution for the deployment of business processes and applications. In the public cloud vision, infrastructure, platform, and software services are provisioned to tenants (i.e., customers and service providers) on a pay-as-you-go basis. Cloud tenants can use cloud resources at lower prices, and higher performance and flexibility, than traditional on-premises resources, without having to care about infrastructure management. Still, cloud tenants remain concerned with the cloud’s level of service and the nonfunctional properties their applications can count on. In the last few years, the research community has been focusing on the nonfunctional aspects of the cloud paradigm, among which cloud security stands out. Several approaches to security have been described and summarized in general surveys on cloud security techniques. The survey in this article focuses on the interface between cloud security and cloud security assurance. First, we provide an overview of the state of the art on cloud security. Then, we introduce the notion of cloud security assurance and analyze its growing impact on cloud security approaches. Finally, we present some recommendations for the development of next-generation cloud security and assurance solutions.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {2},
numpages = {50},
keywords = {Assurance, cloud computing, transparency, security, survey}
}

@article{10.1145/3432191,
author = {Kortbeek, Vito and Bakar, Abu and Cruz, Stefany and Yildirim, Kasim Sinan and Pawe\l{}czak, Przemys\l{}aw and Hester, Josiah},
title = {BFree: Enabling Battery-Free Sensor Prototyping with Python},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432191},
doi = {10.1145/3432191},
abstract = {Building and programming tiny battery-free energy harvesting embedded computer systems is hard for the average maker because of the lack of tools, hard to comprehend programming models, and frequent power failures. With the high ecologic cost of equipping the next trillion embedded devices with batteries, it is critical to equip the makers, hobbyists, and novice embedded systems programmers with easy-to-use tools supporting battery-free energy harvesting application development. This way, makers can create untethered embedded systems that are not plugged into the wall, the desktop, or even a battery, providing numerous new applications and allowing for a more sustainable vision of ubiquitous computing. In this paper, we present BFree, a system that makes it possible for makers, hobbyists, and novice embedded programmers to develop battery-free applications using Python programming language and widely available hobbyist maker platforms. BFree provides energy harvesting hardware and a power failure resilient version of Python, with durable libraries that enable common coding practice and off the shelf sensors. We develop demonstration applications, benchmark BFree against battery-powered approaches, and evaluate our system in a user study. This work enables makers to engage with a future of ubiquitous computing that is useful, long-term, and environmentally responsible.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {135},
numpages = {39},
keywords = {Making, Energy Harvesting, Intermittent Computing}
}

@inproceedings{10.5555/2337223.2337331,
author = {Kumar, Sandeep and Khoo, Siau-Cheng and Roychoudhury, Abhik and Lo, David},
title = {Inferring Class Level Specifications for Distributed Systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Distributed systems often contain many behaviorally similar processes, which are conveniently grouped into classes. In system modeling, it is common to specify such systems by describing the class level behavior, instead of object level behavior. While there have been techniques that mine specifications of such distributed systems from their execution traces, these methods only mine object-level specifications involving concrete process objects. This leads to specifications which are large, hard to comprehend, and sensitive to simple changes in the system (such as the number of objects).  In this paper, we develop a class level specification mining framework for distributed systems. A specification that describes interaction snippets between various processes in a distributed system forms a natural and intuitive way to document their behavior. Our mining method groups together such interactions between behaviorally similar processes, and presents a mined specification involving "symbolic" Message Sequence Charts. Our experiments indicate that our mined symbolic specifications are significantly smaller than mined concrete specifications, while at the same time achieving better precision and recall.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {914–924},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3503222.3507763,
author = {Ahmad, Hammad and Huang, Yu and Weimer, Westley},
title = {CirFix: Automatically Repairing Defects in Hardware Design Code},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507763},
doi = {10.1145/3503222.3507763},
abstract = {This paper presents CirFix, a framework for automatically repairing defects in hardware designs implemented in languages like Verilog. We propose a novel fault localization approach based on assignments to wires and registers, and a fitness function tailored to the hardware domain to bridge the gap between software-level automated program repair and hardware descriptions. We also present a benchmark suite of 32 defect scenarios corresponding to a variety of hardware projects. Overall, CirFix produces plausible repairs for 21/32 and correct repairs for 16/32 of the defect scenarios. This repair rate is comparable to that of successful program repair approaches for software, indicating CirFix is effective at bringing over the benefits of automated program repair to the hardware domain for the first time.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {990–1003},
numpages = {14},
keywords = {HDL benchmark, automated program repair, hardware designs},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@article{10.1145/3419100,
author = {Turan, Furkan and Verbauwhede, Ingrid},
title = {Trust in FPGA-Accelerated Cloud Computing},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419100},
doi = {10.1145/3419100},
abstract = {Platforms combining Central Processing Systems (CPUs) with Field Programmable Gate Arrays (FPGAs) have become popular, as they promise high performance with energy efficiency. This is the result of the combination of FPGA accelerators tuned to the application, with the CPU providing the programming flexibility. Unfortunately, the security of these new platforms has received little attention: The classic software security assumption that hardware is immutable no longer holds. It is expected that attack surfaces will expand and threats will evolve, hence the trust models, and security solutions should be prepared. The attacker model should be enhanced and consider the following three basic entities as the source of threats: applications run by users, accelerators designed by third-party developers, and the cloud service providers enabling the computation on their platforms. In our work, we review current trust models and existing security assumptions and point out their shortcomings. We survey existing research that target secure remote FPGA configuration, the protection of intellectual property, and secure shared use of FPGAs. When combined, these are the foundations to build a solution for secure use of FPGAs in the cloud. In addition to analysing the existing research, we provide discussions on how to improve it and disclose various concerns that have not been addressed yet.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {128},
numpages = {28},
keywords = {CPU, trust, FPGA, security}
}

@inproceedings{10.1145/3238147.3238171,
author = {Gallaba, Keheliya and Macho, Christian and Pinzger, Martin and McIntosh, Shane},
title = {Noise and Heterogeneity in Historical Build Data: An Empirical Study of Travis CI},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238171},
doi = {10.1145/3238147.3238171},
abstract = {Automated builds, which may pass or fail, provide feedback to a development team about changes to the codebase. A passing build indicates that the change compiles cleanly and tests (continue to) pass. A failing (a.k.a., broken) build indicates that there are issues that require attention. Without a closer analysis of the nature of build outcome data, practitioners and researchers are likely to make two critical assumptions: (1) build results are not noisy; however, passing builds may contain failing or skipped jobs that are actively or passively ignored; and (2) builds are equal; however, builds vary in terms of the number of jobs and configurations.  To investigate the degree to which these assumptions about build breakage hold, we perform an empirical study of 3.7 million build jobs spanning 1,276 open source projects. We find that: (1) 12% of passing builds have an actively ignored failure; (2) 9% of builds have a misleading or incorrect outcome on average; and (3) at least 44% of the broken builds contain passing jobs, i.e., the breakage is local to a subset of build variants. Like other software archives, build data is noisy and complex. Analysis of build data requires nuance.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {87–97},
numpages = {11},
keywords = {Build Breakage, Continuous Integration, Automated Builds},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.1145/3038228.3038233,
author = {Zhu, Zhiting and Kim, Sangman and Rozhanski, Yuri and Hu, Yige and Witchel, Emmett and Silberstein, Mark},
title = {Understanding The Security of Discrete GPUs},
year = {2017},
isbn = {9781450349154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3038228.3038233},
doi = {10.1145/3038228.3038233},
abstract = {GPUs have become an integral part of modern systems, but their implications for system security are not yet clear. This paper demonstrates both that discrete GPUs cannot be used as secure co-processors and that GPUs provide a stealthy platform for malware. First, we examine a recent proposal to use discrete GPUs as secure co-processors and show that the security guarantees of the proposed system do not hold on the GPUs we investigate. Second, we demonstrate that (under certain circumstances) it is possible to bypass IOMMU protections and create stealthy, long-lived GPU-based malware. We demonstrate a novel attack that compromises the in-kernel GPU driver and one that compromises GPU microcode to gain full access to CPU physical memory. In general, we find that the highly sophisticated, but poorly documented GPU hardware architecture, hidden behind obscure close-source device drivers and vendor-specific APIs, not only make GPUs a poor choice for applications requiring strong security, but also make GPUs into a security threat.},
booktitle = {Proceedings of the General Purpose GPUs},
pages = {1–11},
numpages = {11},
location = {Austin, TX, USA},
series = {GPGPU-10}
}

@inproceedings{10.1145/2568225.2568281,
author = {Boci\'{c}, Ivan and Bultan, Tevfik},
title = {Inductive Verification of Data Model Invariants for Web Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568281},
doi = {10.1145/2568225.2568281},
abstract = {Modern software applications store their data in remote cloud servers. Users interact with these applications using web browsers or thin clients running on mobile devices. A key issue in dependability of these applications is the correctness of the actions that update the data store, which are triggered by user requests. In this paper, we present techniques for au- tomatically checking if the actions of an application preserve the data model invariants. Our approach first automatically extracts a data model specification, which we call an abstract data store, from a given application using instrumented exe- cution. The abstract data store identifies the sets of objects and relations (associations) used by the application, and the actions that update the data store by deleting or creating objects or by changing the relations among the objects. We show that checking invariants of an abstract data store corre- sponds to inductive invariant verification, and can be done using a mapping to First Order Logic (FOL) and using a FOL theorem prover. We implemented this approach for the Rails framework and applied it to three open source applications. We found four previously unknown bugs and reported them to the developers, who confirmed and imme- diately fixed two of them.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {620–631},
numpages = {12},
keywords = {data model, Ruby on Rails, inductive invariants, Automated verification},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3332165.3347912,
author = {Klokmose, Clemens N. and Remy, Christian and Kristensen, Janus Bager and Bagge, Rolf and Beaudouin-Lafon, Michel and Mackay, Wendy},
title = {Videostrates: Collaborative, Distributed and Programmable Video Manipulation},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347912},
doi = {10.1145/3332165.3347912},
abstract = {We present Videostrates, a concept and a toolkit for creating real-time collaborative video editing tools. Videostrates supports both live and recorded video composition with a declarative HTML-based notation, combining both simple and sophisticated editing tools that can be used collaboratively. Videostrates is programmable and unleashes the power of the modern web platform for video manipulation. We demonstrate its potential through three use scenarios: collaborative video editing with multiple tools and devices; orchestration of multiple live streams that are recorded and broadcast to a popular streaming platform; and programmatic creation of video using WebGL and shaders for blue screen effects. These scenarios only scratch the surface of Videostrates' potential, which opens up a design space for novel collaborative video editors with fully programmable interfaces.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {233–247},
numpages = {15},
keywords = {video editing, collaborative video, information substrates},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3459637.3481911,
author = {Qiu, Minghui and Li, Peng and Wang, Chengyu and Pan, Haojie and Wang, Ang and Chen, Cen and Jia, Xianyan and Li, Yaliang and Huang, Jun and Cai, Deng and Lin, Wei},
title = {EasyTransfer: A Simple and Scalable Deep Transfer Learning Platform for NLP Applications},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481911},
doi = {10.1145/3459637.3481911},
abstract = {The literature has witnessed the success of leveraging Pre-trained Language Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural Language Processing (NLP) applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to develop deep TL algorithms for NLP applications. EasyTransfer is backended with a high-performance and scalable engine for efficient training and inference, and also integrates comprehensive deep TL algorithms, to make the development of industrial-scale TL applications easier. In EasyTransfer, the built-in data and model parallelism strategies, combined with AI compiler optimization, show to be 4.0x faster than the community version of distributed training. EasyTransfer supports various NLP models in the ModelZoo, including mainstream PLMs and multi-modality models. It also features various in-house developed TL algorithms, together with the AppZoo for NLP applications. The toolkit is convenient for users to quickly start model training, evaluation, and online deployment. EasyTransfer is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, conversational question answering, etc. Extensive experiments on real-world datasets and online applications show that EasyTransfer is suitable for online production with cutting-edge performance for various applications. The source code of EasyTransfer is released at Github1.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4075–4084},
numpages = {10},
keywords = {pre-trained language model, transfer learning, natural language processing},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@proceedings{10.1145/3084863,
title = {SIGGRAPH '17: ACM SIGGRAPH 2017 Studio},
year = {2017},
isbn = {9781450350099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Cyborg Self: Extensions, Adaptations, and Integrations of Technology Within the BodyAs the growth of wearable devices continues to expand, humans are developing an ever-expanding toolset of extensions, insertions, and interventions that lead us to question the future of hybridization in our physical evolution. These new technologies change how we use innovative devices to experience the world within augmented bodies. The SIGGRAPH 2017 Studio presents a broad range of concepts related to the convergence of the physical body and evolving technologies with an emphasis on wearables, e-textiles, bio-tech, and sensory extensions across physical and virtual platforms.},
location = {Los Angeles, California}
}

@article{10.1145/3480027,
author = {Liu, Chao and Xia, Xin and Lo, David and Gao, Cuiyun and Yang, Xiaohu and Grundy, John},
title = {Opportunities and Challenges in Code Search Tools},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3480027},
doi = {10.1145/3480027},
abstract = {Code search is a core software engineering task. Effective code search tools can help developers substantially improve their software development efficiency and effectiveness. In recent years, many code search studies have leveraged different techniques, such as deep learning and information retrieval approaches, to retrieve expected code from a large-scale codebase. However, there is a lack of a comprehensive comparative summary of existing code search approaches. To understand the research trends in existing code search studies, we systematically reviewed 81 relevant studies. We investigated the publication trends of code search studies, analyzed key components, such as codebase, query, and modeling technique used to build code search tools, and classified existing tools into focusing on supporting seven different search tasks. Based on our findings, we identified a set of outstanding challenges in existing studies and a research roadmap for future code search research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {196},
numpages = {40},
keywords = {code retrieval, Code search, modeling}
}

@inproceedings{10.1145/1289927.1289967,
author = {Auerbach, Joshua and Bacon, David F. and Blainey, Bob and Cheng, Perry and Dawson, Michael and Fulton, Mike and Grove, David and Hart, Darren and Stoodley, Mark},
title = {Design and Implementation of a Comprehensive Real-Time Java Virtual Machine},
year = {2007},
isbn = {9781595938251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1289927.1289967},
doi = {10.1145/1289927.1289967},
abstract = {The emergence of standards for programming real-time systems in Java has encouraged many developers to consider its use for systems previously only built using C, Ada, or assembly language. However, the RTSJ standard in isolation leaves many important problems unaddressed, and suffers from some serious problems in usability and safety.As a result, the use of Java for real-time programming has continued to be viewed as risky and adoption has been slow.In this paper we provide a description of IBM's new real-time Java virtual machine product, which combines Metronome real-time garbage collection, ahead-of-time compilation, and a complete implementation of the RTSJ standard, running on top of a custom real-time multiprocessor Linux kernel.We will describe the implementation of each of these components, including how they interacted both positively and negatively, and the extensions to previous work required to move it from research prototype to a system implementing the complete semantics of the Java language. The system has been adopted for hard real-time development of naval weapons systems and soft real-time telecommunications servers. We present measurements showing that the system is able to provide sub-millisecond worst-case garbage collection latencies, 50 microsecond Linux scheduling accuracy, and eliminate non-determinism due to JIT compilation.},
booktitle = {Proceedings of the 7th ACM &amp; IEEE International Conference on Embedded Software},
pages = {249–258},
numpages = {10},
keywords = {JIT, garbage collection, java, AOT, JVM, real time},
location = {Salzburg, Austria},
series = {EMSOFT '07}
}

@inproceedings{10.5555/3026877.3026935,
author = {Zhang, Yunqi and Prekas, George and Fumarola, Giovanni Matteo and Fontoura, Marcus and Goiri, \'{I}\~{n}igo and Bianchini, Ricardo},
title = {History-Based Harvesting of Spare Cycles and Storage in Large-Scale Datacenters},
year = {2016},
isbn = {9781931971331},
publisher = {USENIX Association},
address = {USA},
abstract = {An effective way to increase utilization and reduce costs in datacenters is to co-locate their latency-critical services and batch workloads. In this paper, we describe systems that harvest spare compute cycles and storage space for co-location purposes. The main challenge is minimizing the performance impact on the services, while accounting for their utilization and management patterns. To overcome this challenge, we propose techniques for giving the services priority over the resources, and leveraging historical information about them. Based on this information, we schedule related batch tasks on servers that exhibit similar patterns and will likely have enough available resources for the tasks' durations, and place data replicas at servers that exhibit diverse patterns. We characterize the dynamics of how services are utilized and managed in ten large-scale production datacenters. Using real experiments and simulations, we show that our techniques eliminate data loss and unavailability in many scenarios, while protecting the co-located services and improving batch job execution time.},
booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {755–770},
numpages = {16},
location = {Savannah, GA, USA},
series = {OSDI'16}
}

@inproceedings{10.1145/336512.336567,
author = {Roman, Gruia-Catalin and Picco, Gian Pietro and Murphy, Amy L.},
title = {Software Engineering for Mobility: A Roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336567},
doi = {10.1145/336512.336567},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {241–258},
numpages = {18},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/2025113.2025121,
author = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
title = {How Do Fixes Become Bugs?},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025121},
doi = {10.1145/2025113.2025121},
abstract = {Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation.This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8%--24.4% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {26–36},
numpages = {11},
keywords = {software bugs, testing, human factor, bug fixing, incorrect fixes},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/3377811.3380410,
author = {Overney, Cassandra and Meinicke, Jens and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {How to Not Get Rich: An Empirical Study of Donations in Open Source},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380410},
doi = {10.1145/3377811.3380410},
abstract = {Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1209–1221},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3180155.3180164,
author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
title = {DeFlaker: Automatically Detecting Flaky Tests},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180164},
doi = {10.1145/3180155.3180164},
abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {433–444},
numpages = {12},
keywords = {software testing, code coverage, flaky tests},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@proceedings{10.1145/3568364,
title = {WSSE '22: Proceedings of the 4th World Symposium on Software Engineering},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@article{10.1145/3510820,
author = {Ren, Haoyu and Anicic, Darko and Runkler, Thomas A.},
title = {Towards Semantic Management of On-Device Applications in Industrial IoT},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3510820},
doi = {10.1145/3510820},
abstract = {The Internet of Things (IoT) is revolutionizing the industry. Powered by pervasive embedded devices, the Industrial IoT (IIoT) provides a unique solution for retrieving and analyzing data near the source in real-time. Many emerging techniques, such as Tiny Machine Learning (TinyML) and Complex Event Processing (CEP), are actively being developed to support decision making at the edge, shifting the paradigm from centralized processing to distributed computing. However, distributed computing presents management challenges, as IoT devices are diverse and constrained, and their number is growing exponentially. The situation is even more challenging when various on-device applications (so-called artifacts) are deployed across decentralized IoT networks. Questions to be addressed include how to discover an appropriate function, whether that function can be executed on a certain device, and how to orchestrate a cross-platform service. To tackle these challenges, we propose an approach for the scalable management of on-device applications among distributed IoT devices. By leveraging the W3C Web of Things (WoT), the capabilities of each IoT device, or more precisely, its interaction patterns, can be semantically expressed in a Thing Description (TD). In addition, we introduce semantic modeling of on-device applications to supplement an TD with additional information regarding applications on the device. Specifically, we demonstrate two examples of semantic modeling: neural networks (NN) and CEP rules. The ontologies are evaluated by answering a set of competency questions. By hosting the enriched semantic knowledge of the entire IoT system in a Knowledge Graph (KG), we can discover and interoperate edge devices and artifacts across the decentralized network. This can reduce fragmentation and increase the reusability of IoT components. We demonstrate the feasibility of our concept on an industrial workstation consisting of a conveyor belt and several IoT devices. Finally, the requirements for constructing an IoT semantic management system are discussed.},
journal = {ACM Trans. Internet Technol.},
month = {nov},
articleno = {102},
numpages = {30},
keywords = {Tiny machine learning, Industrial IoT, semantic management, neural network, complex event processing, semantic modeling}
}

@article{10.1145/3494519,
author = {Marijan, Dusica and Sen, Sagar},
title = {Industry–Academia Research Collaboration and Knowledge Co-Creation: Patterns and Anti-Patterns},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3494519},
doi = {10.1145/3494519},
abstract = {Increasing the impact of software engineering research in the software industry and the society at large has long been a concern of high priority for the software engineering community. The problem of two cultures, research conducted in a vacuum (disconnected from the real world), or misaligned time horizons are just some of the many complex challenges standing in the way of successful industry–academia collaborations. This article reports on the experience of research collaboration and knowledge co-creation between industry and academia in software engineering as a way to bridge the research–practice collaboration gap. Our experience spans 14 years of collaboration between researchers in software engineering and the European and Norwegian software and IT industry. Using the participant observation and interview methods, we have collected and afterwards analyzed an extensive record of qualitative data. Drawing upon the findings made and the experience gained, we provide a set of 14 patterns and 14 anti-patterns for industry–academia collaborations, aimed to support other researchers and practitioners in establishing and running research collaboration projects in software engineering.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {45},
numpages = {52},
keywords = {research collaboration, technology transfer, Industry-academia collaboration, collaboration model, software engineering, collaboration gap, anti-patterns, knowledge transfer, research co-creation, patterns}
}

@article{10.1145/3572778,
author = {Einziger, Gil and Himelbrand, Omri and Waisbard, Erez},
title = {Boosting Cache Performance by Access Time Measurements},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/3572778},
doi = {10.1145/3572778},
abstract = {Most modern systems utilize caches to reduce the average data access time and optimize their performance. Recently proposed policies implicitly assume uniform access times, but variable access times naturally appear in domains such as storage, web search, and DNS resolution.Our work measures the access times for various items and exploits variations in access times as an additional signal for caching algorithms. Using such a signal, we introduce adaptive access time-aware cache policies that consistently improve the average access time compared with the best alternative in diverse workloads. Our adaptive algorithm attains an average access time reduction of up to 46% in storage workloads, up to 16% in web searches, and 8.4% on average when considering all experiments in our study.},
journal = {ACM Trans. Storage},
month = {feb},
articleno = {8},
numpages = {29},
keywords = {Cross domain caching, access times aware caching, dynamic caching, access time dataset}
}

@article{10.1145/3628159,
author = {Liu, Jiawei and Huang, Yuheng and Wang, Zhijie and Ma, Lei and Fang, Chunrong and Gu, Mingzheng and Zhang, Xufan and Chen, Zhenyu},
title = {Generation-Based Differential Fuzzing for Deep Learning Libraries},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628159},
doi = {10.1145/3628159},
abstract = {Deep learning (DL) libraries have become the key component in developing and deploying DL-based software nowadays. With the growing popularity of applying DL models in both academia and industry across various domains, any bugs inherent in the DL libraries can potentially cause unexpected server outcomes. As such, there is an urgent demand for improving the software quality of DL libraries. Although there are some existing approaches specifically designed for testing DL libraries, their focus is usually limited to one specific domain, such as computer vision (CV). It is still not very clear how the existing approaches perform in detecting bugs of different DL libraries regarding different task domains and to what extent. To bridge this gap, we first conduct an empirical study on four representative and state-of-the-art DL library testing approaches. Our empirical study results reveal that it is hard for existing approaches to generalize to other task domains. We also find that the test inputs generated by these approaches usually lack diversity, with only a few types of bugs. What’s worse, the false-positive rate of existing approaches is also high (up to 58%). To address these issues, we propose a guided differential fuzzing approach based on generation, namely, Gandalf. To generate testing inputs across diverse task domains effectively, Gandalf adopts the context-free grammar to ensure validity and utilizes a Deep Q-Network to maximize the diversity. Gandalf also includes 15 metamorphic relations to make it possible for the generated test cases to generalize across different DL libraries. Such a design can decrease the false positives because of the semantic difference for different APIs. We evaluate the effectiveness of Gandalf on nine versions of three representative DL libraries, covering 309 operators from computer vision, natural language processing, and automated speech recognition. The evaluation results demonstrate that Gandalf can effectively and efficiently generate diverse test inputs. Meanwhile, Gandalf successfully detects five categories of bugs with only 3.1% false-positive rates. We report all 49 new unique bugs found during the evaluation to the DL libraries’ developers, and most of these bugs have been confirmed. Details about our empirical study and evaluation results are available on our project website.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
keywords = {Software testing, deep learning libraries, generation-based fuzzing}
}

@inproceedings{10.1109/ICSE48619.2023.00016,
author = {Jia, Haoxiang and Wen, Ming and Xie, Zifan and Guo, Xiaochen and Wu, Rongxin and Sun, Maolin and Chen, Kang and Jin, Hai},
title = {Detecting JVM JIT Compiler Bugs via Exploring Two-Dimensional Input Spaces},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00016},
doi = {10.1109/ICSE48619.2023.00016},
abstract = {Java Virtual Machine (JVM) is the fundamental software system that supports the interpretation and execution of Java bytecode. To support the surging performance demands for the increasingly complex and large-scale Java programs, JustIn-Time (JIT) compiler was proposed to perform sophisticated runtime optimization. However, this inevitably induces various bugs, which are becoming more pervasive over the decades and can often cause significant consequences. To facilitate the design of effective and efficient testing techniques to detect JIT compiler bugs. This study first performs a preliminary study aiming to understand the characteristics of JIT compiler bugs and the corresponding triggering test cases. Inspired by the empirical findings, we propose JOpFuzzer, a new JVM testing approach with a specific focus on JIT compiler bugs. The main novelty of JOpFuzzer is embodied in three aspects. First, besides generating new seeds, JOpFuzzer also searches for diverse configurations along the new dimension of optimization options. Second, JOpFuzzer learns the correlations between various code features and different optimization options to guide the process of seed mutation and option exploration. Third, it leverages the profile data, which can reveal the program execution information, to guide the fuzzing process. Such novelties enable JOpFuzzer to effectively and efficiently explore the two-dimensional input spaces. Extensive evaluation shows that JOpFuzzer outperforms the state-of-the-art approaches in terms of the achieved code coverages. More importantly, it has detected 41 bugs in OpenJDK, and 25 of them have already been confirmed or fixed by the corresponding developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {43–55},
numpages = {13},
keywords = {JIT compiler, JVM testing, JVM},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/505452.505453,
author = {Fu, Kevin and Kaashoek, M. Frans and Mazi\`{e}res, David},
title = {Fast and Secure Distributed Read-Only File System},
year = {2002},
issue_date = {February 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/505452.505453},
doi = {10.1145/505452.505453},
abstract = {Internet users increasingly rely on publicly available data for everything from software installation to investment decisions. Unfortunately, the vast majority of public content on the Internet comes with no integrity or authenticity guarantees. This paper presents the self-certifying read-only file system, a content distribution system providing secure, scalable access to public, read-only data.The read-only file system makes the security of published content independent from that of the distribution infrastructure. In a secure area (perhaps off-line), a publisher creates a digitally signed database out of a file system's contents. The publisher then replicates the database on untrusted content-distribution servers, allowing for high availability.The read-only file system avoids performing any cryptographic operations on servers and keeps the overhead of cryptography low on clients, allowing servers to scale to a large number of clients. Measurements of an implementation show that an individual server running on a 550-Mhz Pentium III with FreeBSD can support 1,012 connections per second and 300 concurrent clients compiling a large software package.},
journal = {ACM Trans. Comput. Syst.},
month = {feb},
pages = {1–24},
numpages = {24},
keywords = {File systems, security, read-only}
}

@proceedings{10.1145/3615453,
title = {WiNTECH '23: Proceedings of the 17th ACM Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; Characterization},
year = {2023},
isbn = {9798400703409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@inproceedings{10.1145/3579371.3589348,
author = {Firoozshahian, Amin and Coburn, Joel and Levenstein, Roman and Nattoji, Rakesh and Kamath, Ashwin and Wu, Olivia and Grewal, Gurdeepak and Aepala, Harish and Jakka, Bhasker and Dreyer, Bob and Hutchin, Adam and Diril, Utku and Nair, Krishnakumar and Aredestani, Ehsan K. and Schatz, Martin and Hao, Yuchen and Komuravelli, Rakesh and Ho, Kunming and Abu Asal, Sameer and Shajrawi, Joe and Quinn, Kevin and Sreedhara, Nagesh and Kansal, Pankaj and Wei, Willie and Jayaraman, Dheepak and Cheng, Linda and Chopda, Pritam and Wang, Eric and Bikumandla, Ajay and Karthik Sengottuvel, Arun and Thottempudi, Krishna and Narasimha, Ashwin and Dodds, Brian and Gao, Cao and Zhang, Jiyuan and Al-Sanabani, Mohammed and Zehtabioskuie, Ana and Fix, Jordan and Yu, Hangchen and Li, Richard and Gondkar, Kaustubh and Montgomery, Jack and Tsai, Mike and Dwarakapuram, Saritha and Desai, Sanjay and Avidan, Nili and Ramani, Poorvaja and Narayanan, Karthik and Mathews, Ajit and Gopal, Sethu and Naumov, Maxim and Rao, Vijay and Noru, Krishna and Reddy, Harikrishna and Venkatapuram, Prahlad and Bjorlin, Alexis},
title = {MTIA: First Generation Silicon Targeting Meta's Recommendation Systems},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589348},
doi = {10.1145/3579371.3589348},
abstract = {Meta has traditionally relied on using CPU-based servers for running inference workloads, specifically Deep Learning Recommendation Models (DLRM), but the increasing compute and memory requirements of these models have pushed the company towards using specialized solutions such as GPUs or other hardware accelerators. This paper describes the company's effort in constructing its first silicon specifically designed for recommendation systems; it describes the accelerator architecture and platform design, the software stack for enabling and optimizing PyTorch-based models and provides an initial performance evaluation. With our emerging software stack, we have made significant progress towards reaching the same or higher efficiency as the GPU: We averaged 0.9x perf/W across various DLRMs, and benchmarks show operators such as GEMMs reaching 2x perf/W. Finally, the paper describes the lessons we learned during this journey which can improve the performance and programmability of future generations of architecture.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {80},
numpages = {13},
keywords = {accelerators, machine learning, programmability, performance, inference, recommendation systems},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3290605.3300327,
author = {Howard, Dorothy and Irani, Lilly},
title = {Ways of Knowing When Research Subjects Care},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300327},
doi = {10.1145/3290605.3300327},
abstract = {This paper investigates a hidden dimension of research with real world stakes: research subjects who care -- sometimes deeply -- about the topic of the research in which they participate. They manifest this care, we show, by managing how they are represented in the research process, by exercising politics in shaping knowledge production, and sometimes in experiencing trauma in the process. We draw first-hand reflections on participation in diversity research on Wikipedia, transforming participants from objects of study to active negotiators of research process. We depict how care, vulnerability, harm, and emotions shape ethnographic and qualitative data. We argue that, especially in reflexive cultures, research subjects are active agents with agendas, accountabilities, and political projects of their own. We propose ethics of care and collaboration to open up new possibilities for knowledge production and socio-technical intervention in HCI.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {feminism, gender, care, ethics, online communities, qualitative methods},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/2851502,
author = {Zhou, Mingzhou and Wu, Bo and Shen, Xipeng and Gao, Yaoqing and Yiu, Graham},
title = {Examining and Reducing the Influence of Sampling Errors on Feedback-Driven Optimizations},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/2851502},
doi = {10.1145/2851502},
abstract = {Feedback-driven optimization (FDO) is an important component in mainstream compilers. By allowing the compiler to reoptimize the program based on some profiles of the program's dynamic behaviors, it often enhances the quality of the generated code substantially. A barrier for using FDO is that it often requires many training runs to collect enough profiles to amortize the sensitivity of program optimizations to program input changes. Various sampling techniques have been explored to alleviate this time-consuming process. However, the lowered profile accuracy caused by sampling often hurts the benefits of FDO.This article gives the first systematic study in how sampling rates affect the accuracy of collected profiles and how the accuracy correlates with the usefulness of the profile for modern FDO. Studying basic block and edge profiles for FDO in two mature compilers reveals several counterintuitive observations, one of which is that profiling accuracy does not strongly correlate with the benefits of the FDO. A detailed analysis identifies three types of sampling-caused errors that critically impair the quality of the profiles for FDO. It then introduces a simple way to rectify profiles based on the findings. Experiments demonstrate that the simple rectification fixes most of those critical errors in sampled profiles and significantly enhances the effectiveness of FDO.},
journal = {ACM Trans. Archit. Code Optim.},
month = {apr},
articleno = {6},
numpages = {24},
keywords = {Performance, feedback-driven optimization, influence of sampling errors}
}

@inproceedings{10.1145/3544548.3581452,
author = {Li, Wenchao and Sch\"{o}ttler, Sarah and Scott-Brown, James and Wang, Yun and Chen, Siming and Qu, Huamin and Bach, Benjamin},
title = {NetworkNarratives: Data Tours for Visual Network Exploration and Analysis},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581452},
doi = {10.1145/3544548.3581452},
abstract = {This paper introduces semi-automatic data tours to aid the exploration of complex networks. Exploring networks requires significant effort and expertise and can be time-consuming and challenging. Distinct from guidance and recommender systems for visual analytics, we provide a set of goal-oriented tours for network overview, ego-network analysis, community exploration, and other tasks. Based on interviews with five network analysts, we developed a user interface (NetworkNarratives) and 10 example tours. The interface allows analysts to navigate an interactive slideshow featuring facts about the network using visualizations and textual annotations. On each slide, an analyst can freely explore the network and specify nodes, links, or subgraphs as seed elements for follow-up tours. Two studies, comprising eight expert and 14 novice analysts, show that data tours reduce exploration effort, support learning about network exploration, and can aid the dissemination of analysis results. NetworkNarratives is available online, together with detailed illustrations for each tour.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {172},
numpages = {15},
keywords = {network visualization, Guided exploration},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/582475.582486,
author = {Casanova, Henri},
title = {Distributed Computing Research Issues in Grid Computing},
year = {2002},
issue_date = {September 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {0163-5700},
url = {https://doi.org/10.1145/582475.582486},
doi = {10.1145/582475.582486},
abstract = {Ensembles of distributed, heterogeneous resources, or Computational Grids, have emerged as popular platforms for deploying large-scale and resource-intensive applications. Large collaborative efforts are currently underway to provide the necessary software infrastructure. Grid computing raises challenging issues in many areas of computer science, and especially in the area of distributed computing, as Computational Grids cover increasingly large networks and span many organizations. In this paper we briefly motivate Grid computing and introduce its basic concepts. We then highlight a number of distributed computing research questions, and discuss both the relevance and the short-comings of previous research results when applied to Grid computing. We choose to focus on issues concerning the dissemination and retrieval of information and data on Computational Grid platforms. We feel that these issues are particularly critical at this time, and as we can point to preliminary ideas, work, and results in the Grid community and the distributed computing community. This paper is of interest to distributing computing researchers because Grid computing provides new challenges that need to be addressed, as well as actual platforms for experimentation and research.},
journal = {SIGACT News},
month = {sep},
pages = {50–70},
numpages = {21}
}

@inproceedings{10.1145/3332165.3347890,
author = {Kim, Donghwi and Park, Sooyoung and Ko, Jihoon and Ko, Steven Y. and Lee, Sung-Ju},
title = {X-Droid: A Quick and Easy Android Prototyping Framework with a Single-App Illusion},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347890},
doi = {10.1145/3332165.3347890},
abstract = {We present X-Droid, a framework that provides Android app developers an ability to quickly and easily produce functional prototypes. Our work is motivated by the need for such ability and the lack of tools that provide it. Developers want to produce a functional prototype rapidly to test out potential features in real-life situations. However, current prototyping tools for mobile apps are limited to creating non-functional UI mockups that do not demonstrate actual features. With X-Droid, developers can create a new app that imports various kinds of functionality provided by other existing Android apps. In doing so, developers do not need to understand how other Android apps are implemented or need access to their source code. X-Droid provides a developer tool that enables developers to use the UIs of other Android apps and import desired functions into their prototypes. X-Droid also provides a run-time system that executes other apps' functionality in the background on off-the-shelf Android devices for seamless integration. Our evaluation shows that with the help of X-Droid, a developer imported a function from an existing Android app into a new prototype with only 51 lines of Java code, while the function itself requires 10,334 lines of Java code to implement (i.e., 200\texttimes{} improvement).},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {95–108},
numpages = {14},
keywords = {functional prototyping, development frameworks, programming by demonstration (pbd), android},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@article{10.1145/3274412,
author = {Puussaar, Aare and Johnson, Ian G. and Montague, Kyle and James, Philip and Wright, Peter},
title = {Making Open Data Work for Civic Advocacy},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274412},
doi = {10.1145/3274412},
abstract = {The value of data in supporting citizen participation in processes of place-making and community building is widely recognised. While the open data movement now permits citizens to acquire governmental data relating to their communities, little to no effort is made to ensure that these datasets are accessible and interpretable by non-professionals. Through a series of community engagements spanning an 18-month period, we co-designed Data:In Place, an open source web tool which supports citizens in accessing, interpreting and making sense of open data. Leveraging visual map-based querying, citizens can access official statistics about their community, interrogate the data, and map their own data sources to create data visualisations. Reflecting on the participatory design process and the designed technology, we provide a framing to make open data work for civic advocacy.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {143},
numpages = {20},
keywords = {data-in-place, civic technology, sensemaking, open data, data visualisation}
}

@article{10.1145/3360581,
author = {Marcozzi, Micha\"{e}l and Tang, Qiyi and Donaldson, Alastair F. and Cadar, Cristian},
title = {Compiler Fuzzing: How Much Does It Matter?},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360581},
doi = {10.1145/3360581},
abstract = {Despite much recent interest in randomised testing (fuzzing) of compilers, the practical impact of fuzzer-found compiler bugs on real-world applications has barely been assessed. We present the first quantitative and qualitative study of the tangible impact of miscompilation bugs in a mature compiler. We follow a rigorous methodology where the bug impact over the compiled application is evaluated based on (1) whether the bug appears to trigger during compilation; (2) the extent to which generated assembly code changes syntactically due to triggering of the bug; and (3) whether such changes cause regression test suite failures, or whether we can manually find application inputs that trigger execution divergence due to such changes. The study is conducted with respect to the compilation of more than 10 million lines of C/C++ code from 309 Debian packages, using 12% of the historical and now fixed miscompilation bugs found by four state-of-the-art fuzzers in the Clang/LLVM compiler, as well as 18 bugs found by human users compiling real code or as a by-product of formal verification efforts. The results show that almost half of the fuzzer-found bugs propagate to the generated binaries for at least one package, in which case only a very small part of the binary is typically affected, yet causing two failures when running the test suites of all the impacted packages. User-reported and formal verification bugs do not exhibit a higher impact, with a lower rate of triggered bugs and one test failure. The manual analysis of a selection of the syntactic changes caused by some of our bugs (fuzzer-found and non fuzzer-found) in package assembly code, shows that either these changes have no semantic impact or that they would require very specific runtime circumstances to trigger execution divergence.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {155},
numpages = {29},
keywords = {software testing, bug impact, fuzzing, Clang, compilers, LLVM}
}

@inproceedings{10.1145/1198555.1198686,
author = {Phelps, Andrew M. and Egert, Christopher A. and Bierre, Kevin J. and Parks, David M.},
title = {An Open-Source CVE for Programming Education: A Case Study},
year = {2005},
isbn = {9781450378338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1198555.1198686},
doi = {10.1145/1198555.1198686},
booktitle = {ACM SIGGRAPH 2005 Courses},
pages = {1–es},
location = {Los Angeles, California},
series = {SIGGRAPH '05}
}

@inproceedings{10.1145/2635868.2635920,
author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
title = {An Empirical Analysis of Flaky Tests},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635920},
doi = {10.1145/2635868.2635920},
abstract = {Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {643–653},
numpages = {11},
keywords = {Empirical study, non-determinism, flaky tests},
location = {Hong Kong, China},
series = {FSE 2014}
}

@article{10.1145/3386333,
author = {Van Roy, Peter and Haridi, Seif and Schulte, Christian and Smolka, Gert},
title = {A History of the Oz Multiparadigm Language},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386333},
doi = {10.1145/3386333},
abstract = {Oz is a programming language designed to support multiple programming paradigms in a clean factored way that is easy to program despite its broad coverage. It started in 1991 as a collaborative effort by the DFKI (Germany) and SICS (Sweden) and led to an influential system, Mozart, that was released in 1999 and widely used in the 2000s for practical applications and education. We give the history of Oz as it developed from its origins in logic programming, starting with Prolog, followed by concurrent logic programming and constraint logic programming, and leading to its two direct precursors, the concurrent constraint model and the Andorra Kernel Language (AKL). We give the lessons learned from the Oz effort including successes and failures and we explain the principles underlying the Oz design. Oz is defined through a kernel language, which is a formal model similar to a foundational calculus, but that is designed to be directly useful to the programmer. The kernel language is organized in a layered structure, which makes it straightforward to write programs that use different paradigms in different parts. Oz is a key enabler for the book Concepts, Techniques, and Models of Computer Programming (MIT Press, 2004). Based on the book and the implementation, Oz has been used successfully in university-level programming courses starting from 2001 to the present day.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {83},
numpages = {56},
keywords = {distributed programming, functional programming, Computer programming, multiparadigm programming, concurrent programming, programming education, lazy evaluation, dataflow, logic programming}
}

@article{10.1145/3456630,
author = {Oikonomou, Panagiotis and Karanika, Anna and Anagnostopoulos, Christos and Kolomvatsos, Kostas},
title = {On the Use of Intelligent Models towards Meeting the Challenges of the Edge Mesh},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3456630},
doi = {10.1145/3456630},
abstract = {Nowadays, we are witnessing the advent of the Internet of Things (IoT) with numerous devices performing interactions between them or with their environment. The huge number of devices leads to huge volumes of data that demand the appropriate processing. The “legacy” approach is to rely on Cloud where increased computational resources can realize any desired processing. However, the need for supporting real-time applications requires a reduced latency in the provision of outcomes. Edge Computing (EC) comes as the “solver” of the latency problem. Various processing activities can be performed at EC nodes having direct connection with IoT devices. A number of challenges should be met before we conclude a fully automated ecosystem where nodes can cooperate or understand their status to efficiently serve applications. In this article, we perform a survey of the relevant research activities towards the vision of Edge Mesh (EM), i.e., a “cover” of intelligence upon the EC. We present the necessary hardware and discuss research outcomes in every aspect of EC/EM nodes functioning. We present technologies and theories adopted for data, tasks, and resource management while discussing how machine learning and optimization can be adopted in the domain.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {125},
numpages = {42},
keywords = {Tasks Management, Data Management, Edge Computing, Edge Mesh, Machine Learning, Internet of Things, Resources Management}
}

@inproceedings{10.1145/2906388.2906410,
author = {Bregu, Endri and Casamassima, Nicola and Cantoni, Daniel and Mottola, Luca and Whitehouse, Kamin},
title = {Reactive Control of Autonomous Drones},
year = {2016},
isbn = {9781450342698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2906388.2906410},
doi = {10.1145/2906388.2906410},
abstract = {Aerial drones, ground robots, and aquatic rovers enable mobile applications that no other technology can realize with comparable flexibility and costs. In existing platforms, the low-level control enabling a drone's autonomous movement is currently realized in a time-triggered fashion, which simplifies implementations. In contrast, we conceive a notion of reactive control that supersedes the time-triggered approach by leveraging the characteristics of existing control logic and of the hardware it runs on. Using reactive control, control decisions are taken only upon recognizing the need to, based on observed changes in the navigation sensors. As a result, the rate of execution dynamically adapts to the circumstances. Compared to time-triggered control, this allows us to: i) attain more timely control decisions, ii) improve hardware utilization, iii) lessen the need to over-provision control rates. Based on 260+ hours of real-world experiments using three aerial drones, three different control logic, and three hardware platforms, we demonstrate, for example, up to 41% improvements in control accuracy and up to 22% improvements in flight time.},
booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {207–219},
numpages = {13},
keywords = {reactive control, autonomous drones, reactive programming, aerial drones},
location = {Singapore, Singapore},
series = {MobiSys '16}
}

@article{10.1145/3003147,
author = {Jhaveri, Mohammad Hanif and Cetin, Orcun and Ga\~{n}\'{a}n, Carlos and Moore, Tyler and Eeten, Michel Van},
title = {Abuse Reporting and the Fight Against Cybercrime},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3003147},
doi = {10.1145/3003147},
abstract = {Cybercriminal activity has exploded in the past decade, with diverse threats ranging from phishing attacks to botnets and drive-by-downloads afflicting millions of computers worldwide. In response, a volunteer defense has emerged, led by security companies, infrastructure operators, and vigilantes. This reactionary force does not concern itself with making proactive upgrades to the cyber infrastructure. Instead, it operates on the front lines by remediating infections as they appear. We construct a model of the abuse reporting infrastructure in order to explain how voluntary action against cybercrime functions today, in hopes of improving our understanding of what works and how to make remediation more effective in the future. We examine the incentives to participate among data contributors, affected resource owners, and intermediaries. Finally, we present a series of key attributes that differ among voluntary actions to investigate further through experimentation, pointing toward a research agenda that could establish causality between interventions and outcomes.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {68},
numpages = {27},
keywords = {internet security, security economics, Cybercrime, abuse reporting}
}

@article{10.1145/3241739,
author = {Bi\o{}rn-Hansen, Andreas and Gr\o{}nli, Tor-Morten and Ghinea, Gheorghita},
title = {A Survey and Taxonomy of Core Concepts and Research Challenges in Cross-Platform Mobile Development},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3241739},
doi = {10.1145/3241739},
abstract = {Developing applications targeting mobile devices is a complex task involving numerous options, technologies, and trade-offs, mostly due to the proliferation and fragmentation of devices and platforms. As a result of this, cross-platform app development has enjoyed the attention of practitioners and academia for the previous decade. Throughout this review, we assess the academic body of knowledge and report on the state of research on the field. We do so with a particular emphasis on core concepts, including those of user experience, device features, performance, and security. Our findings illustrate that the state of research demand for empirical verification of an array of unbacked claims, and that a particular focus on qualitative user-oriented research is essential. Through our outlined taxonomy and state of research overview, we identify research gaps and challenges, and provide numerous suggestions for further research.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {108},
numpages = {34},
keywords = {mobile computing, Cross-platform mobile development, app development}
}

@inproceedings{10.1145/3132747.3132749,
author = {Kaldor, Jonathan and Mace, Jonathan and Bejda, Micha\l{} and Gao, Edison and Kuropatwa, Wiktor and O'Neill, Joe and Ong, Kian Win and Schaller, Bill and Shan, Pingjia and Viscomi, Brendan and Venkataraman, Vinod and Veeraraghavan, Kaushik and Song, Yee Jiun},
title = {Canopy: An End-to-End Performance Tracing And Analysis System},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132749},
doi = {10.1145/3132747.3132749},
abstract = {This paper presents Canopy, Facebook's end-to-end performance tracing infrastructure. Canopy records causally related performance data across the end-to-end execution path of requests, including from browsers, mobile applications, and backend services. Canopy processes traces in near real-time, derives user-specified features, and outputs to performance datasets that aggregate across billions of requests. Using Canopy, Facebook engineers can query and analyze performance data in real-time. Canopy addresses three challenges we have encountered in scaling performance analysis: supporting the range of execution and performance models used by different components of the Facebook stack; supporting interactive ad-hoc analysis of performance data; and enabling deep customization by users, from sampling traces to extracting and visualizing features. Canopy currently records and processes over 1 billion traces per day. We discuss how Canopy has evolved to apply to a wide range of scenarios, and present case studies of its use in solving various performance challenges.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {34–50},
numpages = {17},
location = {Shanghai, China},
series = {SOSP '17}
}

@proceedings{10.1145/3590837,
title = {ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
year = {2022},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@proceedings{10.1145/3578527,
title = {ISEC '23: Proceedings of the 16th Innovations in Software Engineering Conference},
year = {2023},
isbn = {9798400700644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Allahabad, India}
}

@inbook{10.1145/2886107.2886110,
title = {Resilient Distributed Datasets},
year = {2018},
isbn = {9781970001570},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2886107.2886110},
abstract = {Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines.At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too.This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing.We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective.This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.},
booktitle = {An Architecture for Fast and General Data Processing on Large Clusters}
}

@article{10.14778/2732232.2732238,
author = {Tian, Yuanyuan and Balmin, Andrey and Corsten, Severin Andreas and Tatikonda, Shirish and McPherson, John},
title = {From "Think like a Vertex" to "Think like a Graph"},
year = {2013},
issue_date = {November 2013},
publisher = {VLDB Endowment},
volume = {7},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732232.2732238},
doi = {10.14778/2732232.2732238},
abstract = {To meet the challenge of processing rapidly growing graph and network data created by modern applications, a number of distributed graph processing systems have emerged, such as Pregel and GraphLab. All these systems divide input graphs into partitions, and employ a "think like a vertex" programming model to support iterative graph computation. This vertex-centric model is easy to program and has been proved useful for many graph algorithms. However, this model hides the partitioning information from the users, thus prevents many algorithm-specific optimizations. This often results in longer execution time due to excessive network messages (e.g. in Pregel) or heavy scheduling overhead to ensure data consistency (e.g. in GraphLab). To address this limitation, we propose a new "think like a graph" programming paradigm. Under this graph-centric model, the partition structure is opened up to the users, and can be utilized so that communication within a partition can bypass the heavy message passing or scheduling machinery. We implemented this model in a new system, called Giraph++, based on Apache Giraph, an open source implementation of Pregel. We explore the applicability of the graph-centric model to three categories of graph algorithms, and demonstrate its flexibility and superior performance, especially on well-partitioned data. For example, on a web graph with 118 million vertices and 855 million edges, the graph-centric version of connected component detection algorithm runs 63X faster and uses 204X fewer network messages than its vertex-centric counterpart.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {193–204},
numpages = {12}
}

@inproceedings{10.1145/2950290.2950335,
author = {Hasabnis, Niranjan and Sekar, R.},
title = {Extracting Instruction Semantics via Symbolic Execution of Code Generators},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950335},
doi = {10.1145/2950290.2950335},
abstract = {Binary analysis and instrumentation form the basis of many tools and frameworks for software debugging, security hardening, and monitoring. Accurate modeling of instruction semantics is paramount in this regard, as errors can lead to program crashes, or worse, bypassing of security checks. Semantic modeling is a daunting task for modern processors such as x86 and ARM that support over a thousand instructions, many of them with complex semantics. This paper describes a new approach to automate this semantic modeling task. Our approach leverages instruction semantics knowledge that is already encoded into today's production compilers such as GCC and LLVM. Such an approach can greatly reduce manual effort, and more importantly, avoid errors introduced by manual modeling. Furthermore, it is applicable to any of the numerous architectures already supported by the compiler. In this paper, we develop a new symbolic execution technique to extract instruction semantics from a compiler's source code. Unlike previous applications of symbolic execution that were focused on identifying a single program path that violates a property, our approach addresses the all paths problem, extracting the entire input/output behavior of the code generator. We have applied it successfully to the 120K lines of C-code used in GCC's code generator to extract x86 instruction semantics. To demonstrate architecture-neutrality, we have also applied it to AVR, a processor used in the popular Arduino platform.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {301–313},
numpages = {13},
keywords = {Symbolic execution, Instruction-set semantics extraction, Code generators},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3372885.3373824,
author = {The mathlib Community},
title = {The Lean Mathematical Library},
year = {2020},
isbn = {9781450370974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372885.3373824},
doi = {10.1145/3372885.3373824},
abstract = {This paper describes mathlib, a community-driven effort to build a unified library of mathematics formalized in the Lean proof assistant. Among proof assistant libraries, it is distinguished by its dependently typed foundations, focus on classical mathematics, extensive hierarchy of structures, use of large- and small-scale automation, and distributed organization. We explain the architecture and design decisions of the library and the social organization that has led to its development.},
booktitle = {Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {367–381},
numpages = {15},
keywords = {formal proof, Lean, formal library, mathlib},
location = {New Orleans, LA, USA},
series = {CPP 2020}
}

@inproceedings{10.1145/1217935.1217938,
author = {Portokalidis, Georgios and Slowinska, Asia and Bos, Herbert},
title = {Argos: An Emulator for Fingerprinting Zero-Day Attacks for Advertised Honeypots with Automatic Signature Generation},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217938},
doi = {10.1145/1217935.1217938},
abstract = {As modern operating systems and software become larger and more complex, they are more likely to contain bugs, which may allow attackers to gain illegitimate access. A fast and reliable mechanism to discern and generate vaccines for such attacks is vital for the successful protection of networks and systems. In this paper we present Argos, a containment environment for worms as well as human orchestrated attacks. Argos is built upon a fast x86 emulator which tracks network data throughout execution to identify their invalid use as jump targets, function addresses, instructions, etc. Furthermore, system call policies disallow the use of network data as arguments to certain calls. When an attack is detected, we perform 'intelligent' process- or kernel-aware logging of the corresponding emulator state for further offline processing. In addition, our own forensics shellcode is injected, replacing the malevolent shellcode, to gather information about the attacked process. By correlating the data logged by the emulator with the data collected from the network, we are able to generate accurate network intrusion detection signatures for the exploits that are immune to payload mutations. The entire process can be automated and has few if any false positives, thus rapid global scale deployment of the signatures is possible.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {15–27},
numpages = {13},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@article{10.1145/3468505,
author = {Beaudouin-Lafon, Michel and B\o{}dker, Susanne and Mackay, Wendy E.},
title = {Generative Theories of Interaction},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3468505},
doi = {10.1145/3468505},
abstract = {Although Human–Computer Interaction research has developed various theories and frameworks for analyzing new and existing interactive systems, few address the generation of novel technological solutions, and new technologies often lack theoretical foundations. We introduce Generative Theories of Interaction, which draw insights from empirical theories about human behavior in order to define specific concepts and actionable principles, which, in turn, serve as guidelines for analyzing, critiquing, and constructing new technological artifacts. After introducing and defining Generative Theories of Interaction, we present three detailed examples from our own work: Instrumental Interaction, Human–Computer Partnerships, and Communities &amp; Common Objects. Each example describes the underlying scientific theory and how we derived and applied HCI-relevant concepts and principles to the design of innovative interactive technologies. Summary tables offer sample questions that help analyze existing technology with respect to a specific theory, critique both positive and negative aspects, and inspire new ideas for constructing novel interactive systems.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {nov},
articleno = {45},
numpages = {54},
keywords = {Theory, generative theory, generative principles}
}

@proceedings{10.1145/3603165,
title = {ACM TURC '23: Proceedings of the ACM Turing Award Celebration Conference - China 2023},
year = {2023},
isbn = {9798400702334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Wuhan, China}
}

@article{10.1145/3178854,
author = {Antonelli, Humberto Lidio and Igawa, Rodrigo Augusto and Fortes, Renata Pontin De Mattos and Rizo, Eduardo Henrique and Watanabe, Willian Massami},
title = {Drop-Down Menu Widget Identification Using HTML Structure Changes Classification},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/3178854},
doi = {10.1145/3178854},
abstract = {Widgets have been deployed in rich internet applications for more than 10 years. However, many of the widgets currently available on the web do not implement current accessibility design solutions standardized in ARIA (Accessible Rich Internet Applications) specification, hence are not accessible to disabled users. This article sets out an approach for automatically identifying widgets on the basis of machine-learning algorithms and the classification of mutation records; it is an HTML5 technology that logs all changes that occur in the structure of a web application. Automatic widget identification is an essential component for the elaboration of automatic ARIA evaluation and adaptation strategies. Thus, the aim of this article is to take steps toward easing the software-engineering process of ARIA widgets. The proposed approach focuses on the identification of drop-down menu widgets. An experiment with real-world web applications was conducted and the results showed evidence that this approach is capable of identifying these widgets and can outperform previous state-of-the-art techniques based on an F-measure analysis conducted during the experiment.},
journal = {ACM Trans. Access. Comput.},
month = {jun},
articleno = {10},
numpages = {23},
keywords = {drop-down menu widget, Automatic identification of widgets, ARIA, web accessibility, widgets, fly-out menus}
}

@proceedings{10.1145/3594536,
title = {ICAIL '23: Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is my great pleasure to present to you the proceedings of the Nineteenth International Conference on Artificial Intelligence and Law (ICAIL 2023). The conference will be held June 19-23 at the Universidade do Minho in Braga, Portugal. It has been organized by the International Association for Artificial Intelligence and Law (IAAIL) and is held in cooperation with AAAI and ACM SIGAI. IAAIL's mission is to facilitate research, collaboration, and interdisciplinary communication at the intersection of law and the technical disciplines belonging to the field of artificial intelligence. The first ICAIL conference was held in 1987 and its 2023 iteration is the first to be held in person again after the Covid-19 pandemic.},
location = {Braga, Portugal}
}

@article{10.1145/3375633,
author = {Beschastnikh, Ivan and Liu, Perry and Xing, Albert and Wang, Patty and Brun, Yuriy and Ernst, Michael D.},
title = {Visualizing Distributed System Executions},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3375633},
doi = {10.1145/3375633},
abstract = {Distributed systems pose unique challenges for software developers. Understanding the system’s communication topology and reasoning about concurrent activities of system hosts can be difficult. The standard approach, analyzing system logs, can be a tedious and complex process that involves reconstructing a system log from multiple hosts’ logs, reconciling timestamps among hosts with non-synchronized clocks, and understanding what took place during the execution encoded by the log. This article presents a novel approach for tackling three tasks frequently performed during analysis of distributed system executions: (1)&nbsp;understanding the relative ordering of events, (2)&nbsp;searching for specific patterns of interaction between hosts, and (3)&nbsp;identifying structural similarities and differences between pairs of executions. Our approach consists of XVector, which instruments distributed systems to capture partial ordering information that encodes the happens-before relation between events, and ShiViz, which processes the resulting logs and presents distributed system executions as interactive time-space diagrams. Two user studies with a total of 109 students and a case study with 2 developers showed that our method was effective, helping participants answer statistically significantly more system-comprehension questions correctly, with a very large effect size.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {9},
numpages = {38},
keywords = {Distributed systems, log analysis, program comprehension}
}

@inproceedings{10.1145/3368089.3409714,
author = {Gopstein, Dan and Fayard, Anne-Laure and Apel, Sven and Cappos, Justin},
title = {Thinking Aloud about Confusing Code: A Qualitative Investigation of Program Comprehension and Atoms of Confusion},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409714},
doi = {10.1145/3368089.3409714},
abstract = {Atoms of confusion are small patterns of code that have been empirically validated to be difficult to hand-evaluate by programmers. Previous research focused on defining and quantifying this phenomenon, but not on explaining or critiquing it. In this work, we address core omissions to the body of work on atoms of confusion, focusing on the ‘how’ and ‘why’ of programmer misunderstanding. We performed a think-aloud study in which we observed programmers, both professionals and students, as they hand-evaluated confusing code. We performed a qualitative analysis of the data and found several surprising results, which explain previous results, outline avenues of further research, and suggest improvements of the research methodology. A notable observation is that correct hand-evaluations do not imply understanding, and incorrect evaluations not misunderstanding. We believe this and other observations may be used to improve future studies and models of program comprehension. We argue that thinking of confusion as an atomic construct may pose challenges to formulating new candidates for atoms of confusion. Ultimately, we question whether hand-evaluation correctness is, itself, a sufficient instrument to study program comprehension.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {605–616},
numpages = {12},
keywords = {Program Understanding, Atoms of Confusion, Think-Aloud Study},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3386325,
author = {Syme, Don},
title = {The Early History of F#},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3386325},
doi = {10.1145/3386325},
abstract = {This paper describes the genesis and early history of the F# programming language. I start with the origins of strongly-typed functional programming (FP) in the 1970s, 80s and 90s. During the same period, Microsoft was founded and grew to dominate the software industry. In 1997, as a response to Java, Microsoft initiated internal projects which eventually became the .NET programming framework and the C# language. From 1997 the worlds of academic functional programming and industry combined at Microsoft Research, Cambridge. The researchers engaged with the company through Project 7, the initial effort to bring multiple languages to .NET, leading to the initiation of .NET Generics in 1998 and F# in 2002. F# was one of several responses by advocates of strongly-typed functional programming to the "object-oriented tidal wave" of the mid-1990s. The development of the core features of F# 1.0 happened from 2004-2007, and I describe the decision-making process that led to the "productization" of F# by Microsoft in 2007-10 and the release of F# 2.0. The origins of F#'s characteristic features are covered: object programming, quotations, statically resolved type parameters, active patterns, computation expressions, async, units-of-measure and type providers. I describe key developments in F# since 2010, including F# 3.0-4.5, and its evolution as an open source, cross-platform language with multiple delivery channels. I conclude by examining some uses of F# and the influence F# has had on other languages so far.},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {75},
numpages = {58},
keywords = {Type Providers, Dimensions, Asynchronous Programming, Units of Measure, Programming Languages, F#, Functional Programming, Pattern Matching, Object-oriented Programming}
}

@article{10.1145/3449126,
author = {Shrestha, Nischal and Barik, Titus and Parnin, Chris},
title = {Remote, but Connected: How #TidyTuesday Provides an Online Community of Practice for Data Scientists.},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449126},
doi = {10.1145/3449126},
abstract = {Data science practitioners face the challenge of continually honing their skills such as data wrangling and visualization. As data scientists seek online spaces to network, learn and share resources with one another, each individual has to employ their own ad-hoc strategy to practice their data science skills. Given these disjointed efforts, it is crucial to ask: how can we build an inclusive, welcoming online community of practice that unites data scientists in their collective efforts to become experts? Daily hashtags on Twitter are used on specific days and have shown promise in forming a community of practice (CoP) in social networking sites like Twitter, but how do they benefit the community and its members? To understand how daily hashtags benefit data scientists and form an online CoP, we conducted a qualitative study on #TidyTuesday---a daily hashtag project for data scientists using R---using the framework of CoP as a lens for analysis. We conducted semi-structured interviews with 26 participants and uncovered motivations behind their participation in #TidyTuesday, how the project benefited them, and how it cultivated an online CoP. Our findings contribute to the CSCW research on community of practices by providing design trade-offs of using daily hashtags on Twitter, and guidelines on growing and sustaining an online community of practice for data scientists.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {52},
numpages = {31},
keywords = {online social movements, hashtag movements, data science, community of practice}
}

@inproceedings{10.1145/3355047.3359423,
author = {Mokhov, Serguei A. and Song, Miao and Mudur, Sudhir P. and Grogono, Peter},
title = {Dataflow Programming and Processing for Artists and Beyond},
year = {2019},
isbn = {9781450369411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355047.3359423},
doi = {10.1145/3355047.3359423},
abstract = {We complement the last three editions of the course at SIGGRAPH Asia (2015, 2016, 2018) and SIGGRAPH (2017) to make it more of a hands-on nature and include OpenISS. We explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2 and its OpenISS core framework for creative near-realtime broadcasting, and the use of AI and HCI techniques in art.},
booktitle = {SIGGRAPH Asia 2019 Courses},
articleno = {134},
numpages = {33},
keywords = {illimitable space system (ISS), human-computer interfaces, OpenISS, OpenGL, jitter/MAX, interaction, computer graphics education, RGBD cameras, real-time, processing},
location = {Brisbane, Queensland, Australia},
series = {SA '19}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@article{10.1145/3383773,
author = {Parker, James and Hicks, Michael and Ruef, Andrew and Mazurek, Michelle L. and Levin, Dave and Votipka, Daniel and Mardziel, Piotr and Fulton, Kelsey R.},
title = {Build It, Break It, Fix It: Contesting Secure Development},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3383773},
doi = {10.1145/3383773},
abstract = {Typical security contests focus on breaking or mitigating the impact of buggy systems. We present the Build-it, Break-it, Fix-it (BIBIFI) contest, which aims to assess the ability to securely build software, not just break it. In BIBIFI, teams build specified software with the goal of maximizing correctness, performance, and security. The latter is tested when teams attempt to break other teams’ submissions. Winners are chosen from among the best builders and the best breakers. BIBIFI was designed to be open-ended—teams can use any language, tool, process, and so on, that they like. As such, contest outcomes shed light on factors that correlate with successfully building secure software and breaking insecure software. We ran three contests involving a total of 156 teams and three different programming problems. Quantitative analysis from these contests found that the most efficient build-it submissions used C/C++, but submissions coded in a statically type safe language were 11\texttimes{} less likely to have a security flaw than C/C++ submissions. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.},
journal = {ACM Trans. Priv. Secur.},
month = {apr},
articleno = {10},
numpages = {36},
keywords = {Contest, security, engineering, software, education}
}

@inproceedings{10.1145/3477132.3483549,
author = {Gong, Sishuai and Altinb\"{u}ken, Deniz and Fonseca, Pedro and Maniatis, Petros},
title = {Snowboard: Finding Kernel Concurrency Bugs through Systematic Inter-Thread Communication Analysis},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483549},
doi = {10.1145/3477132.3483549},
abstract = {Kernel concurrency bugs are challenging to find because they depend on very specific thread interleavings and test inputs. While separately exploring kernel thread interleavings or test inputs has been closely examined, jointly exploring interleavings and test inputs has received little attention, in part due to the resulting vast search space. Using precious, limited testing resources to explore this search space and execute just the right concurrent tests in the proper order is critical.This paper proposes Snowboard a testing framework that generates and executes concurrent tests by intelligently exploring thread interleavings and test inputs jointly. The design of Snowboard is based on a concept called potential memory communication (PMC), a guess about pairs of tests that, when executed concurrently, are likely to perform memory accesses to shared addresses, which in turn may trigger concurrency bugs. To identify PMCs, Snowboard runs tests sequentially from a fixed initial kernel state, collecting their memory accesses. It then pairs up tests that write and read the same region into candidate concurrent tests. It executes those tests using the associated PMC as a scheduling hint to focus interleaving search only on those schedules that directly affect the relevant memory accesses. By clustering candidate tests on various features of their PMCs, Snowboard avoids testing similar behaviors, which would be inefficient. Finally, by executing tests from small clusters first, it prioritizes uncommon suspicious behaviors that may have received less scrutiny.Snowboard discovered 14 new concurrency bugs in Linux kernels 5.3.10 and 5.12-rc3, of which 12 have been confirmed by developers. Six of these bugs cause kernel panics and filesystem errors, and at least two have existed in the kernel for many years, showing that this approach can uncover hard-to-find, critical bugs. Furthermore, we show that covering as many distinct pairs of uncommon read/write instructions as possible is the test-prioritization strategy with the highest bug yield for a given test-time budget.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {66–83},
numpages = {18},
keywords = {Operating systems security, Kernel concurrency bug, Software testing and debugging, Concurrency programming},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{10.1145/945445.945450,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
title = {The Google File System},
year = {2003},
isbn = {1581137575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945445.945450},
doi = {10.1145/945445.945450},
booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
pages = {29–43},
numpages = {15},
keywords = {clustered storage, fault tolerance, data storage, scalability},
location = {Bolton Landing, NY, USA},
series = {SOSP '03}
}

@inproceedings{10.1145/3277644.3277760,
author = {Mokhov, Serguei A. and Song, Miao and Mudur, Sudhir P. and Grogono, Peter},
title = {Hands-on: Rapid Interactive Application Prototyping for Media Arts and Stage Performance and Beyond},
year = {2018},
isbn = {9781450360265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277644.3277760},
doi = {10.1145/3277644.3277760},
abstract = {We complement the last three editions of the course at SIGGRAPH Asia (2015, 2016) and SIGGRAPH (2017) to make it more of a hands-on nature and include OpenISS. We explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2 and its OpenISS core.},
booktitle = {SIGGRAPH Asia 2018 Courses},
articleno = {9},
numpages = {32},
keywords = {illimitable space system (ISS), jitter/MAX, OpenISS, human-computer interfaces, real-time, computer graphics education, interaction, OpenGL, processing, RGBD cameras},
location = {Tokyo, Japan},
series = {SA '18}
}

@inproceedings{10.1145/3306306.3328008,
author = {Mokhov, Serguei A. and Song, Miao and Mudur, Sudhir P. and Grogono, Peter},
title = {Hands-on: Rapid Interactive Application Prototyping for Media Arts and Performing Arts in Illimitable Space},
year = {2019},
isbn = {9781450363167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306306.3328008},
doi = {10.1145/3306306.3328008},
abstract = {We complement the last three editions of the course at SIGGRAPH Asia (2015, 2016, 2018) and SIGGRAPH (2017) to make it more of a hands-on nature and include OpenISS. We explore a rapid prototyping of interactive graphical applications for stage and beyond using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2 and its OpenISS core framework for creative near-realtime broadcasting, and the use of AI and HCI techniques in art.},
booktitle = {ACM SIGGRAPH 2019 Studio},
articleno = {8},
numpages = {33},
keywords = {illimitable space system (ISS), interaction, processing, real-time, computer graphics education, Jitter/MAX, openISS, openGL, RGBD cameras, human-computer interfaces},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@article{10.1145/3631606,
author = {Gupta, Jit and Kant, Krishna and Pal, Amitangshu and Biswas, Joyanta},
title = {Configuring and Coordinating End-to-End QoS for Emerging Storage Infrastructure},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2376-3639},
url = {https://doi.org/10.1145/3631606},
doi = {10.1145/3631606},
abstract = {Modern data center storage systems are invariably networked to allow for consolidation and flexible management of storage. They also include high performance storage devices based on flash or other emerging technologies, generally accessed through low-latency and high throughput protocols such as NVMe (or its derivatives) carried over the network. With the increasing complexity and data-centric nature of the applications, properly configuring the quality of service (QoS) for the storage path has become crucial for ensuring the desired application performance. Such QoS is substantially influenced by the QoS in the network path, in the access protocol, and in the storage device. In this paper, we define a new transport level QoS mechanism for the network segment and demonstrate how it can augment and coordinate with the access level QoS mechanism defined for NVMe, and a similar QoS mechanism configured in the device. We show that the transport QoS mechanism not only provides the desired QoS to different classes of storage accesses but is also able to protect the access to the shared persistent memory (PM) devices located along with the storage but requiring much lower latency than storage. We demonstrate that a proper coordinated configuration of the 3 QoS’es on the path is crucial to achieve the desired differentiation depending on where the bottlenecks appear.},
note = {Just Accepted},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {nov},
keywords = {RDMA, Latency, Data Center TCP, Persistent Memory, NVMe, QoS}
}

@inproceedings{10.1145/3503222.3507733,
author = {Zhao, Mark and Gao, Mingyu and Kozyrakis, Christos},
title = {ShEF: Shielded Enclaves for Cloud FPGAs},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507733},
doi = {10.1145/3503222.3507733},
abstract = {FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs, map ShEF to a performant and secure storage application, and measure the performance benefits of customizable security using five additional accelerators.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1070–1085},
numpages = {16},
keywords = {cloud computing, FPGAs, enclaves, reconfigurable computing, trusted execution},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@article{10.14778/3476249.3476292,
author = {Durner, Dominik and Chandramouli, Badrish and Li, Yinan},
title = {Crystal: A Unified Cache Storage System for Analytical Databases},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476292},
doi = {10.14778/3476249.3476292},
abstract = {Cloud analytical databases employ a disaggregated storage model, where the elastic compute layer accesses data persisted on remote cloud storage in block-oriented columnar formats. Given the high latency and low bandwidth to remote storage and the limited size of fast local storage, caching data at the compute node is important and has resulted in a renewed interest in caching for analytics. Today, each DBMS builds its own caching solution, usually based on file-or block-level LRU. In this paper, we advocate a new architecture of a smart cache storage system called Crystal, that is co-located with compute. Crystal's clients are DBMS-specific "data sources" with push-down predicates. Similar in spirit to a DBMS, Crystal incorporates query processing and optimization components focusing on efficient caching and serving of single-table hyper-rectangles called regions. Results show that Crystal, with a small DBMS-specific data source connector, can significantly improve query latencies on unmodified Spark and Greenplum while also saving on bandwidth from remote storage.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2432–2444},
numpages = {13}
}

@inproceedings{10.1145/3579856.3595803,
author = {Ge, Jingquan and Zhang, Fengwei},
title = {FlushTime: Towards Mitigating Flush-Based Cache Attacks via Collaborating Flush Instructions and Timers on ARMv8-A},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3595803},
doi = {10.1145/3579856.3595803},
abstract = {ARMv8-A processors generally utilize optimization techniques such as multi-layer cache, out-of-order execution and branch prediction to improve performance. These optimization techniques are inevitably threatened by cache-related attacks including Flush+Reload, Flush+Flush, Meltdown, Spectre, and their variants. These attacks can break the isolation boundaries between different processes or even between user and kernel spaces. Researchers proposed many defense schemes to resist these cache-related attacks. However, they either need to modify the hardware architecture, have incomplete coverage, or introduce significant performance overhead. In this paper, we propose FlushTime, a more secure collaborative framework of cache flush instructions and generic timer on ARMv8-A. Based on the instruction/register trap mechanism of ARMv8-A, FlushTime traps cache flush instructions and generic timer from user space into kernel space, and makes them cooperate with each other in kernel space. When a flush instruction is called, the generic timer resolution will be reduced for several time slices. This collaborative mechanism can greatly mitigate the threat of all flush-based cache-related attacks. Since normal applications rarely need to obtain high resolution timestamps immediately after calling a flush instruction, FlushTime does not affect the normal operation of the system. Security and performance evaluations show that FlushTime can resist all flush-based cache-related attacks while introducing an extremely low performance overhead.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {190–204},
numpages = {15},
keywords = {Generic timer, Cache attack, ARMv8-A, Flush instruction, Defense},
location = {Melbourne, VIC, Australia},
series = {ASIA CCS '23}
}

@article{10.1145/2480741.2480751,
author = {Dionisio, John David N. and III, William G. Burns and Gilbert, Richard},
title = {3D Virtual Worlds and the Metaverse: Current Status and Future Possibilities},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2480741.2480751},
doi = {10.1145/2480741.2480751},
abstract = {Moving from a set of independent virtual worlds to an integrated network of 3D virtual worlds or Metaverse rests on progress in four areas: immersive realism, ubiquity of access and identity, interoperability, and scalability. For each area, the current status and needed developments in order to achieve a functional Metaverse are described. Factors that support the formation of a viable Metaverse, such as institutional and popular interest and ongoing improvements in hardware performance, and factors that constrain the achievement of this goal, including limits in computational methods and unrealized collaboration among virtual world stakeholders and developers, are also considered.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {34},
numpages = {38},
keywords = {Immersion, virtual worlds, Internet, second life}
}

@article{10.1145/3571194,
author = {Pulte, Christopher and Makwana, Dhruv C. and Sewell, Thomas and Memarian, Kayvan and Sewell, Peter and Krishnaswami, Neel},
title = {CN: Verifying Systems C Code with Separation-Logic Refinement Types},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571194},
doi = {10.1145/3571194},
abstract = {Despite significant progress in the verification of hypervisors, operating systems, and compilers, and in verification tooling, there exists a wide gap between the approaches used in verification projects and conventional development of systems software. We see two main challenges in bringing these closer together: verification handling the complexity of code and semantics of conventional systems software, and verification usability.  

We describe an experiment in verification tool design aimed at addressing some aspects of both: we design and implement CN, a separation-logic refinement type system for C systems software, aimed at predictable proof automation, based on a realistic semantics of ISO C. CN reduces refinement typing to decidable propositional logic reasoning, uses first-class resources to support pointer aliasing and pointer arithmetic, features resource inference for iterated separating conjunction, and uses a novel syntactic restriction of ghost variables in specifications to guarantee their successful inference. We implement CN and formalise key aspects of the type system, including a soundness proof of type checking. To demonstrate the usability of CN we use it to verify a substantial component of Google's pKVM hypervisor for Android.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {1},
numpages = {32},
keywords = {verification, pKVM, refinement types, Android, separation logic, C}
}

@article{10.1145/1274858.1274861,
author = {Chanet, Dominique and De Sutter, Bjorn and De Bus, Bruno and Van Put, Ludo and De Bosschere, Koen},
title = {Automated Reduction of the Memory Footprint of the Linux Kernel},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/1274858.1274861},
doi = {10.1145/1274858.1274861},
abstract = {The limited built-in configurability of Linux can lead to expensive code size overhead when it is used in the embedded market. To overcome this problem, we propose the application of link-time compaction and specialization techniques that exploit the a priori known, fixed runtime environment of many embedded systems. In experimental setups based on the ARM XScale and i386 platforms, the proposed techniques are able to reduce the kernel memory footprint with over 16%. We also show how relatively simple additions to existing binary rewriters can implement the proposed techniques for a complex, very unconventional program, such as the Linux kernel. We note that even after specialization, a lot of seemingly unnecessary code remains in the kernel and propose to reduce the footprint of this code by applying code-compression techniques. This technique, combined with the previous ones, reduces the memory footprint with over 23% for the i386 platform and 28% for the ARM platform. Finally, we pinpoint an important code size growth problem when compaction and compression techniques are combined on the ARM platform.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
pages = {23–es},
numpages = {48},
keywords = {specialization, system calls, Linux kernel, compaction, compression, operating system}
}

@inproceedings{10.1145/2785956.2787501,
author = {Sherry, Justine and Gao, Peter Xiang and Basu, Soumya and Panda, Aurojit and Krishnamurthy, Arvind and Maciocco, Christian and Manesh, Maziar and Martins, Jo\~{a}o and Ratnasamy, Sylvia and Rizzo, Luigi and Shenker, Scott},
title = {Rollback-Recovery for Middleboxes},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787501},
doi = {10.1145/2785956.2787501},
abstract = {Network middleboxes must offer high availability, with automatic failover when a device fails. Achieving high availability is challenging because failover must correctly restore lost state (e.g., activity logs, port mappings) but must do so quickly (e.g., in less than typical transport timeout values to minimize disruption to applications) and with little overhead to failure-free operation (e.g., additional per-packet latencies of 10-100s of us). No existing middlebox design provides failover that is correct, fast to recover, and imposes little increased latency on failure-free operations. We present a new design for fault-tolerance in middleboxes that achieves these three goals. Our system, FTMB (for Fault-Tolerant MiddleBox), adopts the classical approach of "rollback recovery" in which a system uses information logged during normal operation to correctly reconstruct state after a failure. However, traditional rollback recovery cannot maintain high throughput given the frequent output rate of middleboxes. Hence, we design a novel solution to record middlebox state which relies on two mechanisms: (1) 'ordered logging', which provides lightweight logging of the information needed after recovery, and (2) a `parallel release' algorithm which, when coupled with ordered logging, ensures that recovery is always correct. We implement ordered logging and parallel release in Click and show that for our test applications our design adds only 30$mu$s of latency to median per packet latencies. Our system introduces moderate throughput overheads (5-30%) and can reconstruct lost state in 40-275ms for practical systems.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {227–240},
numpages = {14},
keywords = {middlebox reliability, parallel fault-tolerance},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@proceedings{10.1145/3616961,
title = {Mindtrek '23: Proceedings of the 26th International Academic Mindtrek Conference},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3615366,
title = {LADC '23: Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {La Paz, Bolivia}
}

@inproceedings{10.1145/3491102.3517526,
author = {Belghith, Yasmine and Venkatagiri, Sukrit and Luther, Kurt},
title = {Compete, Collaborate, Investigate: Exploring the Social Structures of Open Source Intelligence Investigations},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517526},
doi = {10.1145/3491102.3517526},
abstract = {Online investigations are increasingly conducted by individuals with diverse skill levels and experiences, with mixed results. Novice investigations often result in vigilantism or doxxing, while expert investigations have greater success rates and fewer mishaps. Many of these experts are involved in a community of practice known as Open Source Intelligence (OSINT), with an ethos and set of techniques for conducting investigations using only publicly available data. Through semi-structured interviews with 14 expert OSINT investigators from nine different organizations, we examine the social dynamics of this community, including the collaboration and competition patterns that underlie their investigations. We also describe investigators’ use of and challenges with existing OSINT tools, and implications for the design of social computing systems to better support crowdsourced investigations.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {129},
numpages = {18},
keywords = {competition, crowdsourcing, experts, investigation, OSINT, collaboration, open source intelligence},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.1145/3622780,
title = {SPLASH-E 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on SPLASH-E},
year = {2023},
isbn = {9798400703904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SPLASH-E symposium is a forum for researchers and educators to discuss  
the intersection of education and the core SPLASH research areas: systems, programming languages, and their applications. We investigate how to deliver systems  
and programming language concepts to students, how systems and languages can  
aid in education broadly, and how to prepare students to apply these concepts to  
their later work in industry or academia.},
location = {Cascais, Portugal}
}

@article{10.14778/3554821.3554829,
author = {Pedreira, Pedro and Erling, Orri and Basmanova, Masha and Wilfong, Kevin and Sakka, Laith and Pai, Krishna and He, Wei and Chattopadhyay, Biswapesh},
title = {Velox: Meta's Unified Execution Engine},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554829},
doi = {10.14778/3554821.3554829},
abstract = {The ad-hoc development of new specialized computation engines targeted to very specific data workloads has created a siloed data landscape. Commonly, these engines share little to nothing with each other and are hard to maintain, evolve, and optimize, and ultimately provide an inconsistent experience to data users. In order to address these issues, Meta has created Velox, a novel open source C++ database acceleration library. Velox provides reusable, extensible, high-performance, and dialect-agnostic data processing components for building execution engines, and enhancing data management systems. The library heavily relies on vectorization and adaptivity, and is designed from the ground up to support efficient computation over complex data types due to their ubiquity in modern workloads. Velox is currently integrated or being integrated with more than a dozen data systems at Meta, including analytical query engines such as Presto and Spark, stream processing platforms, message buses and data warehouse ingestion infrastructure, machine learning systems for feature engineering and data preprocessing (PyTorch), and more. It provides benefits in terms of (a) efficiency wins by democratizing optimizations previously only found in individual engines, (b) increased consistency for data users, and (c) engineering efficiency by promoting reusability.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3372–3384},
numpages = {13}
}

@article{10.1145/3469029,
author = {Murshed, M. G. Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz},
title = {Machine Learning at the Network Edge: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469029},
doi = {10.1145/3469029},
abstract = {Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e., close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {170},
numpages = {37},
keywords = {low-power, Edge intelligence, machine learning, embedded, distributed computing, IoT, mobile edge computing, deep learning, resource-constrained}
}

@proceedings{10.1145/3576914,
title = {CPS-IoT Week '23: Proceedings of Cyber-Physical Systems and Internet of Things Week 2023},
year = {2023},
isbn = {9798400700491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@article{10.1145/505894.505900,
author = {Neumann, Peter G.},
title = {Risks to the Public in Computers and Related Systems},
year = {2001},
issue_date = {January 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/505894.505900},
doi = {10.1145/505894.505900},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {14–38},
numpages = {25}
}

@article{10.1145/3579461,
author = {Bouma-Sims, Elijah and Acar, Yasemin},
title = {Beyond the Boolean: How Programmers Ask About, Use, and Discuss Gender},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579461},
doi = {10.1145/3579461},
abstract = {Categorization via gender is omnipresent throughout society, and thus also computing; gender identity is often requested of users before they use software or web services. Despite this fact, no research has explored how software developers approach requesting gender disclosure from users. To understand how developers think about gender in software, we present an interview study with 15 software developers recruited from the freelancing platform Upwork as well as Twitter. We also collected and categorized 917 threads that contained keywords relevant to gender from programming-related sub-forums on the social media service Reddit. 16 posts that discussed approaches to gender disclosure were further analyzed. We found that while some developers have an understanding of inclusive gender options, programmers rarely consider when gender data is necessary or the way in which they request gender disclosure from users. Our findings have implications for programmers, software engineering educators, and the broader community concerned with inclusivity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {28},
numpages = {31},
keywords = {gender, interview methodology, Reddit, software development}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/1238844.1238856,
author = {Hudak, Paul and Hughes, John and Peyton Jones, Simon and Wadler, Philip},
title = {A History of Haskell: Being Lazy with Class},
year = {2007},
isbn = {9781595937667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1238844.1238856},
doi = {10.1145/1238844.1238856},
abstract = {This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.},
booktitle = {Proceedings of the Third ACM SIGPLAN Conference on History of Programming Languages},
pages = {12–1–12–55},
location = {San Diego, California},
series = {HOPL III}
}

@inproceedings{10.1145/3447786.3456259,
author = {Bashir, Noman and Deng, Nan and Rzadca, Krzysztof and Irwin, David and Kodak, Sree and Jnagal, Rohit},
title = {Take It to the Limit: Peak Prediction-Driven Resource Overcommitment in Datacenters},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456259},
doi = {10.1145/3447786.3456259},
abstract = {To increase utilization, datacenter schedulers often overcommit resources where the sum of resources allocated to the tasks on a machine exceeds its physical capacity. Setting the right level of overcommitment is a challenging problem: low overcommitment leads to wasted resources, while high overcommitment leads to task performance degradation. In this paper, we take a first principles approach to designing and evaluating overcommit policies by asking a basic question: assuming complete knowledge of each task's future resource usage, what is the safest overcommit policy that yields the highest utilization? We call this policy the peak oracle. We then devise practical overcommit policies that mimic this peak oracle by predicting future machine resource usage. We simulate our overcommit policies using the recently-released Google cluster trace, and show that they result in higher utilization and less overcommit errors than policies based on per-task allocations. We also deploy these policies to machines inside Google's datacenters serving its internal production workload. We show that our overcommit policies increase these machines' usable CPU capacity by 10-16% compared to no overcommitment.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {556–573},
numpages = {18},
keywords = {datacenters, overcommit, resource management},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@proceedings{10.1145/3593856,
title = {HOTOS '23: Proceedings of the 19th Workshop on Hot Topics in Operating Systems},
year = {2023},
isbn = {9798400701955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Providence, RI, USA}
}

@inproceedings{10.1145/3551349.3561152,
author = {Singhal, Vidush and Pillai, Akul Abhilash and Saumya, Charitha and Kulkarni, Milind and Machiry, Aravind},
title = {Cornucopia : A Framework for Feedback Guided Generation of Binaries},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561152},
doi = {10.1145/3551349.3561152},
abstract = {Binary analysis is an important capability required for many security and software engineering applications. Consequently, there are many binary analysis techniques and tools with varied capabilities. However, testing these tools requires a large, varied binary dataset with corresponding source-level information. In this paper, we present&nbsp;Cornucopia, an architecture agnostic automated framework that can generate a plethora of binaries from corresponding program source by exploiting compiler optimizations and feedback-guided learning. Our evaluation shows that&nbsp;Cornucopia was able to generate 309K binaries across four architectures (x86, x64, ARM, MIPS) with an average of&nbsp;403 binaries for each program and outperforms&nbsp;BinTuner &nbsp;[53], a similar technique. Our experiments revealed issues with the LLVM optimization scheduler resulting in compiler crashes (∼ 300). Our evaluation of four popular binary analysis tools angr,&nbsp;Ghidra,&nbsp;ida, and&nbsp;radare, using&nbsp;Cornucopia generated binaries, revealed various issues with these tools. Specifically, we found&nbsp;263 crashes in&nbsp;angr and one memory corruption issue in&nbsp;ida. Our differential testing on the analysis results revealed various semantic bugs in these tools. We also tested machine learning tools,&nbsp;Asm2Vec,&nbsp;SAFE, and&nbsp;Debin, that claim to capture binary semantics and show that they perform poorly (e.g., Debin F1 score dropped to 12.9% from reported 63.1%) on Cornucopia generated binaries. In summary, our exhaustive evaluation shows that&nbsp;Cornucopia is an effective mechanism to generate binaries for testing binary analysis techniques effectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {27},
numpages = {13},
keywords = {Fuzzing, Automated Binary Generation, Compiler Optimizations, Binary Code Difference},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@proceedings{10.1145/3500868,
title = {CSCW'22 Companion: Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2022},
isbn = {9781450391900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Taiwan}
}

@article{10.1145/3434322,
author = {Doenges, Ryan and Arashloo, Mina Tahmasbi and Bautista, Santiago and Chang, Alexander and Ni, Newton and Parkinson, Samwise and Peterson, Rudy and Solko-Breslin, Alaia and Xu, Amanda and Foster, Nate},
title = {Petr4: Formal Foundations for P4 Data Planes},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {POPL},
url = {https://doi.org/10.1145/3434322},
doi = {10.1145/3434322},
abstract = {P4 is a domain-specific language for programming and specifying packet-processing systems. It is based on an elegant design with high-level abstractions like parsers and match-action pipelines that can be compiled to efficient implementations in software or hardware. Unfortunately, like many industrial languages, P4 has developed without a formal foundation. The P4 Language Specification is a 160-page document with a mixture of informal prose, graphical diagrams, and pseudocode, leaving many aspects of the language semantics up to individual compilation targets. The P4 reference implementation is a complex system, running to over 40KLoC of C++ code, with support for only a few targets. Clearly neither of these artifacts is suitable for formal reasoning about P4 in general.  This paper presents a new framework, called Petr4, that puts P4 on a solid foundation. Petr4 consists of a clean-slate definitional interpreter and a core calculus that models a fragment of P4. Petr4 is not tied to any particular target: the interpreter is parameterized over an interface that collects features delegated to targets in one place, while the core calculus overapproximates target-specific behaviors using non-determinism.  We have validated the interpreter against a suite of over 750 tests from the P4 reference implementation, exercising our target interface with tests for different targets. We validated the core calculus with a proof of type-preserving termination. While developing Petr4, we reported dozens of bugs in the language specification and the reference implementation, many of which have been fixed.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {41},
numpages = {32},
keywords = {P4, formal semantics}
}

@proceedings{10.1145/3548659,
title = {A-TEST 2022: Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation},
year = {2022},
isbn = {9781450394529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 13th edition of the International Workshop on Automating Test Case Design, Selection and Evaluation (A-TEST 2022), co-located with and organized at ESEC/FSE 2022 during two days November 17-18, 2022 in Singapore. The A-TEST workshop aims to provide a venue for researchers and industry members alike to exchange and discuss trending views, ideas, state of the art, work in progress, and scientific results on automated testing.},
location = {Singapore, Singapore}
}

@article{10.1145/3498335,
author = {Reisinger, Thomas and Wagner, Isabel and Boiten, Eerke Albert},
title = {Security and Privacy in Unified Communication},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3498335},
doi = {10.1145/3498335},
abstract = {The use of unified communication; video conferencing, audio conferencing, and instant messaging has skyrocketed during the COVID-19 pandemic. However, security and privacy considerations have often been neglected. This article provides a comprehensive survey of security and privacy in Unified Communication (UC). We systematically analyze security and privacy threats and mitigations in a generic UC scenario. Based on this, we analyze security and privacy features of the major UC market leaders, and we draw conclusions on the overall UC landscape. While confidentiality in communication channels is generally well protected through encryption, other privacy properties are mostly lacking on UC platforms.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {55},
numpages = {36},
keywords = {audio conferencing, instant messaging, security, cloud service, Unified communication, privacy, LINDDUN, video conferencing, STRIDE}
}

@proceedings{10.1145/3508352,
title = {ICCAD '22: Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Jointly sponsored by ACM and IEEE, ICCAD is the premier forum to explore new challenges, present leading-edge innovative solutions, and identify emerging technologies in the Electronic Design Automation (EDA) research areas. ICCAD covers the full range of Computer-Aided Design (CAD) topics - from device and circuit-level up through system-level, as well as post-CMOS design.},
location = {San Diego, California}
}

@article{10.1145/2934687,
author = {Srba, Ivan and Bielikova, Maria},
title = {A Comprehensive Survey and Classification of Approaches for Community Question Answering},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2934687},
doi = {10.1145/2934687},
abstract = {Community question-answering (CQA) systems, such as Yahoo! Answers or Stack Overflow, belong to a prominent group of successful and popular Web 2.0 applications, which are used every day by millions of users to find an answer on complex, subjective, or context-dependent questions. In order to obtain answers effectively, CQA systems should optimally harness collective intelligence of the whole online community, which will be impossible without appropriate collaboration support provided by information technologies. Therefore, CQA became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research. Nevertheless, in spite of the increasing number of publications emerging each year, so far the research on CQA systems has missed a comprehensive state-of-the-art survey. We attempt to fill this gap by a review of 265 articles published between 2005 and 2014, which were selected from major conferences and journals. According to this evaluation, at first we propose a framework that defines descriptive attributes of CQA approaches. Second, we introduce a classification of all approaches with respect to problems they are aimed to solve. The classification is consequently employed in a review of a significant number of representative approaches, which are described by means of attributes from the descriptive framework. As a part of the survey, we also depict the current trends as well as highlight the areas that require further attention from the research community.},
journal = {ACM Trans. Web},
month = {aug},
articleno = {18},
numpages = {63},
keywords = {online communities, adaptive collaboration support, user modeling, content modeling, exploratory studies, knowledge sharing, Community question answering}
}

@proceedings{10.1145/3544794,
title = {ISWC '22: Proceedings of the 2022 ACM International Symposium on Wearable Computers},
year = {2022},
isbn = {9781450394246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, United Kingdom}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@article{10.1145/3291047,
author = {Pinto, Sandro and Santos, Nuno},
title = {Demystifying Arm TrustZone: A Comprehensive Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291047},
doi = {10.1145/3291047},
abstract = {The world is undergoing an unprecedented technological transformation, evolving into a state where ubiquitous Internet-enabled “things” will be able to generate and share large amounts of security- and privacy-sensitive data. To cope with the security threats that are thus foreseeable, system designers can find in Arm TrustZone hardware technology a most valuable resource. TrustZone is a System-on-Chip and CPU system-wide security solution, available on today’s Arm application processors and present in the new generation Arm microcontrollers, which are expected to dominate the market of smart “things.” Although this technology has remained relatively underground since its inception in 2004, over the past years, numerous initiatives have significantly advanced the state of the art involving Arm TrustZone. Motivated by this revival of interest, this paper presents an in-depth study of TrustZone technology. We provide a comprehensive survey of relevant work from academia and industry, presenting existing systems into two main areas, namely, Trusted Execution Environments and hardware-assisted virtualization. Furthermore, we analyze the most relevant weaknesses of existing systems and propose new research directions within the realm of tiniest devices and the Internet of Things, which we believe to have potential to yield high-impact contributions in the future.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {130},
numpages = {36},
keywords = {survey, TEE, virtualization, Arm, security, TrustZone}
}

@proceedings{10.1145/3623504,
title = {PAINT 2023: Proceedings of the 2nd ACM SIGPLAN International Workshop on Programming Abstractions and Interactive Notations, Tools, and Environments},
year = {2023},
isbn = {9798400703997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Programming environments that integrate tools, notations, and abstractions into a holistic user experience can provide programmers with better support for what they want to achieve. These programming environments can create an engaging place to do new forms of informational work---resulting in enjoyable, creative, and productive experiences with programming.  

In the workshop on Programming Abstractions and Interactive Notations, Tools, and Environments (PAINT), we want to discuss programming environments that support users in working with and creating notations and abstractions that matter to them. We are interested in the relationship between people centric notations and general-purpose programming languages and environments. How do we reflect the various experiences, needs, and priorities of the many people involved in programming --- whether they call it that or not?},
location = {Cascais, Portugal}
}

@book{10.1145/3591366,
editor = {Seneviratne, Oshani and Hendler, James},
title = {Linking the World’s Information: Essays on Tim Berners-Lee’s Invention of the World Wide Web},
year = {2023},
isbn = {9798400707940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {52},
abstract = {When Sir Tim Berners-Lee first proposed the foundations of the World Wide Web at CERN in 1989, his manager called it “vague, but exciting.” How things have changed since then! Twenty-six years later, Berners-Lee won the ACM Turing Award “for inventing the World Wide Web, the first Web browser, and the fundamental protocols and algorithms allowing the Web to scale.” This book is a compilation of articles on the original ideas of a true visionary and the subsequent research and development work he has led, helping to realize the Web’s full potential. It is intended for readers interested in the Web’s original technical development, how it has changed over time, and the social impacts of the Web as steered by Berners-Lee since the very beginning.The book covers Berners-Lee's development of the key protocols, naming schemes, and markup languages that led to his “world wide web” program and ultimately to the Web as we know it today. His early efforts were refined as Web technology spread around the world, and he was further guided by the work of the World Wide Web Consortium, which he founded and still directs. He was instrumental in the conceptualization and realization of the Semantic Web, a field that is gaining momentum in the age of big data and knowledge graphs; was a driving force for the field of Web Science, a new and growing research area dedicated to the study of both the engineering and the impacts of the Web; and he continues to innovate through his research work at MIT on open and decentralized information. Berners-Lee is also known for his contributions to keeping the Web open and ubiquitous via his work with the World Wide Web Foundation, the UK's Open Data Institute and his recent call for a crowdsourced magna carta for the Web. This book will help the reader to understand how Sir Tim’s invention of the World Wide Web has revolutionized not just Computer Science, but global society itself.}
}

@proceedings{10.1145/3579375,
title = {ACSW '23: Proceedings of the 2023 Australasian Computer Science Week},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.1145/3520495,
title = {OzCHI '21: Proceedings of the 33rd Australian Conference on Human-Computer Interaction},
year = {2021},
isbn = {9781450395984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@proceedings{10.5555/3623295,
title = {ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@article{10.1145/1330017.1330019,
author = {Weimer, Westley and Necula, George C.},
title = {Exceptional Situations and Program Reliability},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/1330017.1330019},
doi = {10.1145/1330017.1330019},
abstract = {It is difficult to write programs that behave correctly in the presence of run-time errors. Proper behavior in the face of exceptional situations is important to the reliability of long-running programs. Existing programming language features often provide poor support for executing clean-up code and for restoring invariants.We present a data-flow analysis for finding a certain class of exception-handling defects: those related to a failure to release resources or to clean up properly along all paths. Many real-world programs violate such resource usage rules because of incorrect exception handling. Our flow-sensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 1,300 exception handling defects in over 5 million lines of Java code.Based on those defects we propose a programming language feature, the compensation stack, that keeps track of obligations at run time and ensures that they are discharged. We present a type system for compensation stacks that tracks collections of obligations. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {mar},
articleno = {8},
numpages = {51},
keywords = {Error handling, compensating transactions, resource management, linear sagas, linear types}
}

@inproceedings{10.1145/2897826.2927348,
author = {Jiang, Chenfanfu and Schroeder, Craig and Teran, Joseph and Stomakhin, Alexey and Selle, Andrew},
title = {The Material Point Method for Simulating Continuum Materials},
year = {2016},
isbn = {9781450342896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897826.2927348},
doi = {10.1145/2897826.2927348},
abstract = {Simulating the physical behaviors of deformable objects and fluids has been an important topic in computer graphics. While the Lagrangian Finite Element Method (FEM) is widely used for elasto-plastic solids, it usually requires additional computational components in the case of large deformation, mesh distortion, fracture, self-collision and coupling between materials. Often, special solvers and strategies need to be developed for a particular problem. Recently, the hybrid Eulerian/Lagrangian Material Point Method (MPM) was introduced to the graphics community. It uses a continuum description of the governing equations and utilizes user-controllable elasto-plastic constitutive models. The hybrid nature of MPM allows using a regular Cartesian grid to automate treatment of self-collision and fracture. Like other particle methods such as Smoothed Particle Hydrodynamics (SPH), topology change is easy due to the lack of explicit connectivity between Lagrangian particles. Furthermore, MPM allows a grid-based implicit integration scheme that has conditioning independent of the number of Lagrangian particles. MPM also provides a unified particle simulation framework similar to Position Based Dynamics (PBD) for easy coupling of different materials. The power of MPM has been demonstrated in a number of recent papers for simulating various materials including elastic objects, snow, lava, sand and viscoelastic fluids. It is also highly integrated into the production framework of Walt Disney Animation Studios and has been used in featured animations including Frozen, Big Hero 6 and Zootopia.},
booktitle = {ACM SIGGRAPH 2016 Courses},
articleno = {24},
numpages = {52},
location = {Anaheim, California},
series = {SIGGRAPH '16}
}

@proceedings{10.1145/3586182,
title = {UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@article{10.1145/2534973,
author = {Stefik, Andreas and Siebert, Susanna},
title = {An Empirical Investigation into Programming Language Syntax},
year = {2013},
issue_date = {November 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1145/2534973},
doi = {10.1145/2534973},
abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.},
journal = {ACM Trans. Comput. Educ.},
month = {nov},
articleno = {19},
numpages = {40},
keywords = {Novice Programmers, Syntax, Programming Languages}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3571884,
title = {CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Eindhoven, Netherlands}
}

@proceedings{10.1145/3583120,
title = {IPSN '23: Proceedings of the 22nd International Conference on Information Processing in Sensor Networks},
year = {2023},
isbn = {9798400701184},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3578360,
title = {CC 2023: Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
year = {2023},
isbn = {9798400700880},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 32nd ACM SIGPLAN International Conference on Compiler Construction (CC 2023), held in Montr\'{e}al, Qu\'{e}bec, Canada over February 25–26, 2023. As in the previous eight years, CC is held jointly with the International Symposium on Code Generation and Optimization (CGO), the Symposium on Principles and Practice of Parallel Programming (PPoPP), and the International Symposium on High-Performance Computer Architecture (HPCA). Colocation of these four conferences creates an exciting opportunity for a broad range of researchers in the areas of compilation, optimization, parallelism, and computer architecture to interact and explore collaborative research opportunities.},
location = {Montr\'{e}al, QC, Canada}
}

@proceedings{10.1145/3487553,
title = {WWW '22: Companion Proceedings of the Web Conference 2022},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Lyon, France}
}

@proceedings{10.1145/3603781,
title = {CNIOT '23: Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3615335,
title = {SIGDOC '23: Proceedings of the 41st ACM International Conference on Design of Communication},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@proceedings{10.1145/2517349,
title = {SOSP '13: Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP 2013), held at the Nemacolin Woodlands Resort, Farmington, Pennsylvania, USA. This year's program includes 30 papers, and touches on a wide range of computer systems topics, from kernels to big data, from responsiveness to correctness, and from devices to data centers. The program committee made every effort to identify and include some of the most creative and thought-provoking ideas in computer systems today. Each accepted paper was shepherded by a program committee member to make sure the papers are as readable and complete as possible. We hope you will enjoy the program as much as we did in selecting it.},
location = {Farminton, Pennsylvania}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3624062,
title = {SC-W '23: Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@inproceedings{10.1145/1080148.1080161,
author = {Judd, Glenn and Steenkiste, Peter},
title = {A Simple Mechanism for Capturing and Replaying Wireless Channels},
year = {2005},
isbn = {1595930264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1080148.1080161},
doi = {10.1145/1080148.1080161},
abstract = {Physical layer wireless network emulation has the potential to be a powerful experimental tool. An important challenge in physical emulation, and traditional simulation, is to accurately model the wireless channel. In this paper we examine the possibility of using on-card signal strength measurements to capture wireless channel traces. A key advantage of this approach is the simplicity and ubiquity with which these measurements can be obtained since virtually all wireless devices provide the required metrics. We show that for low delay spread environments wireless traces gathered using this method can be replayed in a physical wireless emulator to produce higher layer network behavior that is similar to the behavior that would have occurred in the real world. Thus, wireless channel traces gathered using on-card metrics are an effective means of enabling existing low delay spread wireless testbeds to be emulated.},
booktitle = {Proceedings of the 2005 ACM SIGCOMM Workshop on Experimental Approaches to Wireless Network Design and Analysis},
pages = {58–63},
numpages = {6},
keywords = {channel capture, wireless, emulation},
location = {Philadelphia, Pennsylvania, USA},
series = {E-WIND '05}
}

@proceedings{10.1145/3570361,
title = {ACM MobiCom '23: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3552326,
title = {EuroSys '23: Proceedings of the Eighteenth European Conference on Computer Systems},
year = {2023},
isbn = {9781450394871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@proceedings{10.1145/3582016,
title = {ASPLOS 2023: Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to introduce the third and final volume of ASPLOS&nbsp;’23. For the first time, ASPLOS has embarked on a new multi-deadline review model. ASPLOS&nbsp;’23 featured 3 deadlines spaced throughout the year and published papers in three volumes. Multiple deadlines are meant to encourage authors to submit their papers when ready and to facilitate the selection of some papers for revision. In this, our final program chairs’ message, we will provide details on the execution of the third submission cycle along with a detailed discussion of the entire ASPLOS&nbsp;’23 process.},
location = {Vancouver, BC, Canada}
}

@proceedings{10.1145/2614217,
title = {SIGGRAPH '14: ACM SIGGRAPH 2014 Posters},
year = {2014},
isbn = {9781450329583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, Canada}
}

@proceedings{10.1145/3597638,
title = {ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3595360,
title = {DEEM '23: Proceedings of the Seventh Workshop on Data Management for End-to-End Machine Learning},
year = {2023},
isbn = {9798400702044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3603269,
title = {ACM SIGCOMM '23: Proceedings of the ACM SIGCOMM 2023 Conference},
year = {2023},
isbn = {9798400702365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/2897826,
title = {SIGGRAPH '16: ACM SIGGRAPH 2016 Courses},
year = {2016},
isbn = {9781450342896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In SIGGRAPH 2016 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses help attendees learn "how to do something" or "how to do something faster, better, smarter, or easier." They are presented in short (1.5 hours), medium (3.25 hours), or all-day formats and often include elements of interactive demonstration, performance, or other imaginative approaches to teaching.The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques to advanced instruction on the current and future technologies.},
location = {Anaheim, California}
}

@proceedings{10.1145/3609021,
title = {eBPF '23: Proceedings of the 1st Workshop on EBPF and Kernel Extensions},
year = {2023},
isbn = {9798400702938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3588444,
title = {MHV '23: Proceedings of the 2nd Mile-High Video Conference},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3581784,
title = {SC '23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Started in 1988, the SC Conference has become the annual nexus for researchers and practitioners from academia, industry and government to share information and foster collaborations to advance the state of the art in High Performance Computing (HPC), Networking, Storage, and Analysis.},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

